import streamlit as st
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
from docx import Document
from docx.shared import Inches, Mm
from docx.enum.section import WD_ORIENT
import PyPDF2
import google.generativeai as genai
import openai
import os
import tempfile
import re
from docx.shared import RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
import io
import base64
import numpy as np
import hashlib
from typing import Dict, List
from sklearn.metrics.pairwise import cosine_similarity

# Gemini File Search í†µí•©
from gemini_file_search import (
    GeminiFileSearchManager,
    search_relevant_guidelines_gemini,
    search_violation_cases_gemini,
    get_gemini_store_manager
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê´‘ì—­ì§€ìì²´ ì¡°ë¡€ ê²€ìƒ‰, ë¹„êµ, ë¶„ì„",
    page_icon="ğŸ›ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì‚¬ìš©ì ì •ì˜ CSS
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #4f46e5, #7c3aed);
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        color: white;
        margin-bottom: 2rem;
    }
    .step-card {
        background: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .result-card {
        background: #ffffff;
        border: 1px solid #d1d5db;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    .law-title {
        color: #dc2626;
        font-weight: bold;
    }

    /* íƒ­ ê¸€ì í¬ê¸° í‚¤ìš°ê¸° */
    .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p {
        font-size: 18px !important;
        font-weight: 600 !important;
        font-size: 1.1em;
        margin-bottom: 0.5rem;
    }
    .metro-name {
        color: #1e40af;
        font-weight: 600;
        margin-bottom: 0.3rem;
    }
    .stButton > button {
        width: 100%;
    }
</style>
""", unsafe_allow_html=True)

# API ì„¤ì •
OC = "climsneys85"
search_url = "http://www.law.go.kr/DRF/lawSearch.do"
detail_url = "http://www.law.go.kr/DRF/lawService.do"

# ê´‘ì—­ì§€ìì²´ ì½”ë“œ ë° ì´ë¦„
metropolitan_govs = {
    '6110000': 'ì„œìš¸íŠ¹ë³„ì‹œ',
    '6260000': 'ë¶€ì‚°ê´‘ì—­ì‹œ',
    '6270000': 'ëŒ€êµ¬ê´‘ì—­ì‹œ',
    '6280000': 'ì¸ì²œê´‘ì—­ì‹œ',
    '6290000': 'ê´‘ì£¼ê´‘ì—­ì‹œ',
    '6300000': 'ëŒ€ì „ê´‘ì—­ì‹œ',
    '5690000': 'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ',
    '6310000': 'ìš¸ì‚°ê´‘ì—­ì‹œ',
    '6410000': 'ê²½ê¸°ë„',
    '6530000': 'ê°•ì›íŠ¹ë³„ìì¹˜ë„',
    '6430000': 'ì¶©ì²­ë¶ë„',
    '6440000': 'ì¶©ì²­ë‚¨ë„',
    '6540000': 'ì „ë¶íŠ¹ë³„ìì¹˜ë„',
    '6460000': 'ì „ë¼ë‚¨ë„',
    '6470000': 'ê²½ìƒë¶ë„',
    '6480000': 'ê²½ìƒë‚¨ë„',
    '6500000': 'ì œì£¼íŠ¹ë³„ìì¹˜ë„'
}

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'search_results' not in st.session_state:
    st.session_state.search_results = []
if 'uploaded_pdf' not in st.session_state:
    st.session_state.uploaded_pdf = None
if 'search_query' not in st.session_state:
    st.session_state.search_query = ""
if 'word_doc_ready' not in st.session_state:
    st.session_state.word_doc_ready = False
if 'word_doc_data' not in st.session_state:
    st.session_state.word_doc_data = None
if 'selected_ordinances' not in st.session_state:
    st.session_state.selected_ordinances = []
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None

# Gemini File Search ê´€ë ¨ session state
if 'use_gemini_search' not in st.session_state:
    st.session_state.use_gemini_search = True  # ê¸°ë³¸ê°’: Gemini File Search ì‚¬ìš©
if 'gemini_store_manager' not in st.session_state:
    st.session_state.gemini_store_manager = None

# Ollama Cloud ê´€ë ¨ session state
if 'use_ollama_cloud' not in st.session_state:
    st.session_state.use_ollama_cloud = True  # ê¸°ë³¸ê°’: Ollama Cloud ì‚¬ìš© (ë¬´ë£Œ)
if 'ollama_api_key' not in st.session_state:
    # secretsì—ì„œ API í‚¤ ë¡œë“œ
    st.session_state.ollama_api_key = st.secrets.get("OLLAMA_API_KEY", "")

# RAG ë²¡í„°ìŠ¤í† ì–´ ê´€ë ¨ session state
if 'rag_vectorstores' not in st.session_state:
    st.session_state.rag_vectorstores = None
if 'rag_loaded' not in st.session_state:
    st.session_state.rag_loaded = False

def load_rag_vectorstores():
    """PKL íŒŒì¼ì—ì„œ RAG ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
    import pickle

    if st.session_state.rag_loaded:
        return st.session_state.rag_vectorstores

    vectorstores = {}

    # ìì¹˜ë²•ê·œ ë§¤ë‰´ì–¼ ë²¡í„°ìŠ¤í† ì–´
    manual_path = "enhanced_vectorstore_20250914_101739.pkl"
    if os.path.exists(manual_path):
        try:
            with open(manual_path, 'rb') as f:
                vectorstores['manual'] = pickle.load(f)
            st.success(f"âœ… ìì¹˜ë²•ê·œ ë§¤ë‰´ì–¼ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            st.warning(f"âš ï¸ ìì¹˜ë²•ê·œ ë§¤ë‰´ì–¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ì¬ì˜Â·ì œì†Œ ì¡°ë¡€ ëª¨ìŒì§‘ ë²¡í„°ìŠ¤í† ì–´
    cases_path = "3. ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ì¬ì˜Â·ì œì†Œ ì¡°ë¡€ ëª¨ìŒì§‘(â…¨) (1)_new_vectorstore.pkl"
    if os.path.exists(cases_path):
        try:
            with open(cases_path, 'rb') as f:
                vectorstores['cases'] = pickle.load(f)
            st.success(f"âœ… ì¬ì˜Â·ì œì†Œ íŒë¡€ ëª¨ìŒì§‘ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            st.warning(f"âš ï¸ ì¬ì˜Â·ì œì†Œ íŒë¡€ ëª¨ìŒì§‘ ë¡œë“œ ì‹¤íŒ¨: {e}")

    st.session_state.rag_vectorstores = vectorstores
    st.session_state.rag_loaded = True
    return vectorstores

def search_rag_context(query, vectorstores, top_k=5):
    """RAG ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
    results = []

    # í’ˆì§ˆ í•„í„° í•¨ìˆ˜: ëª©ì°¨/ì œëª©ë§Œ ìˆëŠ” ì²­í¬ ì œì™¸
    def is_quality_content(text):
        """ìœ ìš©í•œ ë‚´ìš©ì¸ì§€ íŒë‹¨"""
        # ìµœì†Œ ê¸¸ì´ ì²´í¬ (100ì ë¯¸ë§Œì€ ëª©ì°¨ì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
        if len(text) < 100:
            return False

        # ëª©ì°¨/ì œëª© íŒ¨í„´ ê°ì§€
        toc_patterns = [
            r'^ì œ\d+ì¥\s+',  # ì œ1ì¥
            r'^ì œ\d+ì ˆ\s+',  # ì œ1ì ˆ
            r'^\d+\.\s+\w+\s*$',  # 1. ì œëª©
            r'^[ê°€-í£]+\s+\d+$',  # ëª©ì°¨ ë²ˆí˜¸
            r'^\s*ëª©\s*ì°¨\s*$',  # ëª©ì°¨
            r'^\s*ì°¨\s*ë¡€\s*$',  # ì°¨ë¡€
        ]

        for pattern in toc_patterns:
            if re.search(pattern, text.strip(), re.MULTILINE):
                # íŒ¨í„´ì´ ìˆì–´ë„ ë‚´ìš©ì´ ì¶©ë¶„íˆ ìˆìœ¼ë©´ í—ˆìš©
                if len(text) > 300:
                    return True
                return False

        # ë¬¸ì¥ ì™„ì„±ë„ ì²´í¬: ë§ˆì¹¨í‘œê°€ 3ê°œ ì´ìƒ ìˆì–´ì•¼ í•¨ (ì„¤ëª…ì´ ìˆëŠ” í…ìŠ¤íŠ¸)
        sentence_count = text.count('.') + text.count('ë‹¤.') + text.count('í•¨.')
        if sentence_count < 2:
            return False

        # ì‹¤ì œ ë²•ë¥  ìš©ì–´ë‚˜ ì„¤ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€
        useful_keywords = ['íŒë‹¨', 'í•´ì„', 'ë”°ë¼ì„œ', 'ê²½ìš°', 'ê·œì •', 'ìœ„ë°˜', 'ì ë²•', 'ìœ„ë²•', 'ê²€í† ', 'ì‚¬ë¡€', 'íŒë¡€']
        has_useful_content = any(kw in text for kw in useful_keywords)

        return has_useful_content or len(text) > 500

    for store_name, store_data in vectorstores.items():
        try:
            # ë²¡í„°ìŠ¤í† ì–´ í˜•ì‹ì— ë”°ë¼ ê²€ìƒ‰ ìˆ˜í–‰
            if isinstance(store_data, dict):
                # chunks í‚¤ê°€ ìˆëŠ” ê²½ìš° (ìš°ì„  ì‚¬ìš©)
                if 'chunks' in store_data:
                    chunks = store_data['chunks']
                    query_keywords = [kw.lower() for kw in query.split() if len(kw) > 1]

                    scored_chunks = []
                    for chunk in chunks:
                        if isinstance(chunk, dict) and 'text' in chunk:
                            text = chunk['text']
                        elif isinstance(chunk, str):
                            text = chunk
                        else:
                            continue

                        # í’ˆì§ˆ í•„í„°: ìœ ìš©í•œ ë‚´ìš©ì¸ì§€ ì²´í¬
                        if not is_quality_content(text):
                            continue

                        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                        text_lower = text.lower()
                        keyword_score = sum(1 for kw in query_keywords if kw in text_lower)

                        # ë‚´ìš© ë°€ë„ ë³´ë„ˆìŠ¤: ê¸´ í…ìŠ¤íŠ¸ì— ë³´ë„ˆìŠ¤ ì ìˆ˜
                        length_bonus = min(len(text) / 500, 3.0)  # ìµœëŒ€ 3ì  ë³´ë„ˆìŠ¤

                        # ë²•ë¥  ë¶„ì„ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤
                        analysis_keywords = ['íŒë‹¨', 'ê²€í† ', 'ìœ„ë²•', 'ì ë²•', 'ì‚¬ë¡€', 'íŒë¡€', 'í•´ì„', 'ê¸°ì¤€']
                        analysis_bonus = sum(0.5 for kw in analysis_keywords if kw in text)

                        total_score = keyword_score + length_bonus + analysis_bonus

                        if keyword_score > 0:
                            scored_chunks.append((text, total_score))

                    # ìƒìœ„ ê²°ê³¼ ì„ íƒ
                    scored_chunks.sort(key=lambda x: x[1], reverse=True)
                    for text, score in scored_chunks[:top_k]:
                        results.append({
                            'source': store_name,
                            'text': text[:2000],  # ìµœëŒ€ 2000ì
                            'score': score
                        })

                # texts í‚¤ê°€ ìˆëŠ” ê²½ìš°
                elif 'texts' in store_data:
                    texts = store_data['texts']
                    query_keywords = [kw.lower() for kw in query.split() if len(kw) > 1]

                    scored_texts = []
                    for text in texts:
                        if isinstance(text, str):
                            text_lower = text.lower()
                            score = sum(1 for kw in query_keywords if kw in text_lower)
                            if score > 0:
                                scored_texts.append((text, score))

                    scored_texts.sort(key=lambda x: x[1], reverse=True)
                    for text, score in scored_texts[:top_k]:
                        results.append({
                            'source': store_name,
                            'text': text[:2000],
                            'score': score
                        })

                # documents í‚¤ê°€ ìˆëŠ” ê²½ìš°
                elif 'documents' in store_data:
                    docs = store_data['documents']
                    query_keywords = [kw.lower() for kw in query.split() if len(kw) > 1]

                    scored_docs = []
                    for doc in docs:
                        if isinstance(doc, dict):
                            text = doc.get('text', doc.get('content', ''))
                        elif isinstance(doc, str):
                            text = doc
                        else:
                            continue

                        text_lower = text.lower()
                        score = sum(1 for kw in query_keywords if kw in text_lower)
                        if score > 0:
                            scored_docs.append((text, score))

                    scored_docs.sort(key=lambda x: x[1], reverse=True)
                    for text, score in scored_docs[:top_k]:
                        results.append({
                            'source': store_name,
                            'text': text[:2000],
                            'score': score
                        })
            elif hasattr(store_data, 'similarity_search'):
                # LangChain ìŠ¤íƒ€ì¼ ë²¡í„°ìŠ¤í† ì–´
                docs = store_data.similarity_search(query, k=top_k)
                for doc in docs:
                    results.append({
                        'source': store_name,
                        'text': doc.page_content[:2000],
                        'score': 1.0
                    })
        except Exception as e:
            st.warning(f"âš ï¸ {store_name} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")

    # ì ìˆ˜ìˆœ ì •ë ¬
    results.sort(key=lambda x: x.get('score', 0), reverse=True)
    return results[:top_k * 2]  # ìµœëŒ€ top_k * 2ê°œ ë°˜í™˜

def call_ollama_cloud_api(prompt, model="gpt-oss:120b-cloud", max_chars=100000):
    """Ollama Cloud APIë¥¼ í˜¸ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±

    Args:
        prompt: ë¶„ì„ í”„ë¡¬í”„íŠ¸
        model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: gpt-oss:120b-cloud)
        max_chars: ìµœëŒ€ ë¬¸ì ìˆ˜ (ê¸°ë³¸: 100000ì, í•œê¸€ ê¸°ì¤€ ì•½ 50-70K í† í°)
    """
    try:
        api_key = st.session_state.ollama_api_key
        if not api_key or api_key == "YOUR_OLLAMA_API_KEY_HERE":
            st.error("Ollama Cloud API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ë°©ì§€ - í•œê¸€ì€ í† í° íš¨ìœ¨ì´ ë‚®ìŒ)
        original_len = len(prompt)
        if original_len > max_chars:
            st.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({original_len:,}ì). {max_chars:,}ìë¡œ ìë™ ì¶•ì†Œí•©ë‹ˆë‹¤.")
            # í•µì‹¬ ë¶€ë¶„ì„ ìœ ì§€í•˜ë©´ì„œ ì¶•ì†Œ
            # ì•ë¶€ë¶„(ì§€ì‹œì‚¬í•­ + ì¡°ë¡€ì•ˆ)ê³¼ ë’·ë¶€ë¶„(ë¶„ì„ ìš”ì²­)ì„ ìœ ì§€
            front_chars = int(max_chars * 0.4)  # ì•ë¶€ë¶„ 40%
            back_chars = int(max_chars * 0.3)   # ë’·ë¶€ë¶„ 30%

            prompt = (
                prompt[:front_chars] +
                f"\n\n... [ì¤‘ëµ: ì›ë³¸ {original_len:,}ì ì¤‘ {original_len - max_chars:,}ì ìƒëµë¨] ...\n\n" +
                prompt[-back_chars:]
            )
            st.info(f"âœ… í”„ë¡¬í”„íŠ¸ë¥¼ {len(prompt):,}ìë¡œ ì¶•ì†Œí–ˆìŠµë‹ˆë‹¤.")

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "stream": False
        }

        response = requests.post(
            "https://ollama.com/api/chat",
            headers=headers,
            json=payload,
            timeout=180  # íƒ€ì„ì•„ì›ƒ ì¦ê°€ (ê¸´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬)
        )

        if response.status_code == 200:
            result = response.json()
            # Ollama API ì‘ë‹µ í˜•ì‹ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if "message" in result and "content" in result["message"]:
                return result["message"]["content"]
            elif "response" in result:
                return result["response"]
            else:
                st.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ í˜•ì‹: {result}")
                return str(result)
        else:
            st.error(f"Ollama Cloud API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return None

    except requests.exceptions.Timeout:
        st.error("Ollama Cloud API ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (120ì´ˆ)")
        return None
    except Exception as e:
        st.error(f"Ollama Cloud API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
        return None

def get_ordinance_detail(ordinance_id):
    """ì¡°ë¡€ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
    params = {
        'OC': OC,
        'target': 'ordin',
        'ID': ordinance_id,
        'type': 'XML'
    }
    try:
        response = requests.get(detail_url, params=params, timeout=60)
        root = ET.fromstring(response.text)
        articles = []
        for article in root.findall('.//ì¡°'):
            content = article.find('ì¡°ë‚´ìš©').text if article.find('ì¡°ë‚´ìš©') is not None else ""
            if content:
                content = content.replace('<![CDATA[', '').replace(']]>', '')
                content = content.replace('<p>', '').replace('</p>', '\n')
                content = content.replace('<br/>', '\n')
                content = content.replace('<br>', '\n')
                content = content.replace('&nbsp;', ' ')
                content = content.strip()
            if content:
                articles.append(content)
        return articles
    except Exception:
        return []

def search_ordinances(query):
    """ì¡°ë¡€ ê²€ìƒ‰ í•¨ìˆ˜"""
    results = []
    total_count = 0
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_metros = len(metropolitan_govs)
    
    for idx, (org_code, metro_name) in enumerate(metropolitan_govs.items()):
        status_text.text(f"ê²€ìƒ‰ ì¤‘... {metro_name} ({idx + 1}/{total_metros})")
        progress_bar.progress((idx + 1) / total_metros)
        
        try:
            params = {
                'OC': OC,
                'target': 'ordin',
                'type': 'XML',
                'query': query,
                'display': 100,
                'search': 1,
                'sort': 'ddes',
                'page': 1,
                'org': org_code
            }
            
            response = requests.get(search_url, params=params, timeout=60)
            response.raise_for_status()
            
            root = ET.fromstring(response.text)
            
            for law in root.findall('.//law'):
                ordinance_name = law.find('ìì¹˜ë²•ê·œëª…').text if law.find('ìì¹˜ë²•ê·œëª…') is not None else ""
                ordinance_id = law.find('ìì¹˜ë²•ê·œID').text if law.find('ìì¹˜ë²•ê·œID') is not None else None
                ê¸°ê´€ëª… = law.find('ì§€ìì²´ê¸°ê´€ëª…').text if law.find('ì§€ìì²´ê¸°ê´€ëª…') is not None else ""
                
                if ê¸°ê´€ëª… != metro_name:
                    continue
                
                # ê²€ìƒ‰ì–´ ë§¤ì¹­ ë¡œì§
                search_terms = [term.lower() for term in query.split() if term.strip()]
                ordinance_name_clean = ordinance_name.replace(' ', '').lower()
                if not all(term in ordinance_name_clean for term in search_terms):
                    continue
                
                total_count += 1
                articles = get_ordinance_detail(ordinance_id)
                
                results.append({
                    'name': ordinance_name,
                    'content': articles,
                    'metro': metro_name
                })
                
        except Exception as e:
            st.warning(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({metro_name}): {str(e)}")
            continue
    
    progress_bar.empty()
    status_text.empty()
    
    return results, total_count

def create_word_document(query, results):
    """Word ë¬¸ì„œ ìƒì„± í•¨ìˆ˜"""
    doc = Document()
    section = doc.sections[-1]
    section.orientation = WD_ORIENT.LANDSCAPE
    section.page_width = Mm(420)
    section.page_height = Mm(297)

    # ì œëª© ì¶”ê°€
    title = doc.add_heading('ì¡°ë¡€ ê²€ìƒ‰ ê²°ê³¼', level=1)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    doc.add_paragraph(f'ê²€ìƒ‰ì–´: {query}')
    doc.add_paragraph(f'ì´ {len(results)}ê±´ì˜ ì¡°ë¡€ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.\n')

    # ì¡°ë¡€ë¥¼ 3ê°œì”© ê·¸ë£¹í™”í•˜ì—¬ 3ë‹¨ ë¹„êµí‘œ í˜•íƒœë¡œ ìƒì„±
    for i in range(0, len(results), 3):
        current_laws = results[i:i+3]
        while len(current_laws) < 3:
            current_laws.append({'name': '', 'content': [], 'metro': ''})

        # í‘œ ìƒì„± (1í–‰, 3ì—´ ê³ ì •)
        table = doc.add_table(rows=1, cols=3)
        table.style = 'Table Grid'
        table.autofit = True

        # ê° ì…€ì— ì¡°ë¡€ ë‚´ìš© ì¶”ê°€
        for idx, law in enumerate(current_laws):
            cell = table.cell(0, idx)
            paragraph = cell.paragraphs[0]
            
            if law['name']:
                # ì¡°ë¡€ëª… ì¶”ê°€ (ì§€ìì²´ëª… + ì¡°ë¡€ëª…)
                run = paragraph.add_run(f"{law['metro']}\n{law['name']}\n\n")
                run.bold = True
                run.font.color.rgb = RGBColor(255, 0, 0)  # ë¹¨ê°„ìƒ‰
                
                # ì¡°ë¬¸ ë‚´ìš© ì¶”ê°€
                if law['content']:
                    content_text = '\n\n'.join(law['content'])
                    paragraph.add_run(content_text)
                else:
                    paragraph.add_run('(ì¡°ë¬¸ ì—†ìŒ)')

        # ë§ˆì§€ë§‰ í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ í˜ì´ì§€ ë‚˜ëˆ„ê¸° ì¶”ê°€
        if i + 3 < len(results):
            doc.add_page_break()

    return doc

def extract_pdf_text(pdf_file):
    """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜"""
    try:
        reader = PyPDF2.PdfReader(pdf_file)
        text = ''
        for page in reader.pages:
            text += page.extract_text() + '\n'
        return text
    except Exception as e:
        st.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def extract_superior_laws(pdf_text):
    """ì¡°ë¡€ì•ˆì—ì„œ ìƒìœ„ë²•ë ¹ ì¶”ì¶œ í•¨ìˆ˜ - GUI ê²€ì¦ëœ ë¡œì§ ì ìš©"""
    import re

    # ìƒìœ„ë²• í›„ë³´ ì¶”ì¶œì„ ìœ„í•œ í‚¤ì›Œë“œ (ì¡°ë¡€ì•ˆì—ì„œ ìƒìœ„ë²•ë ¹ ì–¸ê¸‰í•˜ëŠ” ëª¨ë“  ë§¥ë½ í¬í•¨)
    law_check_keywords = [
        'ìœ„ë°˜', 'ìœ„ë°°', 'ì¶©ëŒ', 'ì €ì´‰', 'ì¤€ìˆ˜', 'ì í•©', 'ë¶ˆì¼ì¹˜',
        'ìƒìœ„ë²•', 'ìƒìœ„ ë²•ë ¹', 'ìƒìœ„ë²•ë ¹', 'ë²•ë ¹ê³¼ì˜ ê´€ê³„', 'ë²•ë ¹ê³¼ì˜ ì¶©ëŒ', 'ë²•ë ¹ê³¼ì˜ ìœ„ë°°',
        'ê´€ê³„ë²•ë ¹', 'ê·¼ê±°ë²•ë ¹', 'ë²•ì ê·¼ê±°', 'ì°¸ê³ ì‚¬í•­', 'ê´€ë ¨ë²•ë ¹', 'ì†Œê´€ë²•ë ¹',
        'ë²•ë ¹', 'ë²•ë¥ ', 'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™', 'ê·œì •', 'ê°œì •', 'ì œì •', 'ë²•'  # ì¼ë°˜ì ì¸ ë²•ë ¹ ì–¸ê¸‰
    ]

    # ë²•ë ¹ëª… íŒ¨í„´ (ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™ ì¶”ì¶œ ê°œì„ )
    law_pattern = re.compile(r'([ê°€-í£\w\s]*(?:ë²•|ì‹œí–‰ë ¹|ì‹œí–‰ê·œì¹™))\s*(?:[ã€]|$|[.,;:\s])', re.MULTILINE)

    # ìƒìœ„ë²• í›„ë³´ ì¶”ì¶œ
    upper_law_candidates = set()

    # 1. ìƒìœ„ë²• ê´€ë ¨ ë§¥ë½ì´ ìˆëŠ” ì¤„ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ
    for line in pdf_text.split('\n'):
        if any(keyword in line for keyword in law_check_keywords):
            for match in law_pattern.finditer(line):
                law_name = match.group(1).strip()
                if law_name:
                    upper_law_candidates.add(law_name)

    # 2. ì¶”ê°€ íŒ¨í„´: ã€Œë²•ë ¹ëª…ã€ í˜•ì‹ìœ¼ë¡œ ë”°ì˜´í‘œ ì•ˆì— ìˆëŠ” ë²•ë ¹ëª… ì¶”ì¶œ
    quote_pattern = re.compile(r'[ã€Œã€]([^ã€ã€]*(?:ë²•|ì‹œí–‰ë ¹|ì‹œí–‰ê·œì¹™))[ã€ã€]')
    for match in quote_pattern.finditer(pdf_text):
        law_name = match.group(1).strip()
        if law_name:
            upper_law_candidates.add(law_name)

    # 3. ì¶”ê°€ íŒ¨í„´: "â—‹â—‹ë²•ë ¹:" ë˜ëŠ” "ê´€ê³„ë²•ë ¹:" ë’¤ì— ì˜¤ëŠ” ë²•ë ¹ëª…
    relation_pattern = re.compile(r'(?:ê´€ê³„ë²•ë ¹|ê·¼ê±°ë²•ë ¹|ë²•ì ê·¼ê±°|ì†Œê´€ë²•ë ¹|ê´€ë ¨ë²•ë ¹)\s*[:ï¼š]\s*[ã€Œã€]?([^ã€ã€\n]*(?:ë²•|ì‹œí–‰ë ¹|ì‹œí–‰ê·œì¹™))[ã€ã€]?')
    for match in relation_pattern.finditer(pdf_text):
        law_name = match.group(1).strip()
        if law_name:
            upper_law_candidates.add(law_name)

    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì‹¤ì¡´í•˜ì§€ ì•ŠëŠ” ë²•ë ¹ëª…)
    invalid_law_names = {
        'ìì¹˜ì…ë²•', 'ì¡°ë¡€', 'ê·œì¹™', 'ì§€ì¹¨', 'ë‚´ê·œ', 'ì˜ˆê·œ', 'í›ˆë ¹', 'ì ë²•',
        'ì…ë²•', 'ìƒìœ„ë²•', 'ìœ„ë²•', 'í•©ë²•', 'ë¶ˆë²•', 'ë°©ë²•', 'í—Œë²•ìƒ', 'í—Œë²•ì ',
        'ë²•ì ', 'ë²•ë¥ ì ', 'ë²•ë ¹ìƒ', 'ë²•ë¥ ìƒ', 'ë²•ë¥ ', 'ë²•ë ¹', 'ë²•', 'ê·œì •',
        'ì¡°í•­', 'ì¡°ë¬¸', 'ê·œë²”', 'ì›ì¹™', 'ê¸°ì¤€', 'ì‚¬í•­', 'ë‚´ìš©', 'ê´€ë ¨ë²•',
        'ê´€ë ¨ ë²•', 'ê´€ë ¨ë²•ë ¹', 'ê´€ë ¨ ë²•ë ¹'
    }

    def is_valid_law_name(name):
        """ìœ íš¨í•œ ë²•ë ¹ëª…ì¸ì§€ ê²€ì¦"""
        # ëŒ€ì†Œë¬¸ì, ê³µë°± ëª¨ë‘ ì œê±° í›„ ë¹„êµ
        name_clean = name.strip().replace(' ', '').lower()

        # ë¶ˆìš©ì–´ ì²´í¬
        for invalid in invalid_law_names:
            if name_clean == invalid.replace(' ', '').lower():
                return False

        # ìˆ«ì+ë²•(ì˜ˆ: 1ë²•, 2ë²• ë“±)ë„ ì œì™¸
        if name_clean and name_clean[0].isdigit():
            return False

        # ë„ˆë¬´ ì§§ì€ ì´ë¦„ ì œì™¸
        if len(name_clean) < 3:
            return False

        return True

    # ìœ íš¨í•œ ë²•ë ¹ëª…ë§Œ í•„í„°ë§
    valid_laws = []
    for law_name in upper_law_candidates:
        if is_valid_law_name(law_name):
            valid_laws.append(law_name)

    # ğŸ†• ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™ ìë™ ìœ ì¶” ì¶”ê°€
    additional_laws = []
    for law in valid_laws:
        if law.endswith('ë²•') and 'ì‹œí–‰' not in law:
            # í•´ë‹¹ ë²•ë¥ ì˜ ì‹œí–‰ë ¹ê³¼ ì‹œí–‰ê·œì¹™ì„ ìë™ìœ¼ë¡œ ì¶”ê°€
            base_name = law

            # ì‹œí–‰ë ¹ ì¶”ê°€ (ì¼ë°˜ì ì¸ íŒ¨í„´)
            potential_decree = f"{base_name} ì‹œí–‰ë ¹"
            if potential_decree not in valid_laws:
                additional_laws.append(potential_decree)

            # ì‹œí–‰ê·œì¹™ ì¶”ê°€ (ì¼ë°˜ì ì¸ íŒ¨í„´)
            potential_rule = f"{base_name} ì‹œí–‰ê·œì¹™"
            if potential_rule not in valid_laws:
                additional_laws.append(potential_rule)

    # ì¶”ê°€ëœ ë²•ë ¹ë“¤ì„ í¬í•¨
    if additional_laws:
        import streamlit as st
        st.info(f"ğŸ”„ ìë™ ì¶”ê°€ëœ í•˜ìœ„ ë²•ë ¹: {len(additional_laws)}ê°œ")
        with st.expander("ğŸ“‹ ìë™ ì¶”ê°€ëœ ë²•ë ¹", expanded=False):
            for law in additional_laws:
                st.markdown(f"- {law}")
        valid_laws.extend(additional_laws)

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    unique_laws = list(set(valid_laws))
    unique_laws.sort()

    return unique_laws[:20]  # ìµœëŒ€ 20ê°œ ë°˜í™˜

def get_superior_law_content_xml(law_name):
    """XML APIë¥¼ í†µí•´ ìƒìœ„ë²•ë ¹ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì„±ê³µì ì¸ ë¡œì§ ì ìš©)"""
    try:
        import xml.etree.ElementTree as ET
        import re

        # ê²€ìƒ‰ì–´ ìµœì í™”: ë„ì–´ì“°ê¸°ì™€ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        search_query = law_name.strip()

        # 1ë‹¨ê³„: ë²•ë ¹ ê²€ìƒ‰ (ë” ë§ì€ ê²°ê³¼ ë°˜í™˜)
        search_params = {
            'OC': OC,
            'target': 'law',
            'type': 'XML',
            'query': search_query,
            'display': 10  # ë” ë§ì€ ê²°ê³¼ ê²€ìƒ‰
        }
        
        search_response = requests.get(search_url, params=search_params, timeout=30)
        if search_response.status_code != 200:
            return get_superior_law_content_xml_fallback(law_name)
        
        search_root = ET.fromstring(search_response.text)
        
        # í˜„í–‰ ë²•ë ¹ ì°¾ê¸° - ë” ìœ ì—°í•œ ê²€ìƒ‰
        current_laws = []
        for law in search_root.findall('.//law'):
            status = law.find('í˜„í–‰ì—°í˜ì½”ë“œ')
            if status is not None and status.text == 'í˜„í–‰':
                law_id_elem = law.find('ë²•ë ¹ID')
                law_name_elem = law.find('ë²•ë ¹ëª…í•œê¸€')
                if law_id_elem is not None and law_name_elem is not None:
                    current_laws.append({
                        'id': law_id_elem.text,
                        'name': law_name_elem.text
                    })

        if not current_laws:
            return get_superior_law_content_xml_fallback(law_name)
        
        # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë²•ë ¹ ì„ íƒ (ê°œì„ ëœ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜)
        best_law = None
        best_score = -1

        for law_info in current_laws:
            found_name = law_info['name']
            score = 0

            # 1. ì •í™•í•œ ë§¤ì¹­ ìš°ì„ 
            if found_name == law_name:
                score += 1000

            # 2. ë¶€ë¶„ ë§¤ì¹­ ì ìˆ˜ (ì–‘ë°©í–¥)
            if law_name in found_name:
                score += 500
            if found_name in law_name:
                score += 300

            # 3. í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°œì„ ëœ ë¡œì§)
            law_lower = law_name.lower().replace(' ', '')
            found_lower = found_name.lower().replace(' ', '')

            # ì—¬ê°ìë™ì°¨ ìš´ìˆ˜ì‚¬ì—…ë²• ê´€ë ¨ íŠ¹ë³„ ì ìˆ˜
            if 'ì—¬ê°ìë™ì°¨' in law_lower and 'ìš´ìˆ˜ì‚¬ì—…' in law_lower:
                if 'ì—¬ê°ìë™ì°¨' in found_lower and 'ìš´ìˆ˜ì‚¬ì—…' in found_lower:
                    score += 400  # ì—¬ê°ìë™ì°¨ ìš´ìˆ˜ì‚¬ì—…ë²• ê´€ë ¨ ë†’ì€ ì ìˆ˜
                    if 'ì‹œí–‰ê·œì¹™' in law_lower and 'ì‹œí–‰ê·œì¹™' in found_lower:
                        score += 200  # ì‹œí–‰ê·œì¹™ ë§¤ì¹­ ì¶”ê°€ ì ìˆ˜

            # ë„ë¡œêµí†µë²• ê´€ë ¨
            if 'ë„ë¡œ' in law_lower and 'êµí†µ' in law_lower:
                if 'ë„ë¡œêµí†µ' in found_lower and 'íŠ¹ë³„íšŒê³„' not in found_lower:
                    score += 300
                elif 'êµí†µì‹œì„¤' in found_lower:
                    score -= 100

            # 4. ë²•ë ¹ ìœ í˜• ë§¤ì¹­ ì ìˆ˜ (ìš”ì²­ëœ ìœ í˜•ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€)
            requested_type = ''
            if 'ì‹œí–‰ê·œì¹™' in law_lower:
                requested_type = 'ì‹œí–‰ê·œì¹™'
            elif 'ì‹œí–‰ë ¹' in law_lower:
                requested_type = 'ì‹œí–‰ë ¹'
            elif 'ë²•' in law_lower and 'ì‹œí–‰' not in law_lower:
                requested_type = 'ë²•'

            if requested_type:
                if requested_type in found_lower:
                    score += 300  # ìš”ì²­ëœ ë²•ë ¹ ìœ í˜•ê³¼ ì¼ì¹˜í•˜ë©´ ë†’ì€ ì ìˆ˜
                elif requested_type == 'ë²•' and found_lower.endswith('ë²•') and 'ì‹œí–‰' not in found_lower:
                    score += 300
            else:
                # ê¸°ë³¸ ìš°ì„ ìˆœìœ„ (ë²•ë¥  > ì‹œí–‰ë ¹ > ì‹œí–‰ê·œì¹™)
                if found_lower.endswith('ë²•') and not ('ì‹œí–‰ë ¹' in found_lower or 'ì‹œí–‰ê·œì¹™' in found_lower):
                    score += 100
                elif 'ì‹œí–‰ë ¹' in found_lower:
                    score += 50
                elif 'ì‹œí–‰ê·œì¹™' in found_lower:
                    score += 25

            # 5. ê¸¸ì´ í˜ë„í‹° ì™„í™” (ë„ˆë¬´ ê¸´ ë²•ë ¹ëª…ì€ ì•½ê°„ ê°ì )
            if len(found_name) > 30:
                score -= 30

            if score > best_score:
                best_score = score
                best_law = law_info
        
        if best_law:
            law_id = best_law['id']
            exact_law_name = best_law['name']
        else:
            # í´ë°±: ì²« ë²ˆì§¸ ë²•ë ¹
            law_id = current_laws[0]['id']
            exact_law_name = current_laws[0]['name']

        if not law_id:
            return get_superior_law_content_xml_fallback(law_name)
        
        # 2ë‹¨ê³„: ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        detail_params = {
            'OC': OC,
            'target': 'law',
            'type': 'XML',
            'ID': law_id
        }
        
        detail_response = requests.get(detail_url, params=detail_params, timeout=30)
        if detail_response.status_code != 200:
            return get_superior_law_content_xml_fallback(law_name)

        detail_root = ET.fromstring(detail_response.text)
        
        # 3ë‹¨ê³„: ì„±ê³µì ì¸ ì¶”ì¶œ ë¡œì§ ì ìš© - ì—°ê²°ëœ ë³¸ë¬¸ìœ¼ë¡œ ì²˜ë¦¬
        upper_law_text = ""
        jo_count = 0
        hang_count = 0 
        ho_count = 0
        
        for node in detail_root.iter():
            if node.tag == 'ì¡°ë¬¸ë‚´ìš©' and node.text and node.text.strip():
                content = re.sub(r'<[^>]+>', '', node.text)
                content = content.replace('&nbsp;', ' ').replace('&lt;', '<').replace('&gt;', '>').strip()
                upper_law_text += content + '\n'
                jo_count += 1
            elif node.tag == 'í•­ë‚´ìš©' and node.text and node.text.strip():
                content = re.sub(r'<[^>]+>', '', node.text)
                content = content.replace('&nbsp;', ' ').replace('&lt;', '<').replace('&gt;', '>').strip()
                upper_law_text += '    ' + content + '\n'
                hang_count += 1
            elif node.tag == 'í˜¸ë‚´ìš©' and node.text and node.text.strip():
                content = re.sub(r'<[^>]+>', '', node.text)
                content = content.replace('&nbsp;', ' ').replace('&lt;', '<').replace('&gt;', '>').strip()
                upper_law_text += '        ' + content + '\n'
                ho_count += 1

        if upper_law_text.strip():
            # ìŠ¤ë§ˆíŠ¸ í•„í„°ë§: ì¡°ë¡€ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¶€ë¶„ ìš°ì„  ì¶”ì¶œ
            def smart_filter_content(content, max_length=50000):
                """ì¡°ë¡€ì™€ ê´€ë ¨ì„± ë†’ì€ ë¶€ë¶„ì„ ìš°ì„  ì¶”ì¶œ"""
                lines = content.split('\n')
                
                # ì¡°ë¡€ ê´€ë ¨ í‚¤ì›Œë“œ (ë„ë¡œêµí†µë²• ê´€ë ¨)
                priority_keywords = [
                    'ì‹œì¥', 'êµ°ìˆ˜', 'êµ¬ì²­ì¥', 'ì§€ë°©ìì¹˜ë‹¨ì²´', 'ì¡°ë¡€', 'ì‹œë„', 'ì‹œêµ°êµ¬',
                    'ìœ„ì„', 'ìœ„íƒ', 'ê¶Œí•œ', 'ì‚¬ë¬´', 'ì‹ ê³ ', 'í—ˆê°€', 'ìŠ¹ì¸', 'ì§€ì •',
                    'ì£¼ì°¨', 'ì •ì°¨', 'ê¸ˆì§€', 'ì œí•œ', 'êµ¬ì—­', 'ì‹œì„¤', 'ì„¤ì¹˜', 'ê´€ë¦¬'
                ]
                
                # ìš°ì„ ìˆœìœ„ë³„ë¡œ ë¼ì¸ ë¶„ë¥˜
                high_priority = []
                medium_priority = []
                low_priority = []
                
                for line in lines:
                    line_lower = line.lower()
                    priority_count = sum(1 for keyword in priority_keywords if keyword in line_lower)
                    
                    if priority_count >= 2:
                        high_priority.append(line)
                    elif priority_count >= 1:
                        medium_priority.append(line)
                    else:
                        low_priority.append(line)
                
                # ìš°ì„ ìˆœìœ„ë³„ë¡œ ê²°í•©
                filtered_content = []
                current_length = 0
                
                # 1ë‹¨ê³„: ë†’ì€ ìš°ì„ ìˆœìœ„
                for line in high_priority:
                    if current_length + len(line) < max_length:
                        filtered_content.append(line)
                        current_length += len(line)
                    else:
                        break
                
                # 2ë‹¨ê³„: ì¤‘ê°„ ìš°ì„ ìˆœìœ„
                for line in medium_priority:
                    if current_length + len(line) < max_length:
                        filtered_content.append(line)
                        current_length += len(line)
                    else:
                        break
                
                # 3ë‹¨ê³„: ë‚®ì€ ìš°ì„ ìˆœìœ„ (ê³µê°„ì´ ë‚¨ìœ¼ë©´)
                for line in low_priority:
                    if current_length + len(line) < max_length:
                        filtered_content.append(line)
                        current_length += len(line)
                    else:
                        break
                
                result = '\n'.join(filtered_content)
                if len(content) > len(result):
                    result += "\n\n[... ì¡°ë¡€ ê´€ë ¨ì„±ì´ ë†’ì€ ë¶€ë¶„ì„ ìš°ì„  í‘œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤ ...]"
                
                return result
            
            # ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì ìš© (Gemini 2.0 flash expëŠ” ë” í° ì»¨í…ìŠ¤íŠ¸ ì§€ì›)
            max_length = 80000
            if len(upper_law_text) > max_length:
                truncated_text = smart_filter_content(upper_law_text, max_length)
            else:
                truncated_text = upper_law_text.strip()
            
            # ëª¨ë“  ì¡°ë¬¸ì„ í•˜ë‚˜ì˜ ì—°ê²°ëœ ë³¸ë¬¸ìœ¼ë¡œ ì²˜ë¦¬
            result = {
                'law_name': exact_law_name,
                'law_id': law_id,
                'content': truncated_text
            }

            return result
        else:
            return get_superior_law_content_xml_fallback(law_name)

    except Exception as e:
        return get_superior_law_content_xml_fallback(law_name)

def get_superior_law_content_xml_fallback(law_name):
    """XML ë°©ì‹ í´ë°± (ê°„ì†Œí™” ë²„ì „)"""
    try:
        search_params = {
            'OC': OC,
            'target': 'law',
            'type': 'XML',
            'query': law_name,
            'display': 5,
            'search': 1
        }

        search_response = requests.get(search_url, params=search_params, timeout=30)
        
        if search_response.status_code != 200:
            return None

        if not search_response.text.strip():
            return None

        try:
            search_root = ET.fromstring(search_response.text)
        except ET.ParseError as xml_err:
            return None
        
        law_id = None
        exact_law_name = None
        
        for law in search_root.findall('.//law'):
            found_name = law.find('ë²•ë ¹ëª…').text if law.find('ë²•ë ¹ëª…') is not None else ""
            found_id = law.find('ë²•ë ¹ID').text if law.find('ë²•ë ¹ID') is not None else None
            
            if found_name == law_name or (law_name in found_name):
                law_id = found_id
                exact_law_name = found_name
                break
        
        if not law_id:
            return None
        
        detail_params = {
            'OC': OC,
            'target': 'law', 
            'ID': law_id,
            'type': 'XML'
        }
        
        detail_response = requests.get(detail_url, params=detail_params, timeout=30)
        detail_root = ET.fromstring(detail_response.text)
        
        articles = []
        for article in detail_root.findall('.//ì¡°'):
            article_num = article.find('ì¡°ë¬¸ë²ˆí˜¸').text if article.find('ì¡°ë¬¸ë²ˆí˜¸') is not None else ""
            article_title = article.find('ì¡°ë¬¸ì œëª©').text if article.find('ì¡°ë¬¸ì œëª©') is not None else ""
            article_content = article.find('ì¡°ë¬¸ë‚´ìš©').text if article.find('ì¡°ë¬¸ë‚´ìš©') is not None else ""
            
            if article_content:
                article_content = article_content.replace('<![CDATA[', '').replace(']]>', '')
                article_content = article_content.replace('<p>', '').replace('</p>', '\n')
                article_content = article_content.replace('<br/>', '\n').replace('<br>', '\n')
                article_content = article_content.replace('&nbsp;', ' ')
                article_content = article_content.strip()
                
                if article_content:
                    articles.append({
                        'number': article_num,
                        'title': article_title,
                        'content': article_content
                    })
        
        return {
            'law_name': exact_law_name,
            'law_id': law_id,
            'articles': articles
        }
        
    except Exception as e:
        return None

# ê¸°ì¡´ í•¨ìˆ˜ë¥¼ ìƒˆ XML ë°©ì‹ìœ¼ë¡œ êµì²´
def get_superior_law_content(law_name):
    """ìƒìœ„ë²•ë ¹ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (XML ë°©ì‹)"""
    return get_superior_law_content_xml(law_name)

def normalize_law_name(law_name):
    """ë²•ë ¹ëª…ì„ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ ì œê±°"""
    import re

    # 1. ê¸°ë³¸ ì •ë¦¬: ì•ë’¤ ê³µë°± ì œê±°
    normalized = law_name.strip()

    # 2. ê³¼ë„í•œ ë„ì–´ì“°ê¸° ì œê±° (2ê°œ ì´ìƒì˜ ê³µë°±ì„ 1ê°œë¡œ)
    normalized = re.sub(r'\s+', ' ', normalized)

    # 3. íŠ¹ì • íŒ¨í„´ ì •ê·œí™”
    # "ê´€ê´‘ì§„í¥ ë²•" -> "ê´€ê´‘ì§„í¥ë²•"
    normalized = re.sub(r'(\w+)\s+(ë²•|ë ¹|ê·œì¹™)$', r'\1\2', normalized)

    # 4. íê´‘ì§€ì—­ê°œë°œì§€ì› ê´€ë ¨ ë²•ë ¹ ì •ê·œí™”
    if 'íê´‘ì§€' in normalized or 'ì—­ê°œë°œ' in normalized:
        if 'íŠ¹ë³„ë²•' in normalized:
            normalized = "íê´‘ì§€ì—­ê°œë°œì§€ì›ì—ê´€í•œíŠ¹ë³„ë²•"

    # 5. ë„ˆë¬´ ì§§ì€ ë²•ë ¹ëª… ì œê±° (3ê¸€ì ì´í•˜)
    if len(normalized) <= 3:
        return None

    # 6. ëª…í™•íˆ ì˜ëª»ëœ ì¶”ì¶œ ì œê±°
    invalid_patterns = [
        r'^í•œíŠ¹ë³„ë²•$',  # "í•œíŠ¹ë³„ë²•"
        r'^\w{1,2}íŠ¹ë³„ë²•$',  # ë„ˆë¬´ ì§§ì€ íŠ¹ë³„ë²•
    ]

    for pattern in invalid_patterns:
        if re.match(pattern, normalized):
            return None

    return normalized

def group_laws_by_hierarchy(superior_laws):
    """ë²•ë ¹ì„ ê³„ì¸µë³„ë¡œ ê·¸ë£¹í™”í•˜ëŠ” í•¨ìˆ˜ (ì •ê·œí™” ì ìš©)"""
    law_groups = {}

    # 1ë‹¨ê³„: ë²•ë ¹ëª… ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°
    normalized_laws = set()
    for law_name in superior_laws:
        normalized = normalize_law_name(law_name)
        if normalized:  # Noneì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
            normalized_laws.add(normalized)

    if len(superior_laws) != len(normalized_laws):
        import streamlit as st
        st.info(f"ğŸ”§ ë²•ë ¹ëª… ì •ê·œí™”: {len(superior_laws)}ê°œ â†’ {len(normalized_laws)}ê°œë¡œ ì¤‘ë³µ ì œê±°")

        # ì œê±°ëœ ì¤‘ë³µ ë²•ë ¹ í‘œì‹œ
        removed_laws = []
        for original in superior_laws:
            normalized = normalize_law_name(original)
            if not normalized or (normalized != original and normalized in normalized_laws):
                removed_laws.append(original)

        if removed_laws:
            with st.expander("ğŸ—‘ï¸ ì œê±°ëœ ì¤‘ë³µ/ì˜ëª»ëœ ë²•ë ¹ëª…", expanded=False):
                for removed in removed_laws:
                    st.markdown(f"- {removed}")

    # ì •ê·œí™” ê³¼ì • ë¡œê¹…
    for original in superior_laws:
        normalized = normalize_law_name(original)

    # 2ë‹¨ê³„: ì •ê·œí™”ëœ ë²•ë ¹ëª…ìœ¼ë¡œ ê·¸ë£¹í™”
    for law_name in normalized_laws:
        # ê¸°ë³¸ ë²•ë ¹ëª… ì¶”ì¶œ (ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ ì œê±°)
        base_name = law_name
        law_type = 'law'  # ê¸°ë³¸ê°’: ë²•ë¥ 
        
        if 'ì‹œí–‰ê·œì¹™' in law_name:
            base_name = law_name.replace(' ì‹œí–‰ê·œì¹™', '').replace('ì‹œí–‰ê·œì¹™', '')
            law_type = 'rule'
        elif 'ì‹œí–‰ë ¹' in law_name:
            base_name = law_name.replace(' ì‹œí–‰ë ¹', '').replace('ì‹œí–‰ë ¹', '')
            law_type = 'decree'
        elif law_name.endswith('ë ¹') and not law_name.endswith('ë²•ë ¹'):
            law_type = 'decree'
        elif law_name.endswith('ê·œì¹™'):
            law_type = 'rule'
            
        # ê·¸ë£¹ì— ì¶”ê°€
        if base_name not in law_groups:
            law_groups[base_name] = {'law': None, 'decree': None, 'rule': None}
        
        law_groups[base_name][law_type] = law_name
    
    return law_groups

def get_all_superior_laws_content(superior_laws):
    """ëª¨ë“  ìƒìœ„ë²•ë ¹ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ - ê³„ì¸µë³„ ê·¸ë£¹í™”"""
    superior_laws_content = []
    
    if not superior_laws:
        return superior_laws_content
    
    # 1ë‹¨ê³„: ë²•ë ¹ì„ ê³„ì¸µë³„ë¡œ ê·¸ë£¹í™”
    law_groups = group_laws_by_hierarchy(superior_laws)
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_laws = sum(1 for laws in law_groups.values() for law in laws.values() if law is not None)
    current_idx = 0
    
    # 2ë‹¨ê³„: ê° ê·¸ë£¹ì˜ ëª¨ë“  ê³„ì¸µ ìˆ˜ì§‘
    for base_name, laws in law_groups.items():
        group_content = {
            'base_name': base_name,
            'laws': {},
            'combined_articles': []
        }
        
        # ë²•ë¥  â†’ ì‹œí–‰ë ¹ â†’ ì‹œí–‰ê·œì¹™ ìˆœì„œë¡œ ìˆ˜ì§‘
        for law_type in ['law', 'decree', 'rule']:
            law_name = laws[law_type]
            if law_name:
                current_idx += 1
                status_text.text(f"ìƒìœ„ë²•ë ¹ ì¡°íšŒ ì¤‘... {law_name} ({current_idx}/{total_laws})")
                progress_bar.progress(current_idx / total_laws)
                
                law_content = get_superior_law_content(law_name)
                if law_content:
                    group_content['laws'][law_type] = law_content
                    # ìƒˆë¡œìš´ ë°ì´í„° êµ¬ì¡° ì²˜ë¦¬: contentê°€ ìˆìœ¼ë©´ ì‚¬ìš©, articlesê°€ ìˆìœ¼ë©´ ë³€í™˜
                    if 'content' in law_content:
                        # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì €ì¥
                        if 'combined_content' not in group_content:
                            group_content['combined_content'] = ""
                        group_content['combined_content'] += law_content['content'] + '\n'
                    elif 'articles' in law_content:
                        # ê¸°ì¡´ articles êµ¬ì¡°ê°€ ìˆìœ¼ë©´ ë³€í™˜
                        group_content['combined_articles'].extend(law_content['articles'])
        
        if group_content['laws']:  # í•˜ë‚˜ ì´ìƒì˜ ë²•ë ¹ì´ ìˆ˜ì§‘ëœ ê²½ìš°ë§Œ ì¶”ê°€
            superior_laws_content.append(group_content)
    
    progress_bar.empty()
    status_text.empty()
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (8ë§Œì) ë° ê´€ë ¨ì„± í•„í„°ë§
    max_chars = 80000
    total_chars = 0
    
    # ê° ë²•ë ¹ ê·¸ë£¹ì˜ í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
    for group in superior_laws_content:
        group_chars = 0
        
        # combined_contentê°€ ìˆëŠ” ê²½ìš°
        if 'combined_content' in group and group['combined_content']:
            group_chars += len(group['combined_content'])
        
        # combined_articlesê°€ ìˆëŠ” ê²½ìš°
        if 'combined_articles' in group and group['combined_articles']:
            for article in group['combined_articles']:
                group_chars += len(article.get('content', ''))
        
        # laws êµ¬ì¡°ê°€ ìˆëŠ” ê²½ìš°
        if 'laws' in group and group['laws']:
            for law_type, law_info in group['laws'].items():
                if law_info and 'articles' in law_info:
                    for article in law_info['articles']:
                        group_chars += len(article.get('content', ''))
        
        group['text_length'] = group_chars
        total_chars += group_chars
    
    return superior_laws_content

def chunk_text(text, chunk_size=1000, overlap=200):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜"""
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        end = min(start + chunk_size, text_length)
        chunk = text[start:end]
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ëë‚˜ë„ë¡ ì¡°ì •
        if end < text_length:
            last_period = chunk.rfind('.')
            last_newline = chunk.rfind('\n')
            last_break = max(last_period, last_newline)
            if last_break > start + chunk_size * 0.7:  # ë„ˆë¬´ ì§§ì§€ ì•Šìœ¼ë©´ ì¡°ì •
                end = start + last_break + 1
                chunk = text[start:end]
        
        if chunk.strip():
            chunks.append({
                'text': chunk.strip(),
                'start': start,
                'end': end
            })
        
        start = end - overlap
    
    return chunks

def get_gemini_embedding(text, api_key):
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    try:
        genai.configure(api_key=api_key)
        result = genai.embed_content(
            model="models/embedding-001",
            content=text,
            task_type="retrieval_document"
        )
        return result['embedding']
    except Exception as e:
        st.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return None

def is_valid_text(text):
    """í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì‚¬"""
    if not text or len(text.strip()) < 10:
        return False

    # í•œê¸€ ê¹¨ì§ ê²€ì‚¬ (ê¹¨ì§„ ë¬¸ì ë¹„ìœ¨ì´ 30% ì´ìƒì´ë©´ ì œì™¸)
    broken_chars = sum(1 for char in text if ord(char) > 55000)  # í•œê¸€ ê¹¨ì§ ë¬¸ì ë²”ìœ„
    if len(text) > 0 and broken_chars / len(text) > 0.3:
        return False

    # ì ì„  ê³¼ë‹¤ ê²€ì‚¬ (ì ì„ ì´ 50% ì´ìƒì´ë©´ ì œì™¸)
    dot_chars = text.count('Â·') + text.count('â€¦') + text.count('.')
    if len(text) > 0 and dot_chars / len(text) > 0.5:
        return False

    # ë°˜ë³µ ë¬¸ì ê³¼ë‹¤ ê²€ì‚¬
    import re
    repeated_patterns = re.findall(r'(.)\1{10,}', text)  # ê°™ì€ ë¬¸ìê°€ 10ë²ˆ ì´ìƒ ë°˜ë³µ
    if repeated_patterns:
        return False

    return True

def clean_text_content(text):
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    import re

    # 1. ê³¼ë„í•œ ì ì„  ì œê±°
    text = re.sub(r'[Â·â€¦]{3,}', ' ', text)
    text = re.sub(r'\.{3,}', ' ', text)

    # 2. ê³¼ë„í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)

    # 3. í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ì œê±°
    text = re.sub(r'\b\d+\s*í˜ì´ì§€?\b', '', text)
    text = re.sub(r'\b\d+\s*ìª½?\b', '', text)

    # 4. ëª©ì°¨ ê´€ë ¨ íŒ¨í„´ ì œê±°
    text = re.sub(r'^[IVX]+\.?\s*', '', text, flags=re.MULTILINE)  # ë¡œë§ˆìˆ«ì
    text = re.sub(r'^\d+\.?\s*$', '', text, flags=re.MULTILINE)   # ë‹¨ë… ìˆ«ì

    # 5. ë°˜ë³µë˜ëŠ” íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    text = re.sub(r'[~`!@#$%^&*()_+=\[\]{}|\\:";\'<>?/,-]{5,}', ' ', text)

    return text.strip()

def extract_legal_reasoning_from_analysis(analysis_text):
    """Gemini ë¶„ì„ ê²°ê³¼ì—ì„œ ë²•ì  ê·¼ê±°ì™€ ë…¼ë¦¬ ì¶”ì¶œ"""
    import re

    extracted_context = {
        'legal_basis': [],      # ë²•ì  ê·¼ê±° (ë²•ë ¹, ì¡°í•­)
        'reasoning': [],        # ì¶”ë¡  ê³¼ì •
        'key_concepts': [],     # í•µì‹¬ ê°œë…
        'problem_details': []   # êµ¬ì²´ì ì¸ ë¬¸ì œì 
    }

    # 1. ë²•ë ¹ ë° ì¡°í•­ ì¶”ì¶œ
    legal_references = re.findall(r'(?:ì§€ë°©ìì¹˜ë²•|í—Œë²•|í–‰ì •ê¸°ë³¸ë²•|ê±´ì¶•ë²•|ë„ì‹œê³„íšë²•)\s*(?:ì œ\s*\d+ì¡°?(?:ì˜?\d+)?)?', analysis_text)
    extracted_context['legal_basis'].extend(legal_references)

    # 2. ë²•ì  ì›ì¹™/ê°œë… ì¶”ì¶œ
    legal_concepts = [
        'ê¸°ê´€ìœ„ì„ì‚¬ë¬´', 'ìì¹˜ì‚¬ë¬´', 'êµ­ê°€ì‚¬ë¬´', 'ë²•ë¥ ìœ ë³´ì›ì¹™', 'ê¶Œí•œë°°ë¶„',
        'ìƒìœ„ë²•ë ¹', 'ë²•ë ¹ìš°ìœ„', 'ì¡°ë¡€ì œì •ê¶Œ', 'ìœ„ì„ì…ë²•', 'ì²˜ë¶„ê¶Œí•œ',
        'í—Œë²•ìœ„ë°˜', 'ê¸°ë³¸ê¶Œì¹¨í•´', 'í‰ë“±ì›ì¹™', 'ë¹„ë¡€ì›ì¹™', 'ì‹ ë¢°ë³´í˜¸',
        'ì¬ì‚°ê¶Œì¹¨í•´', 'ì˜ì—…ì˜ììœ ', 'ê±°ì£¼ì´ì „ì˜ììœ ', 'í‘œí˜„ì˜ììœ ',
        'ì¡°ì„¸ë²•ë¥ ì£¼ì˜', 'ì£„í˜•ë²•ì •ì£¼ì˜', 'ì ë²•ì ˆì°¨', 'ì •ë‹¹í•œë³´ìƒ'
    ]

    for concept in legal_concepts:
        if concept in analysis_text:
            # í•´ë‹¹ ê°œë… ì£¼ë³€ ë¬¸ë§¥ ì¶”ì¶œ (ì•ë’¤ 50ì)
            matches = re.finditer(re.escape(concept), analysis_text)
            for match in matches:
                start = max(0, match.start() - 50)
                end = min(len(analysis_text), match.end() + 50)
                context = analysis_text[start:end].strip()
                extracted_context['key_concepts'].append({
                    'concept': concept,
                    'context': context
                })

    # 3. ë¬¸ì œì  ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
    problem_patterns = [
        r'ë¬¸ì œ(?:ì |ê°€|ëŠ”)[^.]*?(?:\.|$)',
        r'ìœ„ë²•[^.]*?(?:\.|$)',
        r'ìœ„ë°˜[^.]*?(?:\.|$)',
        r'ë¶€ì ì ˆ[^.]*?(?:\.|$)',
        r'í•œê³„[^.]*?(?:\.|$)'
    ]

    for pattern in problem_patterns:
        matches = re.findall(pattern, analysis_text, re.DOTALL)
        extracted_context['problem_details'].extend(matches)

    # 4. ì¶”ë¡  ê³¼ì • ì¶”ì¶œ (ë”°ë¼ì„œ, ê·¸ëŸ¬ë¯€ë¡œ, ì™œëƒí•˜ë©´ ë“±)
    reasoning_patterns = [
        r'(?:ë”°ë¼ì„œ|ê·¸ëŸ¬ë¯€ë¡œ|ì™œëƒí•˜ë©´|ì´ëŠ”|ì´ì— ë”°ë¼)[^.]*?(?:\.|$)',
        r'(?:ê·¼ê±°|ì´ìœ |ì›ì¸)ëŠ”[^.]*?(?:\.|$)'
    ]

    for pattern in reasoning_patterns:
        matches = re.findall(pattern, analysis_text, re.DOTALL)
        extracted_context['reasoning'].extend(matches)

    return extracted_context


def detect_agency_delegation(superior_article: Dict, ordinance_article: Dict, source_type: str) -> Dict:
    """ê¸°ê´€ìœ„ì„ì‚¬ë¬´ íŠ¹í™” íŒë³„ í•¨ìˆ˜"""
    
    superior_content = superior_article.get('content', '').lower()
    ordinance_content = ordinance_article.get('content', '').lower()
    
    # 1ë‹¨ê³„: êµ­ê°€ì‚¬ë¬´ì¸ì§€ íŒë³„
    national_affairs_indicators = [
        'ê±´ì¶•í—ˆê°€', 'ê°œë°œí–‰ìœ„í—ˆê°€', 'í™˜ê²½ì˜í–¥í‰ê°€', 'ë„ì‹œê³„íš',
        'ì‚°ì—…ë‹¨ì§€', 'ê´€ê´‘ë‹¨ì§€', 'íƒì§€ê°œë°œ', 'ë„ë¡œê°œì„¤',
        'í•˜ì²œì ìš©', 'ì‚°ì§€ì „ìš©', 'ë†ì§€ì „ìš©', 'ì‚°ì—…ì…ì§€',
        'êµ­í† ê³„íš', 'ì§€ì—­ê³„íš', 'ê´‘ì—­ê³„íš'
    ]
    
    is_national_affair = any(indicator in superior_content for indicator in national_affairs_indicators)
    
    # 2ë‹¨ê³„: ì§€ë°©ìì¹˜ë‹¨ì²´ 'ì¥'ì—ê²Œ ìœ„ì„ë˜ì—ˆëŠ”ì§€ í™•ì¸
    delegation_to_head_indicators = [
        'ì‹œì¥', 'êµ°ìˆ˜', 'êµ¬ì²­ì¥', 'ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ì¥',
        'ì‹œì¥ì´', 'êµ°ìˆ˜ê°€', 'êµ¬ì²­ì¥ì´', 'ì¥ì´',
        'ìœ„ì„í•œë‹¤', 'ìœ„íƒí•œë‹¤'
    ]
    
    is_delegated_to_head = any(indicator in superior_content for indicator in delegation_to_head_indicators)
    
    # 3ë‹¨ê³„: ì¡°ë¡€ê°€ í•´ë‹¹ ì‚¬ë¬´ì— ëŒ€í•´ ë³„ë„ ê·œì •ì„ ë‘ê³  ìˆëŠ”ì§€ í™•ì¸
    ordinance_regulation_indicators = [
        'í—ˆê°€', 'ìŠ¹ì¸', 'ì‹ ê³ ', 'ì¸ê°€', 'ì§€ì •', 'ë“±ë¡',
        'ê¸°ì¤€', 'ì ˆì°¨', 'ë°©ë²•', 'ì¡°ê±´', 'ì œí•œ'
    ]
    
    has_ordinance_regulation = any(indicator in ordinance_content for indicator in ordinance_regulation_indicators)
    
    # 4ë‹¨ê³„: ìœ„ë²•ì„± íŒë‹¨
    is_agency_delegation = False
    severity = "ë‚®ìŒ"
    evidence = []
    description = ""
    
    if is_national_affair and is_delegated_to_head and has_ordinance_regulation:
        is_agency_delegation = True
        severity = "ë§¤ìš° ë†’ìŒ"
        description = "ê¸°ê´€ìœ„ì„ì‚¬ë¬´ì— ëŒ€í•´ ì¡°ë¡€ë¡œ ë³„ë„ ê·œì •ì„ ë‘ì–´ ì§€ë°©ìì¹˜ë²• ì œ22ì¡° ìœ„ë°˜"
        
        evidence.extend([
            f"êµ­ê°€ì‚¬ë¬´ í™•ì¸: {[ind for ind in national_affairs_indicators if ind in superior_content][:2]}",
            f"ì§€ë°©ìì¹˜ë‹¨ì²´ ì¥ ìœ„ì„ í™•ì¸: {[ind for ind in delegation_to_head_indicators if ind in superior_content][:2]}",
            f"ì¡°ë¡€ ë³„ë„ ê·œì • í™•ì¸: {[ind for ind in ordinance_regulation_indicators if ind in ordinance_content][:2]}"
        ])
    
    elif is_national_affair and has_ordinance_regulation:
        # êµ­ê°€ì‚¬ë¬´ì¸ë° ì¡°ë¡€ë¡œ ê·œì •í•œ ê²½ìš° (ìœ„ì„ ëŒ€ìƒ ë¶ˆí™•ì‹¤)
        is_agency_delegation = True
        severity = "ë†’ìŒ"
        description = "êµ­ê°€ì‚¬ë¬´ë¡œ ì¶”ì •ë˜ëŠ” ì‚¬í•­ì— ëŒ€í•´ ì¡°ë¡€ê°€ ë³„ë„ ê·œì •, ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ê°€ëŠ¥ì„±"
        
        evidence.extend([
            f"êµ­ê°€ì‚¬ë¬´ ê°€ëŠ¥ì„±: {[ind for ind in national_affairs_indicators if ind in superior_content][:2]}",
            f"ì¡°ë¡€ ë³„ë„ ê·œì •: {[ind for ind in ordinance_regulation_indicators if ind in ordinance_content][:2]}"
        ])
    
    elif is_delegated_to_head and has_ordinance_regulation:
        # ì§€ë°©ìì¹˜ë‹¨ì²´ ì¥ ìœ„ì„ + ì¡°ë¡€ ê·œì •
        is_agency_delegation = True
        severity = "ë†’ìŒ" 
        description = "ì§€ë°©ìì¹˜ë‹¨ì²´ ì¥ì—ê²Œ ìœ„ì„ëœ ì‚¬ë¬´ì— ëŒ€í•´ ì¡°ë¡€ë¡œ ë³„ë„ ê·œì •"
        
        evidence.extend([
            f"ì§€ë°©ìì¹˜ë‹¨ì²´ ì¥ ìœ„ì„: {[ind for ind in delegation_to_head_indicators if ind in superior_content][:2]}",
            f"ì¡°ë¡€ ë³„ë„ ê·œì •: {[ind for ind in ordinance_regulation_indicators if ind in ordinance_content][:2]}"
        ])
    
    return {
        'is_agency_delegation': is_agency_delegation,
        'description': description,
        'evidence': evidence,
        'severity': severity,
        'national_affair': is_national_affair,
        'delegated_to_head': is_delegated_to_head,
        'has_regulation': has_ordinance_regulation
    }

def analyze_ordinance_vs_superior_laws(pdf_text, superior_laws_content):
    """ì¡°ë¡€ì™€ ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ í•¨ìˆ˜ - ê³„ì¸µë³„ í†µí•© ê²€í† """
    analysis_results = []
    
    if not superior_laws_content:
        return "ìƒìœ„ë²•ë ¹ ì •ë³´ê°€ ì—†ì–´ ì§ì ‘ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ì¡°ë¡€ì—ì„œ ì‚¬ë¬´ ê´€ë ¨ ì¡°ë¬¸ ì¶”ì¶œ
    ordinance_provisions = []
    lines = pdf_text.split('\n')
    current_article = ""
    current_content = ""
    
    for line in lines:
        line = line.strip()
        if line.startswith('ì œ') and 'ì¡°' in line:
            if current_article:
                ordinance_provisions.append({
                    'article': current_article,
                    'content': current_content.strip()
                })
            current_article = line
            current_content = ""
        else:
            current_content += line + " "
    
    # ë§ˆì§€ë§‰ ì¡°ë¬¸ ì¶”ê°€
    if current_article:
        ordinance_provisions.append({
            'article': current_article,
            'content': current_content.strip()
        })
    
    # ìƒìœ„ë²•ë ¹ê³¼ ì§ì ‘ ë¹„êµ ë¶„ì„
    comparison_results = []
    
    for ordinance_provision in ordinance_provisions:
        if not ordinance_provision['content']:
            continue
            
        provision_analysis = {
            'ordinance_article': ordinance_provision['article'],
            'ordinance_content': ordinance_provision['content'],
            'superior_law_conflicts': [],
            'delegation_issues': [],
            'authority_issues': []
        }
        
        # ê° ìƒìœ„ë²•ë ¹ ê·¸ë£¹ê³¼ ë¹„êµ (ë²•ë¥ , ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ í†µí•©)
        for law_group in superior_laws_content:
            base_name = law_group['base_name']
            
            # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš° ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ë§Œ ìˆ˜í–‰
            if 'combined_content' in law_group:
                superior_content_lower = law_group['combined_content'].lower()
                ordinance_lower = ordinance_provision['content'].lower()
                
                # í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± í™•ì¸
                common_keywords = []
                for word in ordinance_lower.split():
                    if len(word) > 2 and word in superior_content_lower:
                        common_keywords.append(word)
                
                if len(common_keywords) > 2:  # ìµœì†Œ 3ê°œ ì´ìƒì˜ ê³µí†µ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê´€ë ¨ì„± ìˆìŒ
                    # ê°„ë‹¨í•œ ë¶„ì„ë§Œ ìˆ˜í–‰
                    continue
                else:
                    continue
            
            # ê¸°ì¡´ ë°©ì‹ - articlesê°€ ìˆëŠ” ê²½ìš°
            for superior_article in law_group.get('combined_articles', []):
                superior_content = superior_article['content'].lower()
                ordinance_lower = ordinance_provision['content'].lower()
                
                # ì–´ëŠ ê³„ì¸µ(ë²•ë¥ /ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™)ì—ì„œ ë‚˜ì˜¨ ì¡°ë¬¸ì¸ì§€ í™•ì¸
                article_source = "ë²•ë¥ "  # ê¸°ë³¸ê°’
                for law_type, law_info in law_group['laws'].items():
                    if law_info and 'articles' in law_info:
                        for article in law_info['articles']:
                            if article['content'] == superior_article['content']:
                                if law_type == 'law':
                                    article_source = "ë²•ë¥ "
                                elif law_type == 'decree':
                                    article_source = "ì‹œí–‰ë ¹"
                                elif law_type == 'rule':
                                    article_source = "ì‹œí–‰ê·œì¹™"
                                break
                
                # ğŸ†• íŠ¹í™”ëœ ê¸°ê´€ìœ„ì„ì‚¬ë¬´ íŒë³„ ë¡œì§
                agency_delegation_result = detect_agency_delegation(
                    superior_article, ordinance_article, article_source
                )
                
                if agency_delegation_result['is_agency_delegation']:
                    provision_analysis['delegation_issues'].append({
                        'superior_law': f"{base_name} ({article_source})",
                        'superior_article': f"{superior_article['number']} {superior_article['title']}",
                        'superior_content': superior_article['content'],
                        'issue_type': 'ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ìœ„ë°˜',
                        'description': agency_delegation_result['description'],
                        'evidence': agency_delegation_result['evidence'],
                        'severity': agency_delegation_result['severity'],
                        'hierarchy': article_source
                    })
                
                # ì§ì ‘ì ì¸ ì¶©ëŒ ê²€ì‚¬ - ê³„ì¸µë³„ ìœ„ë°˜ ì‹¬ê°ë„ êµ¬ë¶„
                conflict_indicators = [
                    ('ê¸ˆì§€', 'í—ˆìš©'), ('ì˜ë¬´', 'ë©´ì œ'), ('í•„ìˆ˜', 'ì„ íƒ'),
                    ('ê°•ì œ', 'ì„ì˜'), ('ë°˜ë“œì‹œ', 'ê°€ëŠ¥'), ('ë¶ˆê°€', 'í—ˆìš©')
                ]
                
                for prohibit_word, allow_word in conflict_indicators:
                    if prohibit_word in superior_content and allow_word in ordinance_lower:
                        # ê³„ì¸µë³„ ìœ„ë°˜ ì‹¬ê°ë„
                        severity = "ì‹¬ê°" if article_source == "ë²•ë¥ " else ("ë³´í†µ" if article_source == "ì‹œí–‰ë ¹" else "ê²½ë¯¸")
                        
                        provision_analysis['superior_law_conflicts'].append({
                            'superior_law': f"{base_name} ({article_source})",
                            'superior_article': f"{superior_article['number']} {superior_article['title']}",
                            'conflict_type': f'{article_source} {prohibit_word} vs ì¡°ë¡€ {allow_word}',
                            'superior_content': superior_article['content'],
                            'potential_violation': True,
                            'hierarchy': article_source,
                            'severity': severity
                        })
        
        if provision_analysis['delegation_issues'] or provision_analysis['superior_law_conflicts']:
            comparison_results.append(provision_analysis)
    
    return comparison_results

def create_analysis_prompt(pdf_text, search_results, superior_laws_content=None, relevant_guidelines=None, is_first_ordinance=False, comprehensive_analysis_results=None, theoretical_results=None):
    """ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜"""
    prompt = (
        "ğŸš¨ **í•„ë…: ì¡°ë¡€ ìœ„ë²• íŒë‹¨ ë²•ë¦¬ì  ê°€ì´ë“œë¼ì¸ (ìµœìš°ì„  ì¤€ìˆ˜ ì‚¬í•­)**\n\n"
        "**1. ì¡°ë¡€ ìœ„ë²• íŒë‹¨ì˜ ì›ì¹™**\n"
        "- ì¡°ë¡€ëŠ” ìì¹˜ì…ë²•ê¶Œì— ë”°ë¼ í­ë„“ì€ ì¬ëŸ‰ì„ ê°€ì§€ë©°, ë²•ë¥ ê³¼ ìƒì¶©í•˜ê±°ë‚˜ ì£¼ë¯¼ì˜ ê¶Œë¦¬ë¥¼ ì¹¨í•´í•˜ì§€ ì•ŠëŠ” ì´ìƒ ìœ„ë²•ìœ¼ë¡œ ë³´ì§€ ì•ŠëŠ”ë‹¤.\n"
        "- ì¡°ë¡€ëŠ” ë²•ë¥ ìœ ë³´, í¬ê´„ìœ„ì„ê¸ˆì§€, ì ˆì°¨ì  ìœ„ë²• ë“± ìš”ê±´ì´ ì—„ê²©í•  ë•Œë§Œ ìœ„ë²•ì„±ì´ ë¬¸ì œëœë‹¤.\n"
        "- ë‹¨ìˆœí•œ í‘œí˜„ ë°©ì‹, ê´€í–‰ê³¼ì˜ ì°¨ì´, íƒ€ ì§€ìì²´ì™€ì˜ ë¹„êµë§Œìœ¼ë¡œ ìœ„ë²• íŒë‹¨ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n"
        "**2. ìë™í•´ì‚°Â·í¬ê´„ì¡°í•­ ë“± ìì¹˜ë‹¨ì²´ ì¡°ë¡€ì—ì„œ í”íˆ ì“°ëŠ” ê·œì •ì€ ì›ì¹™ì ìœ¼ë¡œ í—ˆìš©**\n"
        "- íŠ¹ì •ì•ˆê±´í˜•Â·ë¹„ìƒì„¤ ìœ„ì›íšŒì— ëŒ€í•œ ìë™ êµ¬ì„±Â·ìë™ í•´ì‚° ê·œì •ì€ ì¡°ë¡€ê°€ ì •í•œ ì ˆì°¨ì— ë”°ë¥¸ ê²ƒìœ¼ë¡œ ë³´ë©° ìœ„ë²•ìœ¼ë¡œ íŒë‹¨í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
        "- 'ê·¸ ë°–ì— ì‹œì¥ì´ í•„ìš”í•˜ë‹¤ê³  ì¸ì •í•˜ëŠ” ê²½ìš°' ë“± ë³´ì¶©ì  í¬ê´„ì¡°í•­ì€ ê¶Œë¦¬Â·ì˜ë¬´ë¥¼ ì œí•œí•˜ì§€ ì•ŠëŠ” í•œ ìì¹˜ì…ë²•ê¶Œ ë²”ìœ„ì—ì„œ í—ˆìš©ëœë‹¤.\n"
        "- ì´ë¥¼ ë²•ë¥ ìœ ë³´Â·í¬ê´„ìœ„ì„ê¸ˆì§€ë¡œ íŒë‹¨í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n"
        "**3. ìë¬¸ìœ„ì›íšŒì— ëŒ€í•´ì„œëŠ” 'ì ˆì°¨ê¶Œ', 'êµ¬ì„±ê¶Œ' ê°œë…ì„ ì ìš©í•˜ì§€ ì•Šê¸°**\n"
        "- ìœ„ì›íšŒëŠ” ìì¹˜ë‹¨ì²´ì¥ì˜ ìë¬¸ê¸°êµ¬ì´ë©°, ìœ„ì›íšŒ ì¡´íëŠ” ì£¼ë¯¼ì˜ ì ˆì°¨ê¶Œ ë˜ëŠ” ê¶Œìµê³¼ ì§ì ‘ ê´€ë ¨ë˜ì§€ ì•ŠëŠ”ë‹¤.\n"
        "- ì ˆì°¨ê¶Œ ì¹¨í•´, êµ¬ì„±ê¶Œ ì¹¨í•´ ë“±ì˜ ê°œë…ì„ ìë¬¸ìœ„ì›íšŒì— ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n"
        "**4. ìœ„ë²• íŒë‹¨ì€ ë°˜ë“œì‹œ 'ë²•ë¥  ìƒìœ„ê·œë²”ê³¼ì˜ ëª…í™•í•œ ì¶©ëŒ'ì´ ìˆì„ ë•Œë§Œ í•¨**\n"
        "- ìœ„ë²• íŒë‹¨ì„ í•  ë•ŒëŠ” ë°˜ë“œì‹œ ë‹¤ìŒì„ ì¶©ì¡±í•  ë•Œë§Œ ìœ„ë²•ì„±ì„ ì§€ì í•œë‹¤:\n"
        "  1) ì¡°ë¡€ê°€ ìƒìœ„ë²•ì˜ ëª…ë ¹Â·ê¸ˆì§€ë¥¼ ëª…ë°±íˆ ìœ„ë°˜í•˜ëŠ” ê²½ìš°\n"
        "  2) ì£¼ë¯¼ì˜ ê¶Œë¦¬ë¥¼ ì œí•œí•˜ê±°ë‚˜ ì˜ë¬´ë¥¼ ë¶€ê³¼í•˜ë©´ì„œ ë²•ì  ê·¼ê±°ê°€ ëª…í™•íˆ ì—†ëŠ” ê²½ìš°\n"
        "  3) ìì¹˜ì‚¬ë¬´ê°€ ì•„ë‹Œ êµ­ê°€ì‚¬ë¬´ë¥¼ ì¹¨í•´í•˜ëŠ” ê²½ìš°\n"
        "- ê·¸ ì™¸ì—ëŠ” 'ìœ„ë²• ê°€ëŠ¥ì„± ìˆìŒ'ì´ë¼ê³  íŒë‹¨í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n"
        "---\n\n"
        "ğŸš¨ **ì¤‘ìš” ë¯¸ì…˜: ì‹¤ì œ ìœ„ë²• ë‚´ìš© ì°¾ê¸°**\n"
        "ë„ˆëŠ” ì¡°ë¡€ ìœ„ë²•ì„± ì „ë¬¸ ê²€í† ê´€ì´ë‹¤. ì¼ë°˜ì ì¸ ë²•ë¦¬ ì„¤ëª…ì´ ì•„ë‹ˆë¼ **êµ¬ì²´ì ì¸ ìœ„ë²• ì‚¬í•­ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒ**ì´ ëª©í‘œë‹¤.\n"
        "ìƒìœ„ë²•ë ¹ê³¼ ì¡°ë¡€ë¥¼ ì¡°ë¬¸ ëŒ€ ì¡°ë¬¸ìœ¼ë¡œ ì§ì ‘ ë¹„êµí•˜ì—¬ ì‹¤ì œ ì¶©ëŒí•˜ëŠ” ë¶€ë¶„ì„ ì°¾ì•„ë¼.\n\n"
        "**ê²€í†  ì›ì¹™:**\n"
        "- âŒ 'ì´ëŸ° ë‚´ìš©ì´ ìˆìœ¼ë©´ ìœ„ë²•í•˜ë‹¤'ëŠ” ì¼ë°˜ë¡  ê¸ˆì§€\n"
        "- âœ… 'ì¡°ë¡€ ì œ3ì¡°ëŠ” ë„ë¡œêµí†µë²• ì œ12ì¡°ì™€ ì´ë ‡ê²Œ ì¶©ëŒí•œë‹¤'ëŠ” êµ¬ì²´ì  ì§€ì  í•„ìˆ˜\n"
        "- âœ… ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¶€ë¶„ë„ ë°˜ë“œì‹œ ì–¸ê¸‰ (ë‹¨, ìœ„ ê°€ì´ë“œë¼ì¸ 1~4ë¥¼ ì¤€ìˆ˜í•˜ì—¬ ì‹ ì¤‘íˆ íŒë‹¨)\n"
        "- âœ… ìœ„ë²•ì´ ì—†ìœ¼ë©´ 'ìœ„ë²• ì‚¬í•­ ì—†ìŒ'ìœ¼ë¡œ ëª…í™•íˆ ê²°ë¡ \n\n"
        "ì•„ë˜ëŠ” ë‚´ê°€ ì—…ë¡œë“œí•œ ì¡°ë¡€ PDFì˜ ì „ì²´ ë‚´ìš©ì´ì•¼.\n"
        "---\n"
        f"{pdf_text}\n"
        "---\n"
    )
    
    # ìƒìœ„ë²•ë ¹ ë‚´ìš© ì¶”ê°€ (ê³„ì¸µë³„ ê·¸ë£¹í™”)
    if superior_laws_content:
        prompt += "\nê·¸ë¦¬ê³  ì•„ë˜ëŠ” ì¡°ë¡€ì•ˆì—ì„œ ì–¸ê¸‰ëœ ìƒìœ„ë²•ë ¹ë“¤ì˜ ì‹¤ì œ ì¡°ë¬¸ ë‚´ìš©ì´ì•¼. (ë²•ë¥ , ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ì„ ê³„ì¸µë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í†µí•© ë¶„ì„)\n"
        prompt += "---\n"
        for law_group in superior_laws_content:
            base_name = law_group['base_name']
            prompt += f"â—† {base_name}\n"
            
            # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©
            if 'combined_content' in law_group:
                prompt += f"  ë³¸ë¬¸ ë‚´ìš©:\n{law_group['combined_content']}\n"
            else:
                # ê¸°ì¡´ ë°©ì‹ - ê° ê³„ì¸µë³„ ë²•ë ¹ í‘œì‹œ
                for law_type, law_info in law_group['laws'].items():
                    if law_info and 'articles' in law_info:
                        type_name = "ë²•ë¥ " if law_type == 'law' else ("ì‹œí–‰ë ¹" if law_type == 'decree' else "ì‹œí–‰ê·œì¹™")
                        prompt += f"  [{type_name}] {law_info['law_name']}\n"
                
                # í†µí•©ëœ ì¡°ë¬¸ í‘œì‹œ (ìƒìœ„ 15ê°œë§Œ)
                prompt += f"  í†µí•© ì¡°ë¬¸ ({len(law_group['combined_articles'])}ê°œ):\n"
                for article in law_group['combined_articles'][:15]:  
                    prompt += f"    {article['number']} {article['title']}\n"
                    prompt += f"    {article['content']}\n\n"
        prompt += "---\n"
        
        # ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        try:
            comparison_results = analyze_ordinance_vs_superior_laws(pdf_text, superior_laws_content)
            if comparison_results and isinstance(comparison_results, list) and len(comparison_results) > 0:
                prompt += "\n**ì¤‘ìš”: ì¡°ë¡€ì™€ ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ ê²°ê³¼**\n"
                prompt += "ì•„ë˜ëŠ” ì¡°ë¡€ ì¡°ë¬¸ê³¼ ìƒìœ„ë²•ë ¹ì„ í•˜ë‚˜ì”© ì§ì ‘ ë¹„êµí•œ ê²°ê³¼ì´ë‹¤. ì´ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ì—¬ë¶€ì™€ ë²•ë ¹ìœ„ë°˜ ê°€ëŠ¥ì„±ì„ ì •í™•íˆ íŒë‹¨í•´ì¤˜.\n"
                prompt += "---\n"
                
                for result in comparison_results:
                    prompt += f"â—† {result['ordinance_article']}\n"
                    prompt += f"ì¡°ë¡€ ë‚´ìš©: {result['ordinance_content'][:200]}...\n"
                    
                    if result['delegation_issues']:
                        prompt += "âš ï¸ ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ê°€ëŠ¥ì„± ë°œê²¬:\n"
                        for issue in result['delegation_issues']:
                            prompt += f"  - {issue['superior_law']} {issue['superior_article']}\n"
                            prompt += f"    ë¬¸ì œ: {issue['description']}\n"
                    
                    if result['superior_law_conflicts']:
                        prompt += "ğŸš¨ ìƒìœ„ë²•ë ¹ ì¶©ëŒ ê°€ëŠ¥ì„± ë°œê²¬:\n"
                        for conflict in result['superior_law_conflicts']:
                            prompt += f"  - {conflict['superior_law']} {conflict['superior_article']}\n"
                            prompt += f"    ì¶©ëŒ: {conflict['conflict_type']}\n"
                    
                    prompt += "\n"
                prompt += "---\n"
        except Exception as e:
            prompt += f"\nìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n"
    
    # ìì¹˜ë²•ê·œ ê°€ì´ë“œë¼ì¸ ë° ì‚¬ë¡€ ì¶”ê°€
    if relevant_guidelines:
        prompt += "\nê·¸ë¦¬ê³  ì•„ë˜ëŠ” ìì¹˜ë²•ê·œ ê´€ë ¨ ìë£Œì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ë‚´ìš©ì´ì•¼.\n"
        prompt += "**ì¤‘ìš”**: ì†Œê´€ì‚¬ë¬´ì˜ ì›ì¹™, ë²•ë¥ ìœ ë³´ì˜ ì›ì¹™, ë²•ë ¹ìš°ìœ„ì˜ ì›ì¹™ ë“± ë¶€ë¶„ì— ìˆì–´ ì¡°ê¸ˆì´ë¼ë„ ë¬¸ì œê°€ ë  ê²ƒ ê°™ì€ ë¶€ë¶„ì´ ìˆë‹¤ë©´,\n"
        prompt += "ì•„ë˜ ìë£Œì— ìˆ˜ë¡ëœ ì˜ˆì „ì— ë¬¸ì œê°€ ë˜ì—ˆë˜ ì‚¬ë¡€ì™€ ê²€í†  ê¸°ì¤€ì„ ìì„¸íˆ ì°¸ì¡°í•´ì„œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜.\n"
        prompt += "---\n"
        
        # ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‘œì‹œ
        source_groups = {}
        for guideline in relevant_guidelines:
            source_store = guideline.get('source_store', 'ì•Œ ìˆ˜ ì—†ëŠ” ìë£Œ')
            if source_store not in source_groups:
                source_groups[source_store] = []
            source_groups[source_store].append(guideline)
        
        for source_store, guidelines in source_groups.items():
            prompt += f"â—† ì°¸ê³ ìë£Œ: {source_store}\n"
            for i, guideline in enumerate(guidelines):
                similarity_score = guideline.get('similarity', 1-guideline.get('distance', 0))
                prompt += f"  [{i+1}] (ìœ ì‚¬ë„: {similarity_score:.3f})\n"
                prompt += f"  {guideline['text']}\n\n"
        prompt += "---\n"
    
    # ì¢…í•© ìœ„ë²•ì„± íŒë¡€ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
    if comprehensive_analysis_results and isinstance(comprehensive_analysis_results, list) and len(comprehensive_analysis_results) > 0:
        total_risks = sum(len(result['violation_risks']) for result in comprehensive_analysis_results)
        prompt += f"\n**ğŸš¨ ì¤‘ìš”: ì¢…í•© ì¡°ë¡€ ìœ„ë²•ì„± íŒë¡€ ì ìš© ê²°ê³¼ ({total_risks}ê°œ ìœ„í—˜)**\n"
        prompt += "ì°¸ê³  ìë£Œì—ì„œ ê²€ìƒ‰ëœ ì‹¤ì œ ì¡°ë¡€ ìœ„ë²• íŒë¡€ë“¤(ê¸°ê´€ìœ„ì„ì‚¬ë¬´, ìƒìœ„ë²•ë ¹ ìœ„ë°°, ë²•ë¥ ìœ ë³´ ìœ„ë°°, ê¶Œí•œë°°ë¶„ ìœ„ë°° ë“±)ì„\n"
        prompt += "í˜„ì¬ ì¡°ë¡€ì— ì§ì ‘ ì ìš©í•œ ë¶„ì„ ê²°ê³¼ì´ë‹¤. ì´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ìœ í˜•ë³„ ìœ„ë²•ì„±ì„ ì •í™•íˆ íŒë‹¨í•˜ê³  êµ¬ì²´ì ì¸ ê°œì„ ë°©ì•ˆì„ ì œì‹œí•´ì¤˜.\n"
        prompt += "---\n"
        
        for result in comprehensive_analysis_results:
            prompt += f"â—† {result['ordinance_article']}\n"
            prompt += f"ì¡°ë¡€ ë‚´ìš©: {result['ordinance_content'][:150]}...\n"
            
            for i, risk in enumerate(result['violation_risks'][:2]):  # ìƒìœ„ 2ê°œë§Œ í¬í•¨
                prompt += f"  ìœ„í—˜ {i+1}: {risk['violation_type']} (ìœ„í—˜ë„: {risk['risk_score']:.2f}/1.0)\n"
                prompt += f"  ê´€ë ¨ íŒë¡€: {risk['case_summary'][:150]}...\n"
                if risk['legal_principle'] != "í•´ë‹¹ì—†ìŒ":
                    prompt += f"  ë²•ì  ì›ì¹™: {risk['legal_principle']}\n"
                prompt += f"  ê°œì„  ê¶Œê³ : {risk['recommendation']}\n"
                prompt += f"  íŒë¡€ ì¶œì²˜: {risk['case_source']}\n\n"
            
            if len(result['violation_risks']) > 2:
                prompt += f"  ...ì™¸ {len(result['violation_risks']) - 2}ê°œ ì¶”ê°€ ìœ„í—˜\n\n"
        prompt += "---\n"

    # ğŸ†• ê²€ìƒ‰ëœ ê´€ë ¨ íŒë¡€/ì´ë¡  ì¶”ê°€
    if theoretical_results and isinstance(theoretical_results, list) and len(theoretical_results) > 0:
        prompt += f"\n**ğŸ“š ì¤‘ìš”: ë°œê²¬ëœ ë¬¸ì œì  ê´€ë ¨ íŒë¡€/ì´ë¡  ({len(theoretical_results)}ê°œ)**\n"
        prompt += "ì´ëŠ” 1ì°¨ ë¶„ì„ì—ì„œ ë°œê²¬ëœ ë¬¸ì œì ë“¤ê³¼ ì§ì ‘ ê´€ë ¨ëœ íŒë¡€ì™€ ë²•ë¦¬ì´ë‹¤.\n"
        prompt += "ì•„ë˜ íŒë¡€ë“¤ì„ ì°¸ê³ í•˜ì—¬ í˜„ì¬ ì¡°ë¡€ì˜ ìœ„ë²•ì„±ì„ ì •í™•íˆ íŒë‹¨í•˜ê³  êµ¬ì²´ì ì¸ ê°œì„ ë°©ì•ˆì„ ì œì‹œí•´ì¤˜.\n"
        prompt += "---\n"

        for i, theory in enumerate(theoretical_results[:5]):  # ìƒìœ„ 5ê°œë§Œ í¬í•¨
            context_rel = theory.get('context_relevance', 0)
            matched_concepts = theory.get('matched_concepts', [])
            similarity = theory.get('similarity', 0)

            prompt += f"â—† ê´€ë ¨ íŒë¡€/ì´ë¡  {i+1} (ê´€ë ¨ë„: {context_rel:.2f}, ìœ ì‚¬ë„: {similarity:.2f})\n"
            if matched_concepts:
                prompt += f"ê´€ë ¨ ê°œë…: {', '.join(matched_concepts)}\n"

            # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (300ìë¡œ ì œí•œ)
            content = theory.get('content', theory.get('text', 'ë‚´ìš© ì—†ìŒ'))
            content_preview = content[:300] + "..." if len(content) > 300 else content
            prompt += f"ë‚´ìš©: {content_preview}\n\n"

        prompt += "**âš ï¸ ì¤‘ìš”**: ìœ„ íŒë¡€ë“¤ì€ ì¡°ë¡€ì˜ ë¬¸ì œì ê³¼ ì§ì ‘ ê´€ë ¨ì´ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ê·¼ê±°ë¡œ í˜„ì¬ ì¡°ë¡€ì˜ ìœ„ë²•ì„±ì„ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì í•˜ê³  ê°œì„ ë°©ì•ˆì„ ì œì‹œí•˜ë¼.\n"
        prompt += "---\n"

    if is_first_ordinance:
        prompt += (
            "â€» ì°¸ê³ : ì´ ì¡°ë¡€ëŠ” 17ê°œ ì‹œë„ ì¤‘ ìµœì´ˆë¡œ ì œì •ë˜ëŠ” ì¡°ë¡€ë¡œ, íƒ€ì‹œë„ ì¡°ë¡€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            "íƒ€ì‹œë„ ì¡°ë¡€ê°€ ì—†ëŠ” ìƒí™©ì—ì„œ, ì•„ë˜ ê¸°ì¤€ì— ë”°ë¼ ì¡°ë¡€ì˜ ì ì •ì„±, ìƒìœ„ë²•ë ¹ê³¼ì˜ ê´€ê³„, ì‹¤ë¬´ì  ê²€í†  í¬ì¸íŠ¸ ë“±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•´ì¤˜.\n"
        )
    else:
        prompt += "ê·¸ë¦¬ê³  ì•„ë˜ëŠ” íƒ€ì‹œë„ ì¡°ë¡€ëª…ê³¼ ê° ì¡°ë¬¸ ë‚´ìš©ì´ì•¼.\n"
        for result in search_results:
            prompt += f"ì¡°ë¡€ëª…: {result['name']}\n"
            for idx, article in enumerate(result['content']):
                prompt += f"ì œ{idx+1}ì¡°: {article}\n"
    
    prompt += (
        "---\n"
        "ì•„ë˜ ê¸°ì¤€ì— ë”°ë¼ ë¶„ì„í•´ì¤˜. ë°˜ë“œì‹œ í•œê¸€ë¡œ ë‹µë³€í•´ì¤˜.\n"
        "1. [ë¹„êµë¶„ì„ ìš”ì•½í‘œ(ì¡°ë¬¸ë³„)]\n"
        "- í‘œì˜ ì»¬ëŸ¼: ì¡°ë¬¸(ë‚´ ì¡°ë¡€), ì£¼ìš” ë‚´ìš©, íƒ€ ì‹œë„ ìœ ì‚¬ ì¡°í•­, ë™ì¼ ì—¬ë¶€, ì°¨ì´ ë° ë‚´ ì¡°ë¡€ íŠ¹ì§•, ì¶”ì²œ ì¡°ë¬¸\n"
        "- ë°˜ë“œì‹œ ë‚´ ì¡°ë¡€(PDFë¡œ ì—…ë¡œë“œí•œ ì¡°ë¡€)ì˜ ì¡°ë¬¸ë§Œì„ ê¸°ì¤€ìœ¼ë¡œ, ê° ì¡°ë¬¸ë³„ë¡œ íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµí•´ í‘œë¡œ ì •ë¦¬(ë‚´ ì¡°ë¡€ì— ì—†ëŠ” ì¡°ë¬¸ì€ ë¹„êµí•˜ì§€ ë§ ê²ƒ)\n"
        "- 'ì¶”ì²œ ì¡°ë¬¸' ì¹¸ì—ëŠ” íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµí•´ ë¬´ë‚œí•˜ê²Œ ìƒê°ë˜ëŠ” ì¡°ë¬¸ ì˜ˆì‹œë¥¼ í•œê¸€ë¡œ ì‘ì„±\n\n"
        "2. [ë‚´ ì¡°ë¡€ì˜ ì°¨ë³„ì  ìš”ì•½] (ë³„ë„ ì†Œì œëª©)\n"
        "- íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµí•´ ë…íŠ¹í•˜ê±°ë‚˜ êµ¬ì¡°ì ìœ¼ë¡œ ë‹¤ë¥¸ ì , ë‚´ ì¡°ë¡€ë§Œì˜ ê´€ë¦¬/ìš´ì˜ ë°©ì‹ ë“± ìš”ì•½\n\n"
        "3. [ê²€í†  ì‹œ ìœ ì˜ì‚¬í•­] (ë³„ë„ ì†Œì œëª©)\n"
        "ê° í•­ëª©ë§ˆë‹¤ ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰¬ìš´ ë§ë¡œ ë¶€ì—°ì„¤ëª…ë„ í•¨ê»˜ ì‘ì„±í•´ì¤˜.\n"
        "ë‹¤ìŒ ì›ì¹™ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ê²€í† í•´ì¤˜:\n"
        "a) ì†Œê´€ì‚¬ë¬´ì˜ ì›ì¹™ - *ê¸°ê´€ìœ„ì„ì‚¬ë¬´ëŠ” ì¡°ë¡€ ì œì • ê¸ˆì§€**\n"
        " ìì¹˜ì‚¬ë¬´ì˜ ì˜ˆì‹œëŠ” ì§€ë°©ìì¹˜ë²• ì œ13ì¡°ì œ2í•­ì— ì—´ê±° ë˜ì–´ ìˆìŒ**\n"
        " ê°œë³„ ë²•ë ¹ì—ì„œ êµ­ê°€ ë˜ëŠ” ì¤‘ì•™í–‰ì •ê¸°ê´€ì˜ ì¥ì„ ê¶Œí•œ ì£¼ì²´ë¡œ ì •í•˜ê³  ìˆëŠ” ê²½ìš° êµ­ê°€ì‚¬ë¬´ë¡œ ë³´ì•„ì•¼ í•¨. êµ­ê°€ì‚¬ë¬´ì— ê´€í•œ ì‚¬í•­ì„ ê·œì •í•œ ì¡°ë¡€ëŠ” ìœ„ë²•. êµ­ê°€ì‚¬ë¬´ ì—¬ë¶€ë¥¼ íŒë‹¨í•¨ì— ìˆì–´ì„œ ì§€ë°©ìì¹˜ë²• ì œ15ì¡°ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŒ. ë‹¤ë§Œ, ë²•ë ¹ì—ì„œ ì¼ì • ì‚¬í•­ì„ ì¡°ë¡€ë¡œ ì •í•  ìˆ˜ ìˆë‹¤ê³  ê·œì •í•œë‹¤ë©´ ê·¸ ì‚¬ë¬´ê°€ êµ­ê°€ì‚¬ë¬´ë‚˜ ìì¹˜ì‚¬ë¬´ ê´€ê³„ì—†ì´ ì¡°ë¡€ ì œì • ê°€ëŠ¥**\n"
        " ì§€ë°©ìì¹˜ë‹¨ì²´ ë˜ëŠ” ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ì¥ì„ ê¶Œí•œì£¼ì²´ë¡œ ì •í•˜ê³  ìˆëŠ” ê²½ìš° ìì¹˜ì‚¬ë¬´ë¡œ ë³´ì•„ì•¼ í•¨**\n"
        " ë²•ë ¹ì— êµ­ê°€ì™€ ì§€ë°©ìì¹˜ë‹¨ì²´ë¥¼ ì‚¬ë¬´ ìˆ˜í–‰ì˜ ì£¼ì²´ë¡œ ë³‘ë ¬ì ìœ¼ë¡œ ê·œì •í•˜ëŠ” ê²½ìš° êµ­ê°€ì‚¬ë¬´ì™€ ìì¹˜ì‚¬ë¬´ ì„±ì§ˆì„ ëª¨ë‘ ê°€ì§€ë¯€ë¡œ ì¡°ë¡€ë¡œ ê·œìœ¨ ê°€ëŠ¥ëŠ¥**\n"
        "**ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ì •ì˜**: êµ­ê°€ì‚¬ë¬´ë¥¼ ì§€ë°©ìì¹˜ë‹¨ì²´ì¥(íŠ¹ë³„ì‹œì¥, ë„ì§€ì‚¬, ê´‘ì—­ì‹œì¥, ì‹œì¥, êµ°ìˆ˜, êµ¬ì²­ì¥)ì—ê²Œ ìœ„ì„í•œ ì‚¬ë¬´, ì¡°ë¡€ì—ì„œ ìœ„ì„í•œê²Œ ì•„ë‹ˆê³  ë²•ë¥ , ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ì—ì„œ ìœ„ì„í•œ ê²ƒì„ ë§í•¨\n"
        "**í•µì‹¬ ì›ì¹™**: ê¸°ê´€ìœ„ì„ì‚¬ë¬´ì— ëŒ€í•´ì„œëŠ” ì¡°ë¡€ ì œì •ì´ ì›ì¹™ì ìœ¼ë¡œ ê¸ˆì§€ë¨ (ì§€ë°©ìì¹˜ë²• ì œ22ì¡°)\n"
        "**íŒë³„ ê¸°ì¤€**: \n"
        "  1) ì‚¬ë¬´ê°€ êµ­ê°€ì‚¬ë¬´ì¸ì§€ í™•ì¸ (ì˜ˆ: ê±´ì¶•í—ˆê°€, ë„ì‹œê³„íš, í™˜ê²½ì˜í–¥í‰ê°€ ë“±)\n"
        "  2) í•´ë‹¹ ì‚¬ë¬´ê°€ ì§€ë°©ìì¹˜ë‹¨ì²´ì¥(íŠ¹ë³„ì‹œì¥, ë„ì§€ì‚¬, ê´‘ì—­ì‹œì¥, ì‹œì¥, êµ°ìˆ˜, êµ¬ì²­ì¥)ì—ê²Œ ìœ„ì„ë˜ì—ˆëŠ”ì§€ í™•ì¸\n"
        "  3) ìì¹˜ì‚¬ë¬´ì¸ì§€ ê¸°ê´€ìœ„ì„ì‚¬ë¬´ì¸ì§€ íŒë‹¨í•¨ì— ìˆì–´ ë²•ë ¹ì˜ ê·œì •í˜•ì‹ê³¼ ì·¨ì§€ë¥¼ ìš°ì„  ê³ ë ¤í•´ì•¼ í• ê²ƒì´ë‚˜ ê·¸ ì™¸ì—ë„ ì‚¬ë¬´ì˜ ì„±ì§ˆì´ ì „êµ­ì ìœ¼ë¡œ í†µì¼ì ì¸ ì²˜ë¦¬ë¥¼ ìš”êµ¬í•˜ëŠ” ì‚¬ë¬´ì¸ì§€ ê²½ë¹„ë¶€ë‹´ê³¼ ìµœì¢…ì ì¸ ì±…ì„ê·€ì† ì£¼ì²´ë“±ë„ ê³ ë ¤í•´ íŒë‹¨\n"
        "**ìœ„ë²• ì‚¬ë¡€**: ê±´ì¶•í—ˆê°€, ê°œë°œí–‰ìœ„í—ˆê°€, í™˜ê²½ì˜í–¥í‰ê°€ ë“± êµ­ê°€ìœ„ì„ì‚¬ë¬´ì— ëŒ€í•´ ì¡°ë¡€ë¡œ ì¶”ê°€ ê·œì •ì„ ë‘” ê²½ìš°\n"
        "- ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ìì¹˜ì‚¬ë¬´ì™€ ë²•ë ¹ì— ì˜í•´ ìœ„ì„ëœ ë‹¨ì²´ìœ„ì„ì‚¬ë¬´ì— ëŒ€í•´ì„œë§Œ ì œì • ê°€ëŠ¥í•œì§€\n"
        "- ì‚¬ë¬´ì˜ ì„±ê²©ì´ ì „êµ­ì ìœ¼ë¡œ í†µì¼ì  ì²˜ë¦¬ë¥¼ ìš”êµ¬í•˜ëŠ”ì§€ ì—¬ë¶€ ê²€í† \n\n"
        "b) ë²•ë¥  ìœ ë³´ì˜ ì›ì¹™\n"
        "- ì£¼ë¯¼ì˜ ê¶Œë¦¬ë¥¼ ì œí•œí•˜ê±°ë‚˜ ì˜ë¬´ë¥¼ ë¶€ê³¼ì— ê´€í•œ ì‚¬í•­ì´ë‚˜ ë²Œì¹™ì„ ì •í•  ë•Œì—ëŠ” ë²•ë¥ ì˜ ìœ„ì„ì´ ìˆì–´ì•¼ í•¨\n"
        "- ìƒìœ„ ë²•ë ¹ì—ì„œ ìœ„ì„ë°›ì§€ ì•Šì€ ê¶Œí•œì„ í–‰ì‚¬í•˜ëŠ”ì§€ í™•ì¸ (ë‹¨, ê¶Œë¦¬Â·ì˜ë¬´ë¥¼ ì œí•œí•˜ì§€ ì•ŠëŠ” ì¡°ì§Â·ì ˆì°¨ ê·œì •ì€ ì œì™¸)\n"
        "- ìƒìœ„ ë²•ë ¹ì˜ ìœ„ì„ ë²”ìœ„ë¥¼ ëª…ë°±íˆ ì´ˆê³¼í•˜ëŠ”ì§€ ê²€í† \n\n"
        "**âš ï¸ ì¤‘ìš”: í¬ê´„ì¡°í•­ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ íŒë‹¨ ê¸°ì¤€**\n"
        "- 'ê·¸ ë°–ì— ì‹œì¥ì´ í•„ìš”í•˜ë‹¤ê³  ì¸ì •í•˜ëŠ” ê²½ìš°' ë“± ë³´ì¶©ì  í¬ê´„ì¡°í•­ì€:\n"
        "  â‘  ì£¼ë¯¼ì˜ ê¶Œë¦¬Â·ì˜ë¬´ë¥¼ ì§ì ‘ ì œí•œí•˜ì§€ ì•Šê³ \n"
        "  â‘¡ ìì¹˜ë‹¨ì²´ì˜ ìë¬¸Â·ì‹¬ì˜ê¸°êµ¬ ìš´ì˜ì— ê´€í•œ ì‚¬í•­ì´ë©°\n"
        "  â‘¢ ë‹¤ë¥¸ ì§€ìì²´ ì¡°ë¡€ì—ì„œë„ í”íˆ ì‚¬ìš©ë˜ëŠ” ê²½ìš°\n"
        "  â†’ **ì›ì¹™ì ìœ¼ë¡œ ì ë²•í•œ ê·œì •ìœ¼ë¡œ íŒë‹¨**\n"
        "- íƒ€ ì§€ìì²´ ì¡°ë¡€ì— ìœ ì‚¬ ì¡°í•­ì´ ë‹¤ìˆ˜ ì¡´ì¬í•œë‹¤ë©´ ì´ëŠ” ì¡°ë¡€ ê´€í–‰ìœ¼ë¡œ ì¸ì •ë˜ë¯€ë¡œ ìœ„ë²•ìœ¼ë¡œ ë³´ì§€ ì•ŠìŒ\n"
        "- ë²•ë¥ ìœ ë³´ ìœ„ë°˜ìœ¼ë¡œ íŒë‹¨í•˜ê¸° ìœ„í•´ì„œëŠ” 'ì£¼ë¯¼ì˜ ê¶Œë¦¬ ì œí•œ ë˜ëŠ” ì˜ë¬´ ë¶€ê³¼'ë¼ëŠ” ìš”ê±´ì´ ë°˜ë“œì‹œ ì¶©ì¡±ë˜ì–´ì•¼ í•¨\n\n"
        "c) ë²•ë ¹ìš°ìœ„ì˜ ì›ì¹™ ìœ„ë°˜ ì—¬ë¶€\n"
        "- **ì¡°ë¡€ê°€ ë²•ë ¹ì— ìœ„ë°˜ë˜ëŠ”ì§€ ì—¬ë¶€ëŠ” ë²•ë ¹ê³¼ ì¡°ë¡€ì˜ ê°ê°ì˜ ê·œì • ì·¨ì§€, ê·œì •ì˜ ëª©ì ê³¼ ë‚´ìš© ë° íš¨ê³¼ ë“±ì„ ë¹„êµí•˜ì—¬ ì–‘ì ì‚¬ì´ì— ëª¨ìˆœ, ì €ì´‰ì´ ìˆëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ ê°œë³„ì , êµ¬ì²´ì ìœ¼ë¡œ ê²°ì •í•´ì•¼ í•¨**\n"
        "- **ì¼ë°˜ë¡ ì´ ì•„ë‹Œ êµ¬ì²´ì  ì¶©ëŒ ì§€ì ì„ ì°¾ì„ ê²ƒ - ë‹¨ìˆœíˆ 'ë‹¤ë¥´ë‹¤'ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ìœ„ë²•ì´ ì•„ë‹˜**\n"
        "- 'ë‹¤ë¥¸ ì¡°ë¡€ì— íŠ¹ë³„ ê·œì •ì´ ì—†ìœ¼ë©´ ë³¸ ì¡°ë¡€ê°€ ìš°ì„ 'ì´ë¼ëŠ” ê·œì •ì€ ë‹¤ë¥¸ ì¡°ë¡€ì™€ ë¹„êµí–ˆì„ ë•Œ ìš°ì„ í•œë‹¤ëŠ” ê²ƒì´ì§€ ìƒìœ„ë²•ë ¹ë³´ë‹¤ ìš°ì„ í•œë‹¤ëŠ” ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì ë²•í•¨\n"
        "- ìœ„ì— ì œì‹œëœ ìƒìœ„ë²•ë ¹ ë³¸ë¬¸ì„ í•œ ì¡°ë¬¸ì”© ê¼¼ê¼¼íˆ ì½ê³  ì¡°ë¡€ì™€ ëŒ€ì¡°í•  ê²ƒ\n\n"
        "**âš ï¸ ì¤‘ìš”: ìì¹˜ë‹¨ì²´ ìœ„ì›íšŒÂ·ìë¬¸ê¸°êµ¬ ìš´ì˜ ê·œì •ì— ëŒ€í•œ íŒë‹¨ ê¸°ì¤€**\n"
        "- ìì¹˜ë‹¨ì²´ê°€ ì„¤ì¹˜í•˜ëŠ” ê°ì¢… ìœ„ì›íšŒ, ìë¬¸ê¸°êµ¬, í˜‘ì˜ì²´ ë“±ì˜ êµ¬ì„±Â·ìš´ì˜ì— ê´€í•œ ì‚¬í•­ì€:\n"
        "  â‘  ìì¹˜ë‹¨ì²´ì˜ ë‚´ë¶€ ì¡°ì§Â·ì ˆì°¨ì— ê´€í•œ ì‚¬í•­ìœ¼ë¡œì„œ\n"
        "  â‘¡ ì£¼ë¯¼ì˜ ê¶Œë¦¬Â·ì˜ë¬´ì™€ ì§ì ‘ ê´€ë ¨ì´ ì—†ê³ \n"
        "  â‘¢ ìì¹˜ì…ë²•ê¶Œì˜ í•µì‹¬ ì˜ì—­ì— í•´ë‹¹í•˜ë¯€ë¡œ\n"
        "  â†’ **ìƒìœ„ë²•ì— ëª…ì‹œì  ê¸ˆì§€ ê·œì •ì´ ì—†ëŠ” í•œ ì›ì¹™ì ìœ¼ë¡œ ì ë²•**\n"
        "- íŠ¹ì •ì•ˆê±´í˜• ìœ„ì›íšŒì˜ ìë™ êµ¬ì„±Â·ìë™ í•´ì‚° ì¡°í•­ì€ ì¡°ë¡€ ì œì •ê¶Œì˜ ë²”ìœ„ ë‚´ì—ì„œ í—ˆìš©ë¨\n"
        "- 'ì ˆì°¨ê¶Œ ì¹¨í•´', 'êµ¬ì„±ê¶Œ ì¹¨í•´' ë“±ì˜ ê°œë…ì€ ì£¼ë¯¼ì˜ ê¶Œìµê³¼ ì§ì ‘ ê´€ë ¨ëœ ê²½ìš°ì—ë§Œ ì ìš©ë˜ë©°, ìë¬¸ê¸°êµ¬ì—ëŠ” ì ìš©í•˜ì§€ ì•ŠìŒ\n\n"
        "**ê²€í†  ë°©ë²•**:\n"
        "1) ì¡°ë¡€ ì œ1ì¡°ë¶€í„° ë§ˆì§€ë§‰ ì¡°ë¬¸ê¹Œì§€ í•˜ë‚˜ì”© ê²€í† \n"
        "2) ê° ì¡°ë¡€ ì¡°ë¬¸ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ìƒìœ„ë²•ë ¹ ì¡°ë¬¸ì„ ì°¾ì•„ì„œ ì§ì ‘ ë¹„êµ\n"
        "3) ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì²´ì  ì¶©ëŒì´ ìˆëŠ”ì§€ í™•ì¸:\n"
        "   - ì¡°ë¡€ì—ì„œ ê·œìœ¨í•˜ëŠ” ë‚´ìš©ì— ê´€í•œ ë²•ë ¹ì´ ì—†ëŠ” ê²½ìš° í‰ë“±ì˜ ì›ì¹™, ë¹„ë¡€ì˜ ì›ì¹™, ëª…í™•ì„±ì˜ ì›ì¹™ ê°™ì€ ë²•ì˜ ì¼ë°˜ì›ì¹™ì— ìœ„ë°˜ë˜ì§€ ì•ŠëŠ” ì§€ ê²€í† \n"
        "   - ì¡°ë¡€ì˜ ëª©ì ê³¼ ì·¨ì§€ê°€ ë²•ë ¹ì˜ ëª©ì ê³¼ ì·¨ì§€ì™€ ê°™ì€ ê²½ìš°ì—ë„ ë²•ë ¹ì˜ ì·¨ì§€ê°€ ì „êµ­ì— ê±¸ì³ ì¼ë¥ ì ì¸ ê·œìœ¨ì„ í•˜ë ¤ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê° ì§€ìì²´ê°€ ì§€ë°© ì‹¤ì •ì— ë§ê²Œ ë³„ë„ë¡œ ê·œìœ¨í•˜ëŠ” ê²ƒì„ ìš©ì¸í•œë‹¤ê³  í•´ì„¤ë ë•ŒëŠ” ë²•ë ¹ì— ìœ„ë°˜ ë˜ëŠ” ê²ƒì´ ì•„ë‹˜ \n"
        "   - ìˆ˜ìµì  ë‚´ìš©ì´ë©´ ë²•ë ¹ì— ê·¼ê±°ê°€ ì—†ì–´ë„ ì¡°ë¡€ë¡œ ì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë²•ë ¹ì—ì„œ ì¡°ë¡€ë¡œ ë‹¤ë¥´ê²Œ ì •í•  ìˆ˜ ì—†ë‹¤ê³  ê·œì •í•˜ì§€ ì•ŠëŠ” ì´ìƒ ë²•ë ¹ê³¼ ë‹¤ë¥´ê²Œ ì¡°ë¡€ì— ê·œì •í•  ìˆ˜ ìˆëŠ” ì—¬ì§€ê°€ ë§ìŒ\n"
        "   - ì¹¨ìµì  ë‚´ìš©ì´ë©´ ë²•ë¥ ì—ì„œ ìœ„ì„ë°›ì€ ë²”ìœ„ì—ì„œë§Œ ì¡°ë¡€ë¡œ ì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë²•ë ¹ê³¼ ë‹¤ë¥´ê²Œ ì¡°ë¡€ì— ê·œì •í•  ìˆ˜ ìˆëŠ” ì—¬ì§€ê°€ ê±°ì˜ ì—†ìŒ\n"
        "   - ì¡°ë¡€ê°€ ìƒìœ„ë²•ë ¹ë³´ë‹¤ ê°•í•œ ì˜ë¬´ë‚˜ ì œì¬ë¥¼ ë¶€ê³¼í•˜ëŠ” ê²½ìš°\n"
        "   - ì¡°ë¡€ê°€ ìƒìœ„ë²•ë ¹ì˜ ìœ„ì„ ë²”ìœ„ë¥¼ ëª…ë°±íˆ ë²—ì–´ë‚˜ëŠ” ê²½ìš°\n"
        "   - ì¡°ë¡€ê°€ ìƒìœ„ë²•ë ¹ì—ì„œ êµ­ê°€ë‚˜ ì¤‘ì•™í–‰ì •ê¸°ê´€ ì†Œê´€ìœ¼ë¡œ ì •í•œ ì‚¬ë¬´ì— ê´€ì—¬í•˜ëŠ” ê²½ìš°\n\n"
        "**ìœ„ë²• ë°œê²¬ ì‹œ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ:**\n"
        "  ğŸš¨ **ìœ„ë²• ì‚¬í•­ ë°œê²¬** (ìƒìœ„ ê°€ì´ë“œë¼ì¸ 1~4ë¥¼ ì¶©ì¡±í•˜ëŠ” ê²½ìš°ì—ë§Œ ì§€ì )\n"
        "  * **ì¡°ë¡€ ì¡°ë¬¸**: ì œâ—‹ì¡° â—‹í•­ - \"ì¡°ë¡€ì˜ ì •í™•í•œ ë¬¸êµ¬\"\n"
        "  * **ìƒìœ„ë²•ë ¹**: â—‹â—‹ë²• ì œâ—‹ì¡° â—‹í•­ - \"ìƒìœ„ë²•ë ¹ì˜ ì •í™•í•œ ë¬¸êµ¬\"\n"
        "  * **ì¶©ëŒ ë‚´ìš©**: êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì´ ì–´ë–»ê²Œ ìœ„ë°°ë˜ëŠ”ì§€ ìƒì„¸ ì„¤ëª… (ì¶”ìƒì  ì„¤ëª… ê¸ˆì§€)\n"
        "  * **ìœ„ë²• ìœ í˜•**: (ë²•ë ¹ìš°ìœ„ ìœ„ë°˜/ë²•ë¥ ìœ ë³´ ìœ„ë°˜/ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ìœ„ë°˜)\n"
        "  * **ìœ„ë²• íŒë‹¨ ê·¼ê±°**: ìœ„ ê°€ì´ë“œë¼ì¸ 4ì¡° ì¤‘ ì–´ëŠ ìš”ê±´ì„ ì¶©ì¡±í•˜ëŠ”ì§€ ëª…ì‹œ (1)ìƒìœ„ë²• ëª…ë ¹Â·ê¸ˆì§€ ìœ„ë°˜, 2)ê¶Œë¦¬ì œí•œÂ·ì˜ë¬´ë¶€ê³¼ ê·¼ê±° ë¶€ì¬, 3)êµ­ê°€ì‚¬ë¬´ ì¹¨í•´)\n"
        "  * **ê°œì„  ë°©ì•ˆ**: ìƒìœ„ë²•ë ¹ì— ë§ëŠ” êµ¬ì²´ì  ìˆ˜ì •ì•ˆ\n\n"
        "**âš ï¸ ì¤‘ìš”: ìœ„ë²• íŒë‹¨ì˜ ì—„ê²©ì„±**\n"
        "- ìœ„ë²• ì‚¬í•­ì´ ì—†ëŠ” ê²½ìš° 'ìœ„ë²• ì‚¬í•­ì„ ë°œê²¬í•˜ì§€ ëª»í–ˆìŒ'ìœ¼ë¡œ ëª…í™•íˆ ê²°ë¡ \n"
        "- ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¶€ë¶„ì´ ìˆë”ë¼ë„ **ìœ„ ê°€ì´ë“œë¼ì¸ 1~4ë¥¼ ì¶©ì¡±í•˜ì§€ ì•Šìœ¼ë©´ ìœ„ë²•ìœ¼ë¡œ íŒë‹¨í•˜ì§€ ì•ŠìŒ**\n"
        "- ë‹¨ìˆœ í‘œí˜„ ì°¨ì´, íƒ€ ì§€ìì²´ì™€ì˜ ì¡°ë¬¸ êµ¬ì„± ì°¨ì´ë§Œìœ¼ë¡œëŠ” ìœ„ë²•ì„±ì„ ì§€ì í•˜ì§€ ì•ŠìŒ\n"
        "- í¬ê´„ì¡°í•­, ìë™í•´ì‚° ì¡°í•­, ìë¬¸ê¸°êµ¬ ìš´ì˜ ì¡°í•­ ë“± ìì¹˜ì…ë²•ê¶Œ ë²”ìœ„ ë‚´ ì‚¬í•­ì€ ìœ„ë²•ìœ¼ë¡œ ë³´ì§€ ì•ŠìŒ\n\n"
        "4. ì‹¤ë¬´ì  ê²€í†  í¬ì¸íŠ¸\n"
        "- ì¡°ë¡€ì˜ ì§‘í–‰ ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œì \n"
        "- ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ê³¼ ê·¸ ë°©í–¥ì„±\n\n"
    )

    # ìƒìœ„ë²•ë ¹ë³„ ê°œë³„ ìœ„ë°˜ ì—¬ë¶€ ê²€í†  (Gemini ì „ìš© í”„ë¡¬í”„íŠ¸ ì¶”ê°€)
    if superior_laws_content:
        prompt += "\n5. [ìƒìœ„ë²•ë ¹ë³„ ê°œë³„ ìœ„ë°˜ ì—¬ë¶€ ê²€í† ]\n"
        prompt += "ìœ„ì—ì„œ ì œì‹œí•œ ìƒìœ„ë²•ë ¹ë“¤ ê°ê°ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ìƒì„¸ ë¶„ì„í•´ì¤˜:\n\n"

        section_num = 1
        for law_group in superior_laws_content:
            base_name = law_group['base_name']

            prompt += f"5-{section_num}) [{base_name} ìœ„ë°˜ ì—¬ë¶€ ê²€í† ]\n"
            prompt += f"ìƒìœ„ë²•ë ¹ëª…: {base_name}\n"

            # í•´ë‹¹ ë²•ë ¹ì˜ ë³¸ë¬¸ ì¼ë¶€ ì¬ì°¸ì¡°
            if 'combined_content' in law_group:
                law_content_preview = law_group['combined_content'][:2000]
                prompt += f"ìƒìœ„ ë²•ë ¹ ë³¸ë¬¸ ì¼ë¶€:\n{law_content_preview}\n\n"
            elif 'combined_articles' in law_group and law_group['combined_articles']:
                prompt += "ìƒìœ„ ë²•ë ¹ ì£¼ìš” ì¡°ë¬¸:\n"
                for article in law_group['combined_articles'][:5]:  # ì²˜ìŒ 5ê°œ ì¡°ë¬¸ë§Œ
                    prompt += f"  {article['number']} {article['title']}\n"
                    prompt += f"  {article['content'][:300]}...\n\n"

            prompt += f"**ğŸ” {base_name} ì„¸ë¶€ ê²€í†  ì§€ì‹œì‚¬í•­: (âš ï¸ ìƒìœ„ ë²•ë¦¬ì  ê°€ì´ë“œë¼ì¸ 1~4 ì¤€ìˆ˜)**\n"
            prompt += "ìœ„ ìƒìœ„ë²•ë ¹ ë³¸ë¬¸ì„ ì¡°ë¡€ì™€ í•œ ì¡°ë¬¸ì”© ì§ì ‘ ëŒ€ì¡°í•˜ì—¬ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ë¼:\n\n"
            prompt += "  â‘  **ì¡°ë¬¸ë³„ ì§ì ‘ ëŒ€ì¡° ë¶„ì„** (ë‹¨ìˆœ ì°¨ì´ëŠ” ìœ„ë²•ì´ ì•„ë‹˜)\n"
            prompt += f"  - ì¡°ë¡€ì˜ ê° ì¡°ë¬¸ì´ {base_name}ì˜ ì–´ë–¤ ì¡°ë¬¸ê³¼ ê´€ë ¨ë˜ëŠ”ì§€ ì‹ë³„\n"
            prompt += f"  - {base_name}ì—ì„œ ê¸ˆì§€/í—ˆìš©/ì˜ë¬´í™”í•˜ëŠ” ì‚¬í•­ê³¼ ì¡°ë¡€ ë‚´ìš© ì§ì ‘ ë¹„êµ\n"
            prompt += "  - **ëª…ë°±í•œ ìƒì¶©**ì´ ìˆì„ ë•Œë§Œ ì§€ì  (ë‹¨ìˆœ í‘œí˜„ ì°¨ì´, ì¡°ë¬¸ êµ¬ì„± ì°¨ì´ëŠ” ì œì™¸)\n\n"
            prompt += "  â‘¡ **ê¶Œí•œ ë²”ìœ„ ì´ˆê³¼ ì—¬ë¶€** (ìì¹˜ì…ë²•ê¶Œ ë²”ìœ„ ê³ ë ¤)\n"
            prompt += f"  - {base_name}ì—ì„œ êµ­ê°€/ì¤‘ì•™í–‰ì •ê¸°ê´€ ì „ë‹´ìœ¼ë¡œ 'ëª…ì‹œì 'ìœ¼ë¡œ ì •í•œ ì‚¬ë¬´ê°€ ìˆëŠ”ì§€ í™•ì¸\n"
            prompt += "  - ì¡°ë¡€ê°€ í•´ë‹¹ ì‚¬ë¬´ì— ê°œì…í•˜ê³  ìˆëŠ”ì§€ ì ê²€ (ë‹¨, ìœ„ì„ ê·œì •ì´ ìˆìœ¼ë©´ ì ë²•)\n"
            prompt += "  - ìœ„ì„ ë²”ìœ„ë¥¼ 'ëª…ë°±íˆ' ë²—ì–´ë‚œ ê·œì •ì´ ìˆëŠ”ì§€ í™•ì¸ (í•´ì„ìƒ ì—¬ì§€ê°€ ìˆìœ¼ë©´ ìœ„ë²• ì•„ë‹˜)\n\n"
            prompt += "  â‘¢ **êµ¬ì²´ì  ìœ„ë²• ì‚¬í•­ ë°œê²¬ ì‹œ** (âš ï¸ ê°€ì´ë“œë¼ì¸ 4ì¡° ìš”ê±´ ì¶©ì¡± ì‹œì—ë§Œ)\n"
            prompt += "  ğŸš¨ **ìœ„ë²• ë°œê²¬ ë³´ê³  í˜•ì‹:**\n"
            prompt += "  * **ë¬¸ì œ ì¡°ë¬¸**: ì¡°ë¡€ ì œâ—‹ì¡° - \"ì •í™•í•œ ì¡°ë¬¸ ë‚´ìš©\"\n"
            prompt += f"  * **ê´€ë ¨ ìƒìœ„ë²•ë ¹**: {base_name} ì œâ—‹ì¡° - \"ì •í™•í•œ ì¡°ë¬¸ ë‚´ìš©\"\n"
            prompt += "  * **ìœ„ë²• ì‚¬ìœ **: êµ¬ì²´ì ì¸ ì¶©ëŒ/ìœ„ë°˜ ë‚´ìš© (ì¶”ìƒì  ì„¤ëª… ê¸ˆì§€)\n"
            prompt += "  * **ê°€ì´ë“œë¼ì¸ 4ì¡° ì¶©ì¡± ì—¬ë¶€**: (1)ìƒìœ„ë²• ëª…ë ¹Â·ê¸ˆì§€ ìœ„ë°˜ / (2)ê¶Œë¦¬ì œí•œÂ·ì˜ë¬´ë¶€ê³¼ ê·¼ê±° ë¶€ì¬ / (3)êµ­ê°€ì‚¬ë¬´ ì¹¨í•´ ì¤‘ í•´ë‹¹ í•­ëª© ëª…ì‹œ\n"
            prompt += "  * **ìœ„ë²• ì‹¬ê°ë„**: ê²½ë¯¸/ë³´í†µ/ì‹¬ê°\n"
            prompt += "  * **ìˆ˜ì • ë°©ì•ˆ**: êµ¬ì²´ì ì¸ ê°œì„  ë°©í–¥\n\n"
            prompt += "  â‘£ **ìœ„ë²•ì´ ì•„ë‹Œ ê²½ìš° ëª…í™•íˆ ê¸°ì¬**\n"
            prompt += "  - ë‹¨ìˆœ í‘œí˜„ ì°¨ì´, ì¡°ë¬¸ êµ¬ì„± ì°¨ì´ëŠ” 'ìœ„ë²• ì•„ë‹˜'ìœ¼ë¡œ ëª…ì‹œ\n"
            prompt += "  - í¬ê´„ì¡°í•­, ìë™í•´ì‚° ì¡°í•­, ìë¬¸ê¸°êµ¬ ìš´ì˜ ì¡°í•­ ë“±ì€ 'ìì¹˜ì…ë²•ê¶Œ ë²”ìœ„ ë‚´ ì ë²•'ìœ¼ë¡œ íŒë‹¨\n"
            prompt += "  - ì˜ì‹¬ ì‚¬í•­ì´ ìˆë”ë¼ë„ ê°€ì´ë“œë¼ì¸ 1~4ë¥¼ ì¶©ì¡±í•˜ì§€ ì•Šìœ¼ë©´ 'ìœ„ë²• ì•„ë‹˜'ìœ¼ë¡œ ê²°ë¡ \n\n"

            section_num += 1

    return prompt

def parse_table_from_text(text_content):
    """í…ìŠ¤íŠ¸ì—ì„œ í‘œ í˜•íƒœì˜ ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ Word í‘œ ë°ì´í„°ë¡œ ë³€í™˜"""
    tables_data = []
    lines = text_content.split('\n')
    current_table = None

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # í‘œì˜ ì‹œì‘ì„ ê°ì§€ (|ê°€ í¬í•¨ëœ ë¼ì¸)
        if '|' in line and len([cell for cell in line.split('|') if cell.strip()]) >= 3:
            # í‘œ í—¤ë”ì¸ì§€ êµ¬ë¶„ (ì²« ë²ˆì§¸ |ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸)
            cells = [cell.strip() for cell in line.split('|') if cell.strip()]

            if current_table is None:
                # ìƒˆ í‘œ ì‹œì‘
                current_table = {'headers': cells, 'rows': []}
                tables_data.append(current_table)
            else:
                # êµ¬ë¶„ì„ ì´ ì•„ë‹Œ ë°ì´í„° í–‰ì¸ì§€ í™•ì¸
                if not all(cell.replace('-', '').replace(':', '').strip() == '' for cell in cells):
                    current_table['rows'].append(cells)
        else:
            # í‘œê°€ ëë‚¨
            if current_table is not None:
                current_table = None

    return tables_data

def add_table_to_doc(doc, table_data):
    """Word ë¬¸ì„œì— í‘œ ì¶”ê°€"""
    if not table_data['headers']:
        return

    # ì—´ ìˆ˜ ê³„ì‚°
    max_cols = len(table_data['headers'])
    for row in table_data['rows']:
        max_cols = max(max_cols, len(row))

    # í–‰ ìˆ˜ ê³„ì‚° (í—¤ë” + ë°ì´í„° í–‰)
    row_count = 1 + len(table_data['rows'])

    if row_count == 1:  # í—¤ë”ë§Œ ìˆëŠ” ê²½ìš° ìŠ¤í‚µ
        return

    # í‘œ ìƒì„±
    table = doc.add_table(rows=row_count, cols=max_cols)
    table.style = 'Table Grid'
    table.autofit = True

    # í—¤ë” ì¶”ê°€
    header_cells = table.rows[0].cells
    for i, header in enumerate(table_data['headers']):
        if i < len(header_cells):
            header_cells[i].text = header
            # í—¤ë” ìŠ¤íƒ€ì¼ë§
            paragraph = header_cells[i].paragraphs[0]
            run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()
            run.bold = True

    # ë°ì´í„° í–‰ ì¶”ê°€
    for row_idx, row_data in enumerate(table_data['rows']):
        if row_idx + 1 < len(table.rows):
            cells = table.rows[row_idx + 1].cells
            for col_idx, cell_data in enumerate(row_data):
                if col_idx < len(cells):
                    cells[col_idx].text = cell_data

def create_comparison_document(pdf_text, search_results, analysis_results, superior_laws_content=None, relevant_guidelines=None):
    """ë¹„êµ ë¶„ì„ ë¬¸ì„œ ìƒì„± í•¨ìˆ˜"""
    doc = Document()
    section = doc.sections[-1]
    section.orientation = WD_ORIENT.LANDSCAPE
    section.page_width = Mm(420)
    section.page_height = Mm(297)

    # ì œëª© ì¶”ê°€
    title = doc.add_heading('ì¡°ë¡€ ë¹„êµ ë¶„ì„ ê²°ê³¼', level=1)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph(f'ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')

    # ìƒìœ„ë²•ë ¹ ì •ë³´ ì¶”ê°€ (ê³„ì¸µë³„ ê·¸ë£¹í™”)
    if superior_laws_content:
        doc.add_heading('ê²€í† ëœ ìƒìœ„ë²•ë ¹', level=2)
        for law_group in superior_laws_content:
            base_name = law_group['base_name']
            
            # ê·¸ë£¹ ì œëª© ì¶”ê°€
            doc.add_paragraph(f"â—† {base_name}")
            
            # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°
            if 'combined_content' in law_group:
                content_length = len(law_group['combined_content'])
                doc.add_paragraph(f"  â€¢ ë³¸ë¬¸ {content_length:,}ì")
            else:
                # ê¸°ì¡´ ë°©ì‹ - ê° ê³„ì¸µë³„ ë²•ë ¹ ì •ë³´ í‘œì‹œ
                for law_type, law_info in law_group['laws'].items():
                    if law_info and 'articles' in law_info:
                        type_name = "ë²•ë¥ " if law_type == 'law' else ("ì‹œí–‰ë ¹" if law_type == 'decree' else "ì‹œí–‰ê·œì¹™")
                        doc.add_paragraph(f"  â€¢ {law_info['law_name']} ({type_name}) - {len(law_info['articles'])}ê°œ ì¡°ë¬¸")
                
                combined_articles = law_group.get('combined_articles', [])
                doc.add_paragraph(f"  ì´ {len(combined_articles)}ê°œ ì¡°ë¬¸ í†µí•© ê²€í† ")
            
            doc.add_paragraph("")
        doc.add_paragraph("")
    
    # í™œìš©ëœ ìì¹˜ë²•ê·œ ìë£Œ ì •ë³´ ì¶”ê°€
    if relevant_guidelines:
        doc.add_heading('í™œìš©ëœ ìì¹˜ë²•ê·œ ì°¸ê³ ìë£Œ', level=2)
        
        # ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
        source_groups = {}
        for guideline in relevant_guidelines:
            source_store = guideline.get('source_store', 'ì•Œ ìˆ˜ ì—†ëŠ” ìë£Œ')
            if source_store not in source_groups:
                source_groups[source_store] = []
            source_groups[source_store].append(guideline)
        
        for source_store, guidelines in source_groups.items():
            doc.add_paragraph(f"â—† {source_store} ({len(guidelines)}ê°œ ë‚´ìš©)")
            for i, guideline in enumerate(guidelines):
                similarity_score = guideline.get('similarity', 1-guideline.get('distance', 0))
                doc.add_paragraph(f"   â€¢ ë‚´ìš© {i+1} (ìœ ì‚¬ë„: {similarity_score:.3f})")
        doc.add_paragraph("")

    # ìµœì¢… ë¶„ì„ ê²°ê³¼ë§Œ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
    # ìš°ì„ ìˆœìœ„: ìë£Œ ì°¸ê³  ë³´ê°•ë¶„ì„ > OpenAI ì¶”ê°€ ë¶„ì„ > 1ì°¨ ë¶„ì„
    final_report = None
    for result in reversed(analysis_results):  # ì—­ìˆœìœ¼ë¡œ ìµœì‹  ê²°ê³¼ ìš°ì„ 
        if 'error' not in result:
            if "ìë£Œ ì°¸ê³  ë³´ê°•ë¶„ì„" in result.get('model', ''):
                final_report = result
                break
            elif "ìë£Œ ì°¸ê³ " in result.get('model', '') or "OpenAI" in result.get('model', ''):
                final_report = result
                break

    # ìë£Œ ì°¸ê³ ë‚˜ OpenAIê°€ ì—†ìœ¼ë©´ 1ì°¨ ë¶„ì„ ì‚¬ìš©
    if not final_report:
        for result in analysis_results:
            if 'error' not in result and "1ì°¨ ë¶„ì„" in result.get('model', ''):
                final_report = result
                break

    # ìµœì¢… ë³´ê³ ì„œê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if final_report:
        doc.add_heading(f'ğŸ“‹ {final_report["model"]}', level=2)
        content = final_report.get('content') or final_report.get('analysis', '')

        # ğŸ†• í‘œ íŒŒì‹± ë° ì²˜ë¦¬
        tables_data = parse_table_from_text(content)

        # í…ìŠ¤íŠ¸ë¥¼ ì„¹ì…˜ë³„ë¡œ ì²˜ë¦¬
        lines = content.split('\n')
        current_section = []

        for line in lines:
            line = line.strip()

            # í‘œ ë¼ì¸ì¸ì§€ í™•ì¸ (|ê°€ í¬í•¨ëœ ë¼ì¸)
            if '|' in line and len([cell for cell in line.split('|') if cell.strip()]) >= 3:
                # í‘œ ì‹œì‘ ì „ê¹Œì§€ì˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                if current_section:
                    for text_line in current_section:
                        text_line_clean = text_line.strip()
                        if text_line_clean:
                            # ì œëª© ë¼ì¸ ì²˜ë¦¬ (1., 2., 3. ë“±ìœ¼ë¡œ ì‹œì‘í•˜ê±°ë‚˜ [ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°)
                            if (text_line_clean.startswith(('1.', '2.', '3.', '4.', '5.')) or
                                text_line_clean.startswith('[') and text_line_clean.endswith(']')):
                                # ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì œê±°í•˜ê³  ì œëª©ìœ¼ë¡œ ì¶”ê°€
                                title_text = re.sub(r'[#*`>\-\[\]]+', '', text_line_clean)
                                doc.add_heading(title_text, level=3)
                            else:
                                # ì¼ë°˜ í…ìŠ¤íŠ¸ - ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ(**text**) ì²˜ë¦¬
                                if clean_text := text_line_clean.strip():
                                    p = doc.add_paragraph()
                                    # **í…ìŠ¤íŠ¸** í˜•ì‹ì˜ ë³¼ë“œ ì²˜ë¦¬
                                    parts = re.split(r'(\*\*[^*]+\*\*)', clean_text)
                                    for part in parts:
                                        if part.startswith('**') and part.endswith('**'):
                                            # ë³¼ë“œ í…ìŠ¤íŠ¸
                                            run = p.add_run(part[2:-2])
                                            run.bold = True
                                        elif part:
                                            # ì¼ë°˜ í…ìŠ¤íŠ¸ (ë‚¨ì€ ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì œê±°)
                                            clean_part = re.sub(r'[#`>]+', '', part)
                                            if clean_part:
                                                p.add_run(clean_part)
                    current_section = []

                # í‘œ ì²˜ë¦¬ëŠ” skip (ì´ë¯¸ tables_dataì—ì„œ ì²˜ë¦¬ë¨)
                continue
            else:
                # êµ¬ë¶„ì„ ì´ ì•„ë‹Œ ê²½ìš°ë§Œ í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
                if not (line.replace('-', '').replace(':', '').replace('|', '').strip() == ''):
                    current_section.append(line)

        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²˜ë¦¬
        if current_section:
            for text_line in current_section:
                text_line_clean = text_line.strip()
                if text_line_clean:
                    if (text_line_clean.startswith(('1.', '2.', '3.', '4.', '5.')) or
                        text_line_clean.startswith('[') and text_line_clean.endswith(']')):
                        title_text = re.sub(r'[#*`>\-\[\]]+', '', text_line_clean)
                        doc.add_heading(title_text, level=3)
                    else:
                        # ì¼ë°˜ í…ìŠ¤íŠ¸ - ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ(**text**) ì²˜ë¦¬
                        if clean_text := text_line_clean.strip():
                            p = doc.add_paragraph()
                            # **í…ìŠ¤íŠ¸** í˜•ì‹ì˜ ë³¼ë“œ ì²˜ë¦¬
                            parts = re.split(r'(\*\*[^*]+\*\*)', clean_text)
                            for part in parts:
                                if part.startswith('**') and part.endswith('**'):
                                    # ë³¼ë“œ í…ìŠ¤íŠ¸
                                    run = p.add_run(part[2:-2])
                                    run.bold = True
                                elif part:
                                    # ì¼ë°˜ í…ìŠ¤íŠ¸ (ë‚¨ì€ ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì œê±°)
                                    clean_part = re.sub(r'[#`>]+', '', part)
                                    if clean_part:
                                        p.add_run(clean_part)

        # ğŸ†• íŒŒì‹±ëœ í‘œë“¤ì„ Word ë¬¸ì„œì— ì¶”ê°€
        for table_data in tables_data:
            add_table_to_doc(doc, table_data)
            doc.add_paragraph("")  # í‘œ ê°„ê²©
    else:
        # ìµœì¢… ë³´ê³ ì„œê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ í‘œì‹œ
        doc.add_heading('âš ï¸ ë¶„ì„ ê²°ê³¼ ì—†ìŒ', level=2)
        doc.add_paragraph('ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')

        # ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€
        for result in analysis_results:
            if 'error' in result:
                doc.add_paragraph(f"âŒ {result['model']} ì˜¤ë¥˜: {result['error']}")

    return doc

def main():
    # í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ›ï¸ ê´‘ì—­ì§€ìì²´ ì¡°ë¡€ ê²€ìƒ‰, ë¹„êµ, ë¶„ì„</h1>
        <p>17ê°œ ê´‘ì—­ì§€ìì²´ì˜ ì¡°ë¡€ë¥¼ ê²€ìƒ‰í•˜ê³ , AIë¥¼ í™œìš©í•˜ì—¬ ë¹„êµ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë„êµ¬ì…ë‹ˆë‹¤.</p>
    </div>
    """, unsafe_allow_html=True)

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“‹ ì‘ì—… ìˆœì„œ")
        st.markdown("""
        <div class="step-card">
            <strong>1ë‹¨ê³„:</strong> ì¡°ë¡€ ê²€ìƒ‰ ë° Word ì €ì¥<br>
            ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì—¬ 17ê°œ ì‹œë„ì˜ ì¡°ë¡€ë¥¼ ê²€ìƒ‰í•˜ê³  3ë‹¨ ë¹„êµ í˜•íƒœë¡œ MS Word ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        </div>
        <div class="step-card">
            <strong>2ë‹¨ê³„:</strong> ì¡°ë¡€ì•ˆ PDF ì—…ë¡œë“œ<br>
            ì œì • ë˜ëŠ” ê°œì •í•  ì¡°ë¡€ì•ˆ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
        </div>
        <div class="step-card">
            <strong>3ë‹¨ê³„:</strong> AI ë¹„êµ ë¶„ì„<br>
            ì—…ë¡œë“œí•œ ì¡°ë¡€ì•ˆê³¼ íƒ€ ì‹œë„ ì¡°ë¡€ë¥¼ AIë¡œ ë¹„êµ ë¶„ì„í•˜ì—¬ MS Word ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        </div>
        """, unsafe_allow_html=True)

        st.header("ğŸ¤– AI ë¶„ì„ ì—”ì§„")

        # Ollama Cloud ìƒíƒœ í™•ì¸
        ollama_available = bool(st.session_state.ollama_api_key and st.session_state.ollama_api_key != "YOUR_OLLAMA_API_KEY_HERE")

        if ollama_available:
            st.success("âœ… **ë¬´ë£Œ AI ë¶„ì„ ì„œë¹„ìŠ¤ í™œì„±í™”ë¨**")
            st.info("ğŸš€ API í‚¤ ì…ë ¥ ì—†ì´ ë°”ë¡œ ë¶„ì„ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
            use_ollama = st.checkbox(
                "Ollama Cloud ì‚¬ìš© (ë¬´ë£Œ, ê¶Œì¥)",
                value=st.session_state.use_ollama_cloud,
                help="120B íŒŒë¼ë¯¸í„°ì˜ ê³ ì„±ëŠ¥ AI ëª¨ë¸ì„ ë¬´ë£Œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. API í‚¤ ë°œê¸‰ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤."
            )
            st.session_state.use_ollama_cloud = use_ollama
        else:
            st.warning("âš ï¸ Ollama Cloud ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            use_ollama = False
            st.session_state.use_ollama_cloud = False

        st.markdown("---")

        # ê³ ê¸‰ ì„¤ì • (ì„ íƒì )
        with st.expander("âš™ï¸ ê³ ê¸‰ ì„¤ì • (ì„ íƒì‚¬í•­)", expanded=False):
            st.markdown("**ì¶”ê°€ AI ì„œë¹„ìŠ¤** (ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©)")
            gemini_api_key = st.text_input("Gemini API í‚¤", type="password", help="Google AI Studioì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒì‚¬í•­)")
            openai_api_key = st.text_input("OpenAI API í‚¤", type="password", help="OpenAI í”Œë«í¼ì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒì‚¬í•­)")

            # Gemini File Search Store Manager ì´ˆê¸°í™”
            if gemini_api_key and st.session_state.gemini_store_manager is None:
                try:
                    st.session_state.gemini_store_manager = get_gemini_store_manager(gemini_api_key)
                    st.success("âœ… Gemini File Search ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    st.warning(f"âš ï¸ Gemini File Search ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

            st.markdown("---")
            st.subheader("ğŸ” ê²€ìƒ‰ ì—”ì§„ ì„¤ì •")

            use_gemini = st.checkbox(
                "Gemini File Search ì‚¬ìš©",
                value=st.session_state.use_gemini_search if gemini_api_key else False,
                help="ê¸°ì¡´ ë°©ì‹ ëŒ€ì‹  Gemini File Search APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
                disabled=not gemini_api_key
            )
            st.session_state.use_gemini_search = use_gemini

            if use_gemini:
                if st.session_state.gemini_store_manager:
                    st.success("âœ… Gemini File Search í™œì„±í™”ë¨")
                else:
                    st.warning("âš ï¸ Gemini API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”")

        # ê¸°ë³¸ê°’ ì„¤ì • (expander ì™¸ë¶€)
        if 'gemini_api_key' not in dir():
            gemini_api_key = ""
        if 'openai_api_key' not in dir():
            openai_api_key = ""

        st.header("â„¹ï¸ ì„œë¹„ìŠ¤ ì•ˆë‚´")
        st.markdown("""
        <div class="step-card">
            <strong>ğŸ‰ ë¬´ë£Œ AI ë¶„ì„ ì„œë¹„ìŠ¤</strong><br>
            ë³¸ ì„œë¹„ìŠ¤ëŠ” Ollama Cloudì˜ ê³ ì„±ëŠ¥ AI ëª¨ë¸(120B íŒŒë¼ë¯¸í„°)ì„ ë¬´ë£Œë¡œ ì œê³µí•©ë‹ˆë‹¤.<br>
            <strong>API í‚¤ ë°œê¸‰ ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!</strong>
        </div>
        """, unsafe_allow_html=True)

        with st.expander("ğŸ“‹ ì¶”ê°€ AI ì„œë¹„ìŠ¤ ì•ˆë‚´ (ì„ íƒì‚¬í•­)", expanded=False):
            st.markdown("""
            ë” ë‹¤ì–‘í•œ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°, ì•„ë˜ ì„œë¹„ìŠ¤ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

            ### ğŸ¤– Gemini API (ì„ íƒì‚¬í•­)
            - **ìš©ë„**: Gemini File Searchë¥¼ í†µí•œ ì •ë°€ ê²€ìƒ‰
            - **ë°œê¸‰**: [aistudio.google.com](https://aistudio.google.com)
            - **ë¬´ë£Œ í• ë‹¹ëŸ‰**: ì›” 1,000ë²ˆ ìš”ì²­

            ### ğŸ§  OpenAI API (ì„ íƒì‚¬í•­)
            - **ìš©ë„**: ì¶”ê°€ êµì°¨ ê²€ì¦ ë¶„ì„
            - **ë°œê¸‰**: [platform.openai.com](https://platform.openai.com)
            - **ìš”ê¸ˆ**: ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ê³¼ê¸ˆ

            âš ï¸ **ì°¸ê³ **: ì¶”ê°€ API í‚¤ ì—†ì´ë„ ê¸°ë³¸ ë¶„ì„ì€ ì™„ì „íˆ ì‘ë™í•©ë‹ˆë‹¤!
            """)


    # ë©”ì¸ ì»¨í…ì¸ 
    tab1, tab2, tab3 = st.tabs(["1ï¸âƒ£ ì¡°ë¡€ ê²€ìƒ‰", "2ï¸âƒ£ PDF ì—…ë¡œë“œ", "3ï¸âƒ£ AI ë¶„ì„"])

    with tab1:
        st.header("ì¡°ë¡€ ê²€ìƒ‰")
        
        # ê²€ìƒ‰ í¼ (Enter í‚¤ ì§€ì›)
        with st.form(key="search_form"):
            col1, col2 = st.columns([3, 1])
            with col1:
                search_query = st.text_input(
                    "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (í‚¤ì›Œë“œ)", 
                    placeholder="ì˜ˆ: ì²­ë…„ì§€ì› (Enter í‚¤ë¡œë„ ê²€ìƒ‰ ê°€ëŠ¥)", 
                    value=st.session_state.search_query,
                    help="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•œ í›„ Enter í‚¤ë¥¼ ëˆ„ë¥´ê±°ë‚˜ ê²€ìƒ‰ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
                )
            with col2:
                search_button = st.form_submit_button("ğŸ” ê²€ìƒ‰", type="primary")

        # ê²€ìƒ‰ ì‹¤í–‰ (Enter í‚¤ ë˜ëŠ” ë²„íŠ¼ í´ë¦­ ì‹œ)
        if search_button and search_query.strip():
            st.session_state.search_query = search_query.strip()
            st.session_state.word_doc_ready = False  # ë¬¸ì„œ ì¤€ë¹„ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.selected_ordinances = []  # ì„ íƒëœ ì¡°ë¡€ ì´ˆê¸°í™”
            
            with st.spinner("ê²€ìƒ‰ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                try:
                    results, total_count = search_ordinances(search_query.strip())
                    st.session_state.search_results = results
                    # ì´ˆê¸°ì—ëŠ” ëª¨ë“  ì¡°ë¡€ë¥¼ ì„ íƒëœ ìƒíƒœë¡œ ì„¤ì •
                    st.session_state.selected_ordinances = list(range(len(results)))
                    st.success(f"ê²€ìƒ‰ ì™„ë£Œ! ì´ {len(results)}ê±´ì˜ ì¡°ë¡€ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    st.session_state.search_results = []

        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì„ ë•Œ ì¡°ë¡€ ì„ íƒ ë° Word ë¬¸ì„œ ìƒì„± ê¸°ëŠ¥
        if st.session_state.search_results:
            results = st.session_state.search_results
            
            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ í‘œì‹œ
            if not st.session_state.word_doc_ready:
                st.success(f"ê²€ìƒ‰ ì™„ë£Œ! ì´ {len(results)}ê±´ì˜ ì¡°ë¡€ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ì¡°ë¡€ ì„ íƒ ì„¹ì…˜
            st.subheader("ğŸ“‹ Word ë¬¸ì„œì— í¬í•¨í•  ì¡°ë¡€ ì„ íƒ")
            
            # ì „ì²´ ì„ íƒ/í•´ì œ ë²„íŠ¼
            col1, col2, col3 = st.columns([1, 1, 2])
            
            with col1:
                if st.button("âœ… ì „ì²´ ì„ íƒ", key="select_all_btn"):
                    st.session_state.selected_ordinances = list(range(len(results)))
                    st.rerun()
            
            with col2:
                if st.button("âŒ ì „ì²´ í•´ì œ", key="deselect_all_btn"):
                    st.session_state.selected_ordinances = []
                    st.rerun()
            
            with col3:
                selected_count = len(st.session_state.selected_ordinances)
                st.markdown(f"**ì„ íƒëœ ì¡°ë¡€: {selected_count}ê°œ / ì´ {len(results)}ê°œ**")
            
            # ì¡°ë¡€ ì„ íƒ ì²´í¬ë°•ìŠ¤
            st.markdown("---")
            
            # ì¡°ë¡€ë³„ ì²´í¬ë°•ìŠ¤ í‘œì‹œ
            for idx, result in enumerate(results):
                # ğŸ†• ë‹¨ìˆœí™”: ì²´í¬ë°•ìŠ¤ ìƒíƒœë¥¼ ì§ì ‘ ê´€ë¦¬
                is_selected = idx in st.session_state.selected_ordinances
                checkbox_key = f"ordinance_checkbox_{idx}"

                # ì²´í¬ë°•ìŠ¤ì™€ ì¡°ë¡€ëª…ì„ í•œ ì¤„ì— í‘œì‹œ
                current_checked = st.checkbox(
                    f"**{result['metro']}** - {result['name']}",
                    value=is_selected,
                    key=checkbox_key
                )

                # ğŸ†• ìƒíƒœ ë³€ê²½ ê°ì§€ ë° ì¦‰ì‹œ ë°˜ì˜
                if current_checked != is_selected:
                    if current_checked:
                        # ì²´í¬ë¨ - ëª©ë¡ì— ì¶”ê°€
                        if idx not in st.session_state.selected_ordinances:
                            st.session_state.selected_ordinances.append(idx)
                    else:
                        # ì²´í¬ í•´ì œë¨ - ëª©ë¡ì—ì„œ ì œê±°
                        if idx in st.session_state.selected_ordinances:
                            st.session_state.selected_ordinances.remove(idx)
            
            st.markdown("---")
            
            # Word ë¬¸ì„œ ìƒì„± ë²„íŠ¼
            col1, col2 = st.columns([1, 1])
            
            with col1:
                # ì„ íƒëœ ì¡°ë¡€ê°€ ìˆì„ ë•Œë§Œ ìƒì„± ë²„íŠ¼ í™œì„±í™”
                disabled = len(st.session_state.selected_ordinances) == 0
                
                if st.button("ğŸ“„ ì„ íƒëœ ì¡°ë¡€ë¡œ Word ë¬¸ì„œ ìƒì„±", type="secondary", key="create_word_btn", disabled=disabled):
                    if st.session_state.selected_ordinances:
                        try:
                            with st.spinner("Word ë¬¸ì„œ ìƒì„± ì¤‘..."):
                                # ì„ íƒëœ ì¡°ë¡€ë§Œ í•„í„°ë§
                                selected_results = [results[i] for i in st.session_state.selected_ordinances]
                                
                                # Word ë¬¸ì„œ ìƒì„±
                                doc = create_word_document(st.session_state.search_query, selected_results)
                                
                                # Word ë¬¸ì„œë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
                                doc_io = io.BytesIO()
                                doc.save(doc_io)
                                doc_io.seek(0)
                                doc_bytes = doc_io.getvalue()
                                
                                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                                st.session_state.word_doc_data = doc_bytes
                                st.session_state.word_doc_ready = True
                                
                            st.success(f"âœ… ì„ íƒëœ {len(selected_results)}ê°œ ì¡°ë¡€ë¡œ Word ë¬¸ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ
                            
                        except Exception as e:
                            st.error(f"âŒ Word ë¬¸ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())
                    else:
                        st.warning("ì¡°ë¡€ë¥¼ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
                
                if disabled:
                    st.caption("âš ï¸ ì¡°ë¡€ë¥¼ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
            
            with col2:
                # Word ë¬¸ì„œê°€ ì¤€ë¹„ë˜ë©´ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ
                if st.session_state.word_doc_ready and st.session_state.word_doc_data:
                    filename = f"ì¡°ë¡€_ê²€ìƒ‰ê²°ê³¼_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
                    st.download_button(
                        label="ğŸ’¾ Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
                        data=st.session_state.word_doc_data,
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        key="download_word_btn"
                    )
            # ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ (ì¡°ë¡€ ë‚´ìš© í™•ì¸ìš©)
            st.subheader("ğŸ“– ì¡°ë¡€ ë‚´ìš© ìƒì„¸ë³´ê¸°")
            
            for idx, result in enumerate(results):
                # ğŸ†• ë‹¨ìˆœí™”: ì„ íƒ ìƒíƒœë§Œ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                is_selected = idx in st.session_state.selected_ordinances
                status = " âœ… ì„ íƒë¨" if is_selected else " â­• ì„ íƒì•ˆë¨"

                with st.expander(f"{result['metro']} - {result['name']}{status}", expanded=False):
                    st.markdown(f"<div class='metro-name'>{result['metro']}</div>", unsafe_allow_html=True)
                    st.markdown(f"<div class='law-title'>{result['name']}</div>", unsafe_allow_html=True)
                    
                    if result['content']:
                        for article_idx, article in enumerate(result['content']):
                            st.markdown(f"**ì œ{article_idx+1}ì¡°**")
                            st.markdown(article)
                            st.markdown("---")
                    else:
                        st.markdown("*(ì¡°ë¬¸ ì—†ìŒ)*")
        
        elif search_button and not search_query.strip():
            st.error("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        elif not st.session_state.search_results:
            st.info("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ê³  Enter í‚¤ë¥¼ ëˆ„ë¥´ê±°ë‚˜ ê²€ìƒ‰ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    with tab2:
        st.header("ì¡°ë¡€ì•ˆ PDF ì—…ë¡œë“œ")
        
        uploaded_file = st.file_uploader("ì œì • ë˜ëŠ” ê°œì •í•  ì¡°ë¡€ì•ˆ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf'])
        
        if uploaded_file is not None:
            st.session_state.uploaded_pdf = uploaded_file
            st.success(f"íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {uploaded_file.name}")
            
            # PDF ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° - expanderë¡œ ë³€ê²½í•˜ì—¬ ì¬ì‹¤í–‰ ë°©ì§€
            with st.expander("PDF ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                with st.spinner("PDF ë‚´ìš©ì„ ì½ëŠ” ì¤‘..."):
                    pdf_text = extract_pdf_text(uploaded_file)
                    if pdf_text:
                        st.text_area("PDF ë‚´ìš©", pdf_text[:2000] + "..." if len(pdf_text) > 2000 else pdf_text, height=300)
                    else:
                        st.error("PDF ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    with tab3:
        st.header("AI ë¹„êµ ë¶„ì„")

        # ì¡°ê±´ í™•ì¸ - PDFê°€ ì—…ë¡œë“œë˜ê³  AI ì„œë¹„ìŠ¤ê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ë¶„ì„ ê°€ëŠ¥
        pdf_uploaded = st.session_state.uploaded_pdf is not None
        has_ollama = st.session_state.use_ollama_cloud and bool(st.session_state.ollama_api_key and st.session_state.ollama_api_key != "YOUR_OLLAMA_API_KEY_HERE")
        has_api_key = bool(gemini_api_key or openai_api_key) or has_ollama
        has_search_results = bool(st.session_state.search_results)

        if not pdf_uploaded:
            st.warning("ğŸ“„ ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        elif not has_api_key:
            st.warning("ğŸ”‘ AI ë¶„ì„ ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        else:
            # ê²€ìƒ‰ ê²°ê³¼ ì—¬ë¶€ì— ë”°ë¼ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ
            if not has_search_results:
                st.info("ğŸ’¡ **ìµœì´ˆ ì œì • ì¡°ë¡€ ë¶„ì„**")
                st.markdown("""
                ê²€ìƒ‰ëœ íƒ€ ì‹œë„ ì¡°ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
                - ğŸ†• **ìµœì´ˆ ì œì • ì¡°ë¡€**: 17ê°œ ì‹œë„ ì¤‘ ìµœì´ˆë¡œ ì œì •ë˜ëŠ” ì¡°ë¡€
                - ğŸ” **ê²€ìƒ‰ì–´ ë¶ˆì¼ì¹˜**: ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì¬ê²€ìƒ‰ í›„ ë¶„ì„ ê¶Œì¥
                
                ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ ì¡°ë¡€ì•ˆì˜ **ë²•ì  ê²€í† **ì™€ **ìƒìœ„ë²•ë ¹ ìœ„ë°˜ ì—¬ë¶€** ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
                """)
            else:
                st.success(f"ğŸ“Š {len(st.session_state.search_results)}ê°œì˜ íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.")
        
        # ë¶„ì„ ê°€ëŠ¥í•œ ì¡°ê±´ì¼ ë•Œ ë¶„ì„ ì¸í„°í˜ì´ìŠ¤ í‘œì‹œ
        if pdf_uploaded and has_api_key:
            # ê²€ìƒ‰ì–´ ì…ë ¥ (ì„ íƒì‚¬í•­)
            search_query_analysis = st.text_input(
                "ê²€ìƒ‰ì–´ (ë¶„ì„ìš©)", 
                value=st.session_state.search_query if st.session_state.search_query else "", 
                key="analysis_query",
                help="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ì„ íƒì‚¬í•­)"
            )
            
            # ë¶„ì„ íƒ€ì… í‘œì‹œ (ì„ íƒëœ ì¡°ë¡€ ìˆ˜ ë°˜ì˜)
            if not has_search_results:
                analysis_type = "ìµœì´ˆ ì œì • ì¡°ë¡€ ë¶„ì„"
            elif hasattr(st.session_state, 'selected_ordinances') and st.session_state.selected_ordinances:
                selected_count = len(st.session_state.selected_ordinances)
                analysis_type = f"ì„ íƒëœ {selected_count}ê°œ íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµ ë¶„ì„"
            else:
                analysis_type = f"ì „ì²´ {len(st.session_state.search_results)}ê°œ íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµ ë¶„ì„"
            st.markdown(f"**ë¶„ì„ ìœ í˜•**: {analysis_type}")
            
            # ìë™ ì°¸ê³  ìë£Œ ê²€ìƒ‰ ì˜µì…˜ (ë¬¸ì œ ë°œê²¬ ì‹œ ìë™ í™œìš©)
            use_auto_search = st.checkbox(
                "ğŸ” ë¬¸ì œ ë°œê²¬ ì‹œ ìë™ ì°¸ê³  ìë£Œ ê²€ìƒ‰",
                value=True,
                help="ë²•ì  ë¬¸ì œì ì„ ë°œê²¬í•œ ê²½ìš° Gemini File Searchë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ê´€ë ¨ íŒë¡€ ë° ë²•ë ¹ ìë£Œë¥¼ ê²€ìƒ‰í•˜ì—¬ ê·¼ê±°ë¥¼ ë³´ê°•í•©ë‹ˆë‹¤."
            )
            
            # ğŸ†• ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë¨¼ì € í‘œì‹œ
            if hasattr(st.session_state, 'analysis_results') and st.session_state.analysis_results:
                st.info("ğŸ’¾ **ì´ì „ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤**")

                # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                if hasattr(st.session_state, 'analysis_metadata'):
                    metadata = st.session_state.analysis_metadata
                    st.caption(f"ğŸ“… ë¶„ì„ ì‹œê°„: {metadata.get('analysis_timestamp', 'ì•Œ ìˆ˜ ì—†ìŒ')}")

                col1, col2 = st.columns(2)
                with col1:
                    if st.button("ğŸ“‹ ì´ì „ ë¶„ì„ ê²°ê³¼ ë³´ê¸°", use_container_width=True):
                        st.session_state.show_previous_analysis = True
                        st.rerun()
                with col2:
                    if st.button("ğŸ”„ ìƒˆë¡œ ë¶„ì„í•˜ê¸°", use_container_width=True):
                        # ê¸°ì¡´ ê²°ê³¼ ì´ˆê¸°í™”
                        if hasattr(st.session_state, 'analysis_results'):
                            del st.session_state.analysis_results
                        if hasattr(st.session_state, 'analysis_metadata'):
                            del st.session_state.analysis_metadata
                        if hasattr(st.session_state, 'show_previous_analysis'):
                            del st.session_state.show_previous_analysis
                        st.rerun()

            # ì´ì „ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
            if hasattr(st.session_state, 'show_previous_analysis') and st.session_state.show_previous_analysis and hasattr(st.session_state, 'analysis_results'):
                analysis_results = st.session_state.analysis_results
                metadata = st.session_state.analysis_metadata

                st.markdown("---")
                st.subheader("ğŸ“‹ ì €ì¥ëœ AI ë¶„ì„ ê²°ê³¼")

                # ë¶„ì„ ì™„ë£Œ ë©”ì‹œì§€ (ì €ì¥ëœ ë©”íƒ€ë°ì´í„° ê¸°ë°˜)
                has_problems = metadata.get('has_problems', False)
                relevant_guidelines = metadata.get('relevant_guidelines')
                loaded_stores = metadata.get('loaded_stores')
                is_first_ordinance = metadata.get('is_first_ordinance', False)

                if has_problems and relevant_guidelines and loaded_stores:
                    st.success(f"ğŸ¯ **ë³µí•© ìë£Œ ë³´ê°• ë¶„ì„ ì™„ë£Œ**: ë¬¸ì œì  íƒì§€ â†’ {len(loaded_stores)}ê°œ ìë£Œ ì°¸ê³  â†’ ë³´ê°• ë¶„ì„")
                elif has_problems and relevant_guidelines:
                    st.success("ğŸ¯ **ì§€ëŠ¥í˜• ë¶„ì„ ì™„ë£Œ**: ë¬¸ì œì  íƒì§€ â†’ ìë£Œ ê²€ìƒ‰ â†’ ë³´ê°• ë¶„ì„")
                elif has_problems:
                    st.info("âš ï¸ **ë¬¸ì œì  íƒì§€ ë¶„ì„ ì™„ë£Œ**: ìë£Œ ê²€ìƒ‰ ì—†ì´ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰")
                else:
                    st.success("âœ… **ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ**: íŠ¹ë³„í•œ ë¬¸ì œì ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ")

                # ë¶„ì„ ê²°ê³¼ ìš”ì•½
                analysis_count = len([r for r in analysis_results if 'error' not in r])
                if analysis_count > 0:
                    # ğŸ†• ì €ì¥ëœ ë©”íƒ€ë°ì´í„°ì—ì„œ ì„ íƒëœ ì¡°ë¡€ ìˆ˜ ë°˜ì˜
                    if is_first_ordinance:
                        analysis_type_text = "ìµœì´ˆ ì œì • ì¡°ë¡€"
                    else:
                        saved_search_results = metadata.get('search_results_for_analysis', [])
                        selected_count = len(saved_search_results)
                        analysis_type_text = f"ì„ íƒëœ {selected_count}ê°œ íƒ€ ì‹œë„ ì¡°ë¡€ ë¹„êµ"
                    st.markdown(f"**ğŸ“‹ ë¶„ì„ ìœ í˜•**: {analysis_type_text}")
                    st.markdown(f"**ğŸ¤– ìˆ˜í–‰ëœ ë¶„ì„**: {analysis_count}ê°œ")
                    if relevant_guidelines:
                        guideline_count = len(relevant_guidelines) if isinstance(relevant_guidelines, list) else 0
                        st.markdown(f"**ğŸ“š ì°¸ê³  ê°€ì´ë“œë¼ì¸**: {guideline_count}ê°œ")

                # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                for result in analysis_results:
                    if 'error' not in result:
                        final_report = result
                        # ëª¨ë¸ì— ë”°ë¥¸ êµ¬ë¶„ í‘œì‹œ
                        if "ë³´ê°•" in final_report['model']:
                            st.success("ğŸ¯ **ë³µí•© ìë£Œ ì°¸ê³  ë³´ê°• ë¶„ì„ ê²°ê³¼**")
                            st.caption(f"ğŸ“š **í™œìš© ëª¨ë¸**: {final_report['model']}")
                        elif "ìë£Œ ì°¸ê³ " in final_report['model']:
                            st.success("ğŸ¯ **ì°¸ê³  ìë£Œ ê¸°ë°˜ ë³´ê°• ë¶„ì„ ê²°ê³¼**")
                        elif "OpenAI" in final_report['model']:
                            st.info("ğŸ“Š **OpenAI ì¶”ê°€ ë¶„ì„ ê²°ê³¼**")
                        else:
                            st.info("ğŸ¤– **Gemini ê¸°ë³¸ ë¶„ì„ ê²°ê³¼**")
                        # ë³´ê³ ì„œ ë‚´ìš©
                        st.markdown(final_report['content'])

                # ì˜¤ë¥˜ ë©”ì‹œì§€ í‘œì‹œ
                for result in analysis_results:
                    if 'error' in result:
                        st.error(f"âŒ {result['model']} ì˜¤ë¥˜: {result['error']}")

                # Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ (ë©”íƒ€ë°ì´í„°ì—ì„œ ë³µì›)
                with st.spinner("ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ Word ë¬¸ì„œ ìƒì„± ì¤‘..."):
                    superior_laws_content = metadata.get('superior_laws_content')
                    search_results_for_analysis = metadata.get('search_results_for_analysis')
                    pdf_text = metadata.get('pdf_text')
                    doc = create_comparison_document(pdf_text, search_results_for_analysis, analysis_results, superior_laws_content, relevant_guidelines)
                    doc_io = io.BytesIO()
                    doc.save(doc_io)
                    doc_bytes = doc_io.getvalue()
                    # íŒŒì¼ëª… ì„¤ì •
                    if has_problems and relevant_guidelines and loaded_stores:
                        stores_count = len(loaded_stores)
                        filename_prefix = f"ë³µí•©ìë£Œë³´ê°•ë¶„ì„({stores_count}ê°œìë£Œ)" if is_first_ordinance else f"ì¡°ë¡€ë¹„êµ_ë³µí•©ìë£Œë¶„ì„({stores_count}ê°œìë£Œ)"
                    elif has_problems and relevant_guidelines:
                        filename_prefix = "ìë£Œì°¸ê³ ë³´ê°•ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€ë¹„êµ_ìë£Œë¶„ì„"
                    elif has_problems:
                        filename_prefix = "ë¬¸ì œì íƒì§€ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€ë¹„êµ_ë¬¸ì œì ë¶„ì„"
                    else:
                        filename_prefix = "ìµœì´ˆì¡°ë¡€_ê¸°ë³¸ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€_ê¸°ë³¸ë¹„êµë¶„ì„"
                    st.download_button(
                        label="ğŸ“„ ë¶„ì„ ê²°ê³¼ Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
                        data=doc_bytes,
                        file_name=f"{filename_prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx",
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        key="download_previous_analysis"
                    )

                st.markdown("---")
                st.markdown("ğŸ’¡ **ìƒˆë¡œ ë¶„ì„í•˜ë ¤ë©´ ìœ„ì˜ 'ğŸ”„ ìƒˆë¡œ ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.**")

            else:
                # ì €ì¥ëœ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ìƒˆ ë¶„ì„ì„ ì„ íƒí•œ ê²½ìš°ë§Œ ë¶„ì„ ì‹œì‘ ë²„íŠ¼ í‘œì‹œ
                # ğŸ†• ì„ íƒëœ ì¡°ë¡€ê°€ ì—†ëŠ” ê²½ìš° ê²½ê³  í‘œì‹œ
                if has_search_results and hasattr(st.session_state, 'selected_ordinances') and not st.session_state.selected_ordinances:
                    st.warning("âš ï¸ ë¹„êµí•  ì¡°ë¡€ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¡°ë¡€ ê²€ìƒ‰ íƒ­ì—ì„œ ì¡°ë¡€ë¥¼ ì„ íƒí•˜ê±°ë‚˜, ì„ íƒ ì—†ì´ ìµœì´ˆ ì œì • ì¡°ë¡€ ë¶„ì„ì„ ì§„í–‰í•˜ì„¸ìš”.")

                if st.button("ğŸ¤– AI ë¶„ì„ ì‹œì‘", type="primary"):
                    with st.spinner("AIê°€ ì¡°ë¡€ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        pdf_text = extract_pdf_text(st.session_state.uploaded_pdf)
                    
                    if not pdf_text:
                        st.error("PDF í…ìŠ¤íŠ¸ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        # 1ë‹¨ê³„: ìƒìœ„ë²•ë ¹ ì¶”ì¶œ
                        with st.spinner("ì¡°ë¡€ì•ˆì—ì„œ ìƒìœ„ë²•ë ¹ì„ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                            superior_laws = extract_superior_laws(pdf_text)
                        
                        if superior_laws:
                            # 2ë‹¨ê³„: ìƒìœ„ë²•ë ¹ ë‚´ìš© ì¡°íšŒ
                            with st.spinner("êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°ì—ì„œ ìƒìœ„ë²•ë ¹ ë‚´ìš©ì„ ì¡°íšŒí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                                superior_laws_content = get_all_superior_laws_content(superior_laws)
                            
                            if superior_laws_content:
                                # ìƒìœ„ë²•ë ¹ ì¡°íšŒ ì„±ê³µ (ë””ë²„ê·¸ ë©”ì‹œì§€ ì œê±°)
                                pass

                                # ğŸ†• ìƒìœ„ë²•ë ¹ ë³¸ë¬¸ ë‚´ìš© ë””ë²„ê¹… í‘œì‹œ (expanderë¡œ ë³€ê²½í•˜ì—¬ ì¬ì‹¤í–‰ ë°©ì§€)
                                with st.expander("ğŸ” Geminiê°€ ì°¸ì¡°í•  ìƒìœ„ë²•ë ¹ ë³¸ë¬¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                                    for i, law_group in enumerate(superior_laws_content):
                                        st.markdown(f"### [{i+1}] {law_group['base_name']}")

                                        # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°
                                        if 'combined_content' in law_group and law_group['combined_content']:
                                            content = law_group['combined_content']
                                            st.markdown(f"**ë³¸ë¬¸ ê¸¸ì´**: {len(content):,}ì")
                                            st.text_area(
                                                f"{law_group['base_name']} ë³¸ë¬¸",
                                                content,
                                                height=200,
                                                key=f"content_{i}"
                                            )
                                        else:
                                            # ê°œë³„ ë²•ë ¹ë³„ í‘œì‹œ
                                            for law_type, law_info in law_group['laws'].items():
                                                if law_info and 'articles' in law_info:
                                                    type_name = {"law": "ë²•ë¥ ", "decree": "ì‹œí–‰ë ¹", "rule": "ì‹œí–‰ê·œì¹™"}[law_type]
                                                    st.markdown(f"#### {type_name}")

                                                    # ì¡°ë¬¸ë³„ ë‚´ìš© í‘œì‹œ (ì²˜ìŒ 5ê°œë§Œ)
                                                    for j, article in enumerate(law_info['articles'][:5]):
                                                        st.markdown(f"**ì œ{article.get('number', '?')}ì¡°** {article.get('title', '')}")
                                                        content = article.get('content', '')[:500]
                                                        st.markdown(f"```\n{content}{'...' if len(article.get('content', '')) > 500 else ''}\n```")

                                                    if len(law_info['articles']) > 5:
                                                        st.markdown(f"... (ì´ {len(law_info['articles'])}ê°œ ì¡°ë¬¸ ì¤‘ 5ê°œë§Œ í‘œì‹œ)")

                                        st.markdown("---")
                                
                                # 2-1ë‹¨ê³„: ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„
                                try:
                                    comparison_results = analyze_ordinance_vs_superior_laws(pdf_text, superior_laws_content)
                                    
                                    if comparison_results and isinstance(comparison_results, list) and len(comparison_results) > 0:
                                        st.warning(f"âš ï¸ {len(comparison_results)}ê°œ ì¡°ë¬¸ì—ì„œ ì ì¬ì  ë¬¸ì œì ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                        
                                        with st.expander("ğŸ” ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ ê²°ê³¼", expanded=True):
                                            for i, result in enumerate(comparison_results):
                                                st.markdown(f"**ğŸ” {result['ordinance_article']}**")
                                                st.markdown(f"ì¡°ë¡€ ë‚´ìš©: {result['ordinance_content'][:300]}...")
                                                
                                                if result['delegation_issues']:
                                                    st.error("âš ï¸ **ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ê°€ëŠ¥ì„± ë°œê²¬**")
                                                    for issue in result['delegation_issues']:
                                                        st.markdown(f"- **ê´€ë ¨ ìƒìœ„ë²•ë ¹**: {issue['superior_law']} {issue['superior_article']}")
                                                        st.markdown(f"- **ë¬¸ì œì **: {issue['description']}")
                                                        st.markdown(f"- **ìƒìœ„ë²•ë ¹ ë‚´ìš©**: {issue['superior_content'][:200]}...")
                                                
                                                if result['superior_law_conflicts']:
                                                    st.error("ğŸš¨ **ìƒìœ„ë²•ë ¹ ì¶©ëŒ ê°€ëŠ¥ì„± ë°œê²¬**")
                                                    for conflict in result['superior_law_conflicts']:
                                                        st.markdown(f"- **ê´€ë ¨ ìƒìœ„ë²•ë ¹**: {conflict['superior_law']} {conflict['superior_article']}")
                                                        st.markdown(f"- **ì¶©ëŒ ìœ í˜•**: {conflict['conflict_type']}")
                                                        st.markdown(f"- **ìƒìœ„ë²•ë ¹ ë‚´ìš©**: {conflict['superior_content'][:200]}...")
                                                
                                                st.markdown("---")
                                    else:
                                        st.success("âœ… ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµì—ì„œ ëª…ë°±í•œ ì¶©ëŒì´ë‚˜ ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ë¬¸ì œë¥¼ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                                        
                                except Exception as e:
                                    st.error(f"ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                                
                                # ìƒìœ„ë²•ë ¹ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ê³„ì¸µë³„ ê·¸ë£¹í™”) - expanderë¡œ ë³€ê²½í•˜ì—¬ ì¬ì‹¤í–‰ ë°©ì§€
                                with st.expander("ğŸ” ì¡°íšŒëœ ìƒìœ„ë²•ë ¹ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ê³„ì¸µë³„)", expanded=False):
                                    for idx, law_group in enumerate(superior_laws_content):
                                        base_name = law_group['base_name']

                                        # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°
                                        if 'combined_content' in law_group:
                                            content_preview = law_group['combined_content'][:500] + "..." if len(law_group['combined_content']) > 500 else law_group['combined_content']
                                            with st.expander(f"ğŸ“‹ {base_name} ({len(law_group['combined_content']):,}ì)", expanded=False):
                                                st.text_area("ë³¸ë¬¸ ë‚´ìš©", content_preview, height=300, disabled=True, key=f"content_{base_name}_{idx}")
                                        else:
                                            # ê¸°ì¡´ ë°©ì‹
                                            with st.expander(f"ğŸ“‹ {base_name} ê³„ì¸µ ({len(law_group.get('combined_articles', []))}ê°œ ì¡°ë¬¸)", expanded=False):

                                                # ê³„ì¸µë³„ ë²•ë ¹ ì •ë³´ í‘œì‹œ
                                                st.markdown("**ğŸ“š í¬í•¨ëœ ë²•ë ¹:**")
                                                for law_type, law_info in law_group['laws'].items():
                                                    if law_info and 'articles' in law_info:
                                                        type_name = "ë²•ë¥ " if law_type == 'law' else ("ì‹œí–‰ë ¹" if law_type == 'decree' else "ì‹œí–‰ê·œì¹™")
                                                        st.markdown(f"- [{type_name}] {law_info['law_name']} ({len(law_info['articles'])}ê°œ ì¡°ë¬¸)")

                                                st.markdown("\n**ğŸ“– í†µí•© ì¡°ë¬¸ (ì²˜ìŒ 5ê°œ):**")
                                                combined_articles = law_group.get('combined_articles', [])
                                                for article in combined_articles[:5]:
                                                    st.markdown(f"**{article['number']} {article['title']}**")
                                                    st.markdown(article['content'][:200] + "..." if len(article['content']) > 200 else article['content'])
                                                    st.markdown("---")
                                                if len(combined_articles) > 5:
                                                    st.markdown(f"*(ì´ {len(combined_articles)}ê°œ ì¡°ë¬¸ ì¤‘ 5ê°œë§Œ í‘œì‹œ)*")
                            else:
                                st.warning("âš ï¸ ìƒìœ„ë²•ë ¹ ë‚´ìš© ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
                        else:
                            st.info("â„¹ï¸ ì¡°ë¡€ì•ˆì—ì„œ ëª…ì‹œì ì¸ ìƒìœ„ë²•ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            superior_laws_content = None
                        
                        # 3ë‹¨ê³„: Gemini 1ì°¨ ë¶„ì„ (ë¬¸ì œì  íƒì§€)
                        analysis_results = []
                        is_first_ordinance = not has_search_results

                        # ğŸ†• ì„ íƒëœ ì¡°ë¡€ë§Œ ë¶„ì„ì— ì‚¬ìš©
                        if has_search_results and hasattr(st.session_state, 'selected_ordinances'):
                            selected_results = [st.session_state.search_results[i] for i in st.session_state.selected_ordinances if i < len(st.session_state.search_results)]
                            search_results_for_analysis = selected_results
                            st.info(f"ğŸ“‹ ì„ íƒëœ {len(search_results_for_analysis)}ê°œ ì¡°ë¡€ë¡œ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
                        else:
                            search_results_for_analysis = st.session_state.search_results if has_search_results else []

                        # ğŸ†• 3-1ë‹¨ê³„: ìœ„ë²• íŒë¡€ ì„ ì œ ê²€ìƒ‰ (ëª¨ë“  ì¡°ë¬¸ì— ëŒ€í•´)
                        theoretical_results = []
                        if st.session_state.gemini_store_manager and gemini_api_key:
                            with st.spinner("ğŸ“š ì—…ë¡œë“œëœ ì¡°ë¡€ì˜ ëª¨ë“  ì¡°ë¬¸ì— ëŒ€í•œ ìœ„ë²• íŒë¡€ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                                try:
                                    # PDFì—ì„œ ì¡°ë¡€ëª… ì¶”ì¶œ (ì²˜ìŒ 10ì¤„ì—ì„œ)
                                    ordinance_name = ""
                                    # re ëª¨ë“ˆì€ íŒŒì¼ ìƒë‹¨ì—ì„œ ì´ë¯¸ importë¨
                                    lines = pdf_text.split('\n')
                                    for line in lines[:10]:
                                        line = line.strip()
                                        # ì¡°ë¡€ëª… íŒ¨í„´: "â—‹â—‹ì‹œ â—‹â—‹ ì¡°ë¡€" ë˜ëŠ” "â—‹â—‹ì— ê´€í•œ ì¡°ë¡€"
                                        name_match = re.search(r'([\wê°€-í£]+(?:ì‹œ|ë„|êµ°|êµ¬)\s+[\wê°€-í£\s]+(?:ì¡°ë¡€|ì¡°ë¡€ì•ˆ))', line)
                                        if not name_match:
                                            name_match = re.search(r'([\wê°€-í£\s]+ì—\s+ê´€í•œ\s+ì¡°ë¡€(?:ì•ˆ)?)', line)
                                        if name_match:
                                            ordinance_name = name_match.group(1).strip()
                                            st.info(f"ğŸ“‹ ì¡°ë¡€ëª…: {ordinance_name}")
                                            break

                                    # ì¡°ë¡€ì—ì„œ ëª¨ë“  ì¡°ë¬¸ ì¶”ì¶œ
                                    ordinance_articles = []
                                    current_article = ""
                                    current_content = ""

                                    # ì¡°ë¡€ëª…ì„ ì²« ë²ˆì§¸ í•­ëª©ìœ¼ë¡œ ì¶”ê°€ (ê²€ìƒ‰ì— í™œìš©)
                                    if ordinance_name:
                                        ordinance_articles.append(f"ì¡°ë¡€ëª…: {ordinance_name}")

                                    for line in lines:
                                        line = line.strip()
                                        if line.startswith('ì œ') and 'ì¡°' in line:
                                            if current_article and current_content:
                                                ordinance_articles.append(f"{current_article} {current_content.strip()}")
                                            current_article = line
                                            current_content = ""
                                        else:
                                            current_content += line + " "

                                    # ë§ˆì§€ë§‰ ì¡°ë¬¸ ì¶”ê°€
                                    if current_article and current_content:
                                        ordinance_articles.append(f"{current_article} {current_content.strip()}")

                                    # âŒ ì¡°ê¸° ê²€ìƒ‰ ì œê±°: ì¡°ë¬¸ë§Œìœ¼ë¡œëŠ” ë§¥ë½ì´ ë¶€ì¡±í•˜ì—¬ RAG íš¨ê³¼ê°€ ë‚®ìŒ
                                    # ëŒ€ì‹  1ì°¨ ë¶„ì„ í›„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë°€ ê²€ìƒ‰ ìˆ˜í–‰ (2480í–‰ ì°¸ì¡°)
                                    # if ordinance_articles:
                                    #     theoretical_results_raw = search_violation_cases_gemini(
                                    #         ordinance_articles=ordinance_articles,
                                    #         api_key=gemini_api_key,
                                    #         store_manager=st.session_state.gemini_store_manager,
                                    #         max_results=12
                                    #     )
                                    #     theoretical_results = theoretical_results_raw
                                    #
                                    #     if theoretical_results:
                                    #         st.success(f"âœ… {len(theoretical_results)}ê°œì˜ ê´€ë ¨ ìœ„ë²• íŒë¡€/ì¬ì˜ì œì†Œ ì‚¬ë¡€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

                                    # ì´ˆê¸°í™”: 1ì°¨ ë¶„ì„ í›„ ì¬ê²€ìƒ‰ìœ¼ë¡œ ì±„ì›Œì§ˆ ì˜ˆì •
                                    theoretical_results = []

                                    # ì„¸ì…˜ì— ì €ì¥í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì—ì„œ ì‚¬ìš© (ë‚˜ì¤‘ì— ì¬ê²€ìƒ‰ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨)
                                    st.session_state.theoretical_results = theoretical_results

                                    # ë¯¸ë¦¬ë³´ê¸° ì œê±°: ì¡°ê¸° ê²€ìƒ‰ì„ ì œê±°í–ˆìœ¼ë¯€ë¡œ ì´ ì‹œì ì—ëŠ” ë¹„ì–´ìˆìŒ
                                    # 1ì°¨ ë¶„ì„ í›„ ì •ë°€ ê²€ìƒ‰ ê²°ê³¼ëŠ” 2580í–‰ì˜ "ì •ë°€ ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°"ì—ì„œ í‘œì‹œë¨

                                except Exception as e:
                                    st.warning(f"âš ï¸ ì¡°ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {str(e)}")
                                    theoretical_results = []
                                    st.session_state.theoretical_results = theoretical_results

                        # AI 1ì°¨ ë¶„ì„ (ë¬¸ì œì  íƒì§€ìš©) - Ollama Cloud ìš°ì„  ì‚¬ìš©
                        first_analysis = None
                        has_problems = False
                        analysis_model_name = ""
                        rag_context = ""

                        # Ollama Cloudë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©
                        if has_ollama:
                            try:
                                # comprehensive_analysis_results ì´ˆê¸°í™”
                                comprehensive_analysis_results = None

                                # RAG ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë° ê²€ìƒ‰
                                with st.spinner("ğŸ“š ìì¹˜ë²•ê·œ ë§¤ë‰´ì–¼ ë° íŒë¡€ ìë£Œë¥¼ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                                    vectorstores = load_rag_vectorstores()

                                if vectorstores:
                                    # ì¡°ë¡€ëª… ê°€ì ¸ì˜¤ê¸° (ì´ì „ì— ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸)
                                    current_ordinance_name = st.session_state.get('current_ordinance_name', '')
                                    if not current_ordinance_name:
                                        # PDF í…ìŠ¤íŠ¸ì—ì„œ ì¡°ë¡€ëª… ì¶”ì¶œ ì‹œë„
                                        name_match = re.search(r'([ê°€-í£\s]+(?:ì¡°ë¡€|ê·œì¹™))', pdf_text[:500])
                                        if name_match:
                                            current_ordinance_name = name_match.group(1).strip()

                                    # ì¡°ë¡€ ë‚´ìš©ì—ì„œ ì ì¬ì  ìœ„ë²•ì„± í‚¤ì›Œë“œ ì¶”ì¶œ
                                    potential_issues = []
                                    ordinance_sample = pdf_text[:3000]  # ì²˜ìŒ 3000ì ë¶„ì„

                                    # ìœ„ë²•ì„± ê´€ë ¨ íŒ¨í„´ ê°ì§€
                                    issue_patterns = {
                                        'ìˆ˜ìˆ˜ë£Œ': ['ìˆ˜ìˆ˜ë£Œ', 'ì‚¬ìš©ë£Œ', 'ìš”ê¸ˆ', 'ë¶€ë‹´ê¸ˆ'],
                                        'ë²Œì¹™': ['ë²Œì¹™', 'ê³¼íƒœë£Œ', 'ê³¼ì§•ê¸ˆ', 'ë²Œê¸ˆ', 'ì œì¬'],
                                        'ê¶Œë¦¬ì œí•œ': ['ì œí•œ', 'ê¸ˆì§€', 'ì˜ë¬´', 'í—ˆê°€', 'ì‹ ê³ ', 'ë“±ë¡'],
                                        'ì¬ì •': ['ì§€ì›', 'ë³´ì¡°ê¸ˆ', 'ì¶œì—°', 'ì˜ˆì‚°', 'ì¬ì •'],
                                        'ì¡°ì§': ['ìœ„ì›íšŒ', 'í˜‘ì˜íšŒ', 'ì‹¬ì˜íšŒ', 'ê¸°êµ¬', 'ì¡°ì§'],
                                        'ì¸ì‚¬': ['ì„ëª…', 'ìœ„ì´‰', 'í•´ì„', 'ê²¸ì§', 'ìê²©'],
                                        'ìœ„ì„': ['ìœ„ì„', 'ëŒ€í–‰', 'ìœ„íƒ', 'ëŒ€ë¦¬'],
                                        'ì£¼ë¯¼ê¶Œë¦¬': ['ì£¼ë¯¼', 'ì²­êµ¬', 'íˆ¬í‘œ', 'ì°¸ì—¬', 'ê³µê°œ']
                                    }

                                    for issue_type, keywords in issue_patterns.items():
                                        for keyword in keywords:
                                            if keyword in ordinance_sample:
                                                potential_issues.append(issue_type)
                                                break

                                    # ì¤‘ë³µ ì œê±°
                                    potential_issues = list(set(potential_issues))

                                    # ì¡°ë¡€ ë‚´ìš© ê¸°ë°˜ ë™ì  ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                                    if potential_issues:
                                        issue_keywords = ' '.join(potential_issues[:3])  # ìµœëŒ€ 3ê°œ ì´ìŠˆ
                                        search_query = f"{current_ordinance_name} {issue_keywords} ì¡°ë¡€ ìœ„ë²• íŒë‹¨"
                                        st.info(f"ğŸ” ê°ì§€ëœ ì ì¬ì  ê²€í†  í•„ìš” ì‚¬í•­: {', '.join(potential_issues)}")
                                    else:
                                        search_query = f"{current_ordinance_name} ì¡°ë¡€ ìœ„ë²• íŒë‹¨ ê¸°ì¤€ ìì¹˜ì‚¬ë¬´"

                                    rag_results = search_rag_context(search_query, vectorstores, top_k=5)

                                    if rag_results:
                                        st.success(f"âœ… {len(rag_results)}ê°œì˜ ê´€ë ¨ ìì¹˜ë²•ê·œ ìë£Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

                                        # RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                                        rag_context = "\n\n[ì°¸ê³  ìë£Œ: ìì¹˜ë²•ê·œ ë§¤ë‰´ì–¼ ë° ì¬ì˜Â·ì œì†Œ íŒë¡€]\n"
                                        for i, result in enumerate(rag_results[:5], 1):
                                            source_name = "ìì¹˜ë²•ê·œ ë§¤ë‰´ì–¼" if result['source'] == 'manual' else "ì¬ì˜Â·ì œì†Œ íŒë¡€"
                                            rag_context += f"\n--- {source_name} ì°¸ê³ ìë£Œ {i} ---\n"
                                            rag_context += result['text'][:1500] + "\n"

                                        rag_context += "\n[ì¤‘ìš”] ìœ„ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ ìœ„ë²• ì—¬ë¶€ë¥¼ ì‹ ì¤‘í•˜ê²Œ íŒë‹¨í•˜ì„¸ìš”. ë‹¨ìˆœíˆ ìƒìœ„ë²•ê³¼ ë‹¤ë¥´ë‹¤ê³  í•´ì„œ ìœ„ë²•í•œ ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. ìì¹˜ì‚¬ë¬´ì™€ ìœ„ì„ì‚¬ë¬´ë¥¼ êµ¬ë¶„í•˜ê³ , ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ì¡°ë¡€ì œì •ê¶Œ ë²”ìœ„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.\n"

                                        with st.expander("ğŸ“– RAG ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                                            for i, result in enumerate(rag_results[:5], 1):
                                                source_name = "ìì¹˜ë²•ê·œ ë§¤ë‰´ì–¼" if result['source'] == 'manual' else "ì¬ì˜Â·ì œì†Œ íŒë¡€"
                                                st.markdown(f"**{i}. {source_name}** (ì ìˆ˜: {result.get('score', 0)})")
                                                st.text(result['text'][:500] + "...")
                                                st.markdown("---")
                                    else:
                                        st.info("RAG ê²€ìƒ‰ì—ì„œ ê´€ë ¨ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                                # 1ì°¨ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± (RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
                                theoretical_results = st.session_state.get('theoretical_results', None)
                                first_prompt = create_analysis_prompt(pdf_text, search_results_for_analysis, superior_laws_content, None, is_first_ordinance, comprehensive_analysis_results, theoretical_results)

                                # RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ ì•ë¶€ë¶„ì— ì¶”ê°€
                                if rag_context:
                                    first_prompt = rag_context + "\n\n" + first_prompt

                                # Ollama Cloud ì „ì†¡ í”„ë¡¬í”„íŠ¸ ë””ë²„ê¹… í‘œì‹œ
                                with st.expander("ğŸ” AIì—ê²Œ ì „ì†¡ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ë‚´ìš© í™•ì¸", expanded=False):
                                    st.markdown("### í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ë¶„ì„")
                                    st.markdown(f"**ì „ì²´ ê¸¸ì´**: {len(first_prompt):,}ì")
                                    st.markdown(f"**ì‚¬ìš© ëª¨ë¸**: Ollama Cloud (gpt-oss:120b)")
                                    if rag_context:
                                        st.markdown(f"**RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨**: âœ… ({len(rag_context):,}ì)")

                                    # ì „ì²´ í”„ë¡¬í”„íŠ¸ í‘œì‹œ (ì²˜ìŒ 2000ìë§Œ)
                                    st.text_area(
                                        "ì „ì²´ í”„ë¡¬í”„íŠ¸ (ì²˜ìŒ 2000ì)",
                                        first_prompt[:2000] + "..." if len(first_prompt) > 2000 else first_prompt,
                                        height=400,
                                        key="full_prompt_ollama"
                                    )

                                with st.spinner("ğŸ¤– Ollama Cloud AIê°€ ì¡°ë¡€ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                                    response_text = call_ollama_cloud_api(first_prompt)

                                if response_text:
                                    first_analysis = response_text
                                    analysis_model_name = "Ollama Cloud (gpt-oss:120b)"

                                    # ë¬¸ì œì  í‚¤ì›Œë“œ íƒì§€
                                    problem_keywords = [
                                        "ìœ„ë°˜", "ë¬¸ì œ", "ì¶©ëŒ", "ë¶€ì ì ˆ", "ê°œì„ ", "ìˆ˜ì •", "ë³´ì™„",
                                        "ë²•ë ¹ ìœ„ë°˜", "ìƒìœ„ë²•ë ¹", "ìœ„ë²•", "ë¶ˆì¼ì¹˜", "ëª¨ìˆœ", "ìš°ë ¤"
                                    ]

                                    has_problems = any(keyword in first_analysis for keyword in problem_keywords)

                                    if has_problems:
                                        st.warning(f"âš ï¸ AIê°€ ì ì¬ì  ë¬¸ì œì ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")

                                    # ë¶„ì„ ê²°ê³¼ ì €ì¥
                                    analysis_results.append({
                                        'model': 'Ollama Cloud (1ì°¨ ë¶„ì„)',
                                        'analysis': first_analysis,
                                        'has_problems': has_problems
                                    })
                                else:
                                    st.error("Ollama Cloud 1ì°¨ ë¶„ì„ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

                            except Exception as e:
                                st.error(f"Ollama Cloud 1ì°¨ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                analysis_results.append({
                                    'model': 'Ollama Cloud (1ì°¨ ë¶„ì„)',
                                    'analysis': '',
                                    'error': str(e)
                                })

                        # Gemini APIê°€ ìˆìœ¼ë©´ ì¶”ê°€ ë¶„ì„ (ì„ íƒì )
                        elif gemini_api_key:
                            try:
                                # comprehensive_analysis_results ì´ˆê¸°í™”
                                comprehensive_analysis_results = None

                                genai.configure(api_key=gemini_api_key)
                                model = genai.GenerativeModel('gemini-2.0-flash-lite')

                                # 1ì°¨ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ (ë¬¸ì œì  íƒì§€ ì¤‘ì‹¬)
                                # ê²€ìƒ‰ëœ íŒë¡€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                                theoretical_results = st.session_state.get('theoretical_results', None)
                                first_prompt = create_analysis_prompt(pdf_text, search_results_for_analysis, superior_laws_content, None, is_first_ordinance, comprehensive_analysis_results, theoretical_results)

                                # ğŸ†• Gemini ì „ì†¡ í”„ë¡¬í”„íŠ¸ ë””ë²„ê¹… í‘œì‹œ - expanderë¡œ ë³€ê²½í•˜ì—¬ ì¬ì‹¤í–‰ ë°©ì§€
                                with st.expander("ğŸ” Geminiì—ê²Œ ì „ì†¡ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ë‚´ìš© í™•ì¸", expanded=False):
                                    st.markdown("### í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ë¶„ì„")
                                    st.markdown(f"**ì „ì²´ ê¸¸ì´**: {len(first_prompt):,}ì")

                                    # ìƒìœ„ë²•ë ¹ ë‚´ìš© ë¶€ë¶„ë§Œ ì¶”ì¶œ
                                    if "ìƒìœ„ë²•ë ¹ë“¤ì˜ ì‹¤ì œ ì¡°ë¬¸ ë‚´ìš©" in first_prompt:
                                        law_start = first_prompt.find("ìƒìœ„ë²•ë ¹ë“¤ì˜ ì‹¤ì œ ì¡°ë¬¸ ë‚´ìš©")
                                        law_end = first_prompt.find("3. [ê²€í†  ì‹œ ìœ ì˜ì‚¬í•­]")
                                        if law_end == -1:
                                            law_end = law_start + 5000  # ê¸°ë³¸ê°’

                                        law_content = first_prompt[law_start:law_end]
                                        st.markdown(f"**ìƒìœ„ë²•ë ¹ ë‚´ìš© ê¸¸ì´**: {len(law_content):,}ì")

                                        st.text_area(
                                            "ìƒìœ„ë²•ë ¹ ê´€ë ¨ í”„ë¡¬í”„íŠ¸ ë‚´ìš©",
                                            law_content[:3000] + "..." if len(law_content) > 3000 else law_content,
                                            height=300,
                                            key="prompt_law_content"
                                        )

                                    # ì „ì²´ í”„ë¡¬í”„íŠ¸ í‘œì‹œ (ì²˜ìŒ 2000ìë§Œ)
                                    st.text_area(
                                        "ì „ì²´ í”„ë¡¬í”„íŠ¸ (ì²˜ìŒ 2000ì)",
                                        first_prompt[:2000] + "..." if len(first_prompt) > 2000 else first_prompt,
                                        height=400,
                                        key="full_prompt"
                                    )
                                
                                response = model.generate_content(first_prompt)
                                
                                if response and hasattr(response, 'text') and response.text:
                                    first_analysis = response.text

                                    # ë¬¸ì œì  í‚¤ì›Œë“œ íƒì§€
                                    problem_keywords = [
                                        "ìœ„ë°˜", "ë¬¸ì œ", "ì¶©ëŒ", "ë¶€ì ì ˆ", "ê°œì„ ", "ìˆ˜ì •", "ë³´ì™„",
                                        "ë²•ë ¹ ìœ„ë°˜", "ìƒìœ„ë²•ë ¹", "ìœ„ë²•", "ë¶ˆì¼ì¹˜", "ëª¨ìˆœ", "ìš°ë ¤"
                                    ]

                                    has_problems = any(keyword in first_analysis for keyword in problem_keywords)

                                    if has_problems:
                                        st.warning(f"âš ï¸ Geminiê°€ ì ì¬ì  ë¬¸ì œì ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")

                                    # ğŸ†• 3-2ë‹¨ê³„: Gemini ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì •ë°€ ì¬ê²€ìƒ‰
                                    # âœ… ìœ„ë²•ì„± ìœ ë¬´ì™€ ê´€ê³„ì—†ì´ í•­ìƒ ê²€ìƒ‰ (ìœ ì‚¬ ì‚¬ë¡€ë„ ì°¸ê³  ê°€ì¹˜ ìˆìŒ)
                                    if st.session_state.gemini_store_manager:
                                        with st.spinner("ğŸ” 1ì°¨ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ íŒë¡€ë¥¼ ì •ë°€ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                                            try:
                                                # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ì¡°ë¡€ëª… + êµ¬ì²´ì ì¸ ì¡°í•­ ì œëª©)
                                                # re ëª¨ë“ˆì€ íŒŒì¼ ìƒë‹¨ì—ì„œ ì´ë¯¸ importë¨

                                                # 1. ì¡°ë¡€ëª…ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                                                search_keywords = []
                                                if ordinance_name:
                                                    search_keywords.append(ordinance_name)

                                                # 2. ë¶„ì„ ê²°ê³¼ì—ì„œ ì œâ—‹ì¡° íŒ¨í„´ ì¶”ì¶œ
                                                article_mentions = re.findall(r'ì œ\s*\d+\s*ì¡°[^,\n]{0,30}', first_analysis)
                                                search_keywords.extend(article_mentions[:5])

                                                # 3. í•µì‹¬ ë²•ì  ìŸì  í‚¤ì›Œë“œ ì¶”ì¶œ
                                                key_issues = []
                                                issue_patterns = [
                                                    r'(ê¸°ê´€ìœ„ì„ì‚¬ë¬´)',
                                                    r'(ì§ì—…ì„ íƒì˜\s*ììœ )',
                                                    r'(ê³„ì•½[ì˜]?\s*ììœ )',
                                                    r'(ë²•ë¥ ìœ ë³´[ì›ì¹™]?)',
                                                    r'(í‰ë“±ê¶Œ)',
                                                    r'(ì¬ì‚°ê¶Œ)',
                                                    r'(ì˜ì—…ì˜\s*ììœ )',
                                                    r'(ê³¼ì‰ê¸ˆì§€[ì›ì¹™]?)',
                                                ]
                                                for pattern in issue_patterns:
                                                    matches = re.findall(pattern, first_analysis)
                                                    key_issues.extend(matches)

                                                # ì¤‘ë³µ ì œê±°
                                                key_issues = list(set(key_issues))[:5]

                                                # 1ï¸âƒ£ íŒë¡€ ë° ì‚¬ë¡€ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ê°„ê²°í•˜ê²Œ)
                                                if ordinance_name and key_issues:
                                                    # ì¡°ë¡€ëª… + ë²•ì  ìŸì 
                                                    case_query = f"'{ordinance_name}'ê³¼ ê´€ë ¨ëœ {', '.join(key_issues)} ìœ„ë°˜ íŒë¡€ì™€ ì¬ì˜Â·ì œì†Œ ì‚¬ë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”."
                                                elif ordinance_name:
                                                    # ì¡°ë¡€ëª…ë§Œ
                                                    case_query = f"'{ordinance_name}'ì˜ ìœ„ë²• íŒë¡€, ì¬ì˜ ìš”êµ¬, ì œì†Œ ì‚¬ë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”."
                                                elif key_issues:
                                                    # ë²•ì  ìŸì ë§Œ
                                                    case_query = f"{', '.join(key_issues)} ìœ„ë°˜ ì¡°ë¡€ íŒë¡€ì™€ ì¬ì˜Â·ì œì†Œ ì‚¬ë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”."
                                                else:
                                                    # ì¼ë°˜ ê²€ìƒ‰
                                                    case_query = "ì¡°ë¡€ ìœ„ë²• íŒë¡€ì™€ ì¬ì˜Â·ì œì†Œ ì‚¬ë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”."

                                                # 2ï¸âƒ£ ì´ë¡ ì  ì„¤ëª… ë° ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                                                if key_issues:
                                                    # êµ¬ì²´ì ì¸ ë²•ì  ìŸì ì´ ìˆëŠ” ê²½ìš°
                                                    theory_query = f"{', '.join(key_issues)}ì— ëŒ€í•œ ë²•ë¦¬, ì´ë¡ ì  ì„¤ëª…, íŒë‹¨ ê¸°ì¤€ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                                                else:
                                                    # ì¼ë°˜ì ì¸ ì¡°ë¡€ ì œì • ì´ë¡  ê²€ìƒ‰
                                                    theory_query = "ì¡°ë¡€ ì œì •ì˜ ë²•ë¦¬ì™€ ì›ì¹™, ìƒìœ„ë²•ë ¹ ìœ„ë°° íŒë‹¨ ê¸°ì¤€ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."

                                                # íŒë¡€/ì‚¬ë¡€ ê²€ìƒ‰ ìˆ˜í–‰
                                                case_result = st.session_state.gemini_store_manager.search(
                                                    case_query,
                                                    top_k=5
                                                )

                                                # ì´ë¡ /ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ìˆ˜í–‰
                                                theory_result = st.session_state.gemini_store_manager.search(
                                                    theory_query,
                                                    top_k=5
                                                )

                                                # ê²€ìƒ‰ ê²°ê³¼ í†µí•©
                                                case_answer = case_result.get('answer', '')
                                                case_sources = case_result.get('sources', [])

                                                theory_answer = theory_result.get('answer', '')
                                                theory_sources = theory_result.get('sources', [])

                                                # ë‘ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê²°í•©
                                                combined_answer = ""
                                                combined_sources = []

                                                if case_answer and len(case_answer) > 200:
                                                    combined_answer += "## ğŸ“š ê´€ë ¨ íŒë¡€ ë° ì¬ì˜Â·ì œì†Œ ì‚¬ë¡€\n\n"
                                                    combined_answer += case_answer
                                                    combined_sources.extend(case_sources)

                                                if theory_answer and len(theory_answer) > 200:
                                                    if combined_answer:
                                                        combined_answer += "\n\n---\n\n"
                                                    combined_answer += "## ğŸ“– ì´ë¡ ì  ê·¼ê±° ë° ë²•ë¦¬ í•´ì„¤\n\n"
                                                    combined_answer += theory_answer
                                                    combined_sources.extend(theory_sources)

                                                # ìµœì¢… ë‹µë³€ ì„¤ì •
                                                refined_answer = combined_answer if combined_answer else ""
                                                refined_sources = combined_sources

                                                if refined_answer and len(refined_answer) > 500:
                                                    # ê¸°ì¡´ íŒë¡€ ê²°ê³¼ì— ì¶”ê°€
                                                    search_summary = []
                                                    if case_answer and len(case_answer) > 200:
                                                        search_summary.append(f"íŒë¡€/ì‚¬ë¡€ {len(case_answer)}ì")
                                                    if theory_answer and len(theory_answer) > 200:
                                                        search_summary.append(f"ì´ë¡ /ë²•ë¦¬ {len(theory_answer)}ì")

                                                    refined_case = {
                                                        'violation_type': 'ì •ë°€ ê²€ìƒ‰ ê²°ê³¼ (íŒë¡€ + ì´ë¡ )',
                                                        'content': refined_answer,
                                                        'similarity': 0.98,
                                                        'topic': f'ì •ë°€ ê²€ìƒ‰: íŒë¡€Â·ì‚¬ë¡€ ë° ì´ë¡ ì  ê·¼ê±° ({", ".join(search_summary)})',
                                                        'relevance_score': 0.98,
                                                        'context_relevance': 0.95,
                                                        'matched_concepts': ['íŒë¡€', 'ì´ë¡ ', 'ë²•ë¦¬', 'ê°€ì´ë“œë¼ì¸', 'ì •ë°€ê²€ìƒ‰'],
                                                        'summary': refined_answer[:200] + '...',
                                                        'metadata': {
                                                            'source': 'gemini_file_search_comprehensive',
                                                            'source_files': [s.get('title', '') for s in refined_sources if s.get('title')],
                                                            'query_case': case_query,
                                                            'query_theory': theory_query,
                                                            'search_type': 'comprehensive_analysis_based',
                                                            'has_cases': bool(case_answer and len(case_answer) > 200),
                                                            'has_theory': bool(theory_answer and len(theory_answer) > 200)
                                                        }
                                                    }

                                                    # ì •ë°€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë§¨ ì•ì— ì¶”ê°€ (ê°€ì¥ ê´€ë ¨ì„± ë†’ìŒ)
                                                    theoretical_results.insert(0, refined_case)
                                                    st.session_state.theoretical_results = theoretical_results

                                                    st.success(f"âœ… ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì •ë°€ ê²€ìƒ‰ ì™„ë£Œ: {', '.join(search_summary)}")

                                                    # ë¯¸ë¦¬ë³´ê¸°
                                                    with st.expander("ğŸ¯ ì •ë°€ ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (íŒë¡€ + ì´ë¡ )", expanded=True):
                                                        st.markdown(f"**{refined_case['topic']}**")
                                                        st.markdown(f"ğŸ“„ {refined_answer[:500]}...")

                                                        # ì¶œì²˜ íŒŒì¼ í‘œì‹œ
                                                        unique_sources = list(set([s for s in refined_case['metadata']['source_files'] if s]))
                                                        if unique_sources:
                                                            st.markdown(f"ğŸ“ ì¶œì²˜: {', '.join(unique_sources[:5])}")

                                                        # ê²€ìƒ‰ ìœ í˜• í‘œì‹œ
                                                        if refined_case['metadata']['has_cases']:
                                                            st.markdown("âœ“ íŒë¡€ ë° ì¬ì˜Â·ì œì†Œ ì‚¬ë¡€ í¬í•¨")
                                                        if refined_case['metadata']['has_theory']:
                                                            st.markdown("âœ“ ì´ë¡ ì  ê·¼ê±° ë° ë²•ë¦¬ í•´ì„¤ í¬í•¨")
                                                else:
                                                    st.info("â„¹ï¸ ì •ë°€ ê²€ìƒ‰ì—ì„œ ì¶”ê°€ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                                            except Exception as e:
                                                st.warning(f"âš ï¸ ì •ë°€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {str(e)}")

                                    else:
                                        st.success("âœ… Gemini 1ì°¨ ë¶„ì„ì—ì„œ íŠ¹ë³„í•œ ë¬¸ì œì ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                                    analysis_results.append({
                                        'model': 'Gemini (1ì°¨ ë¶„ì„)',
                                        'content': first_analysis
                                    })
                                else:
                                    st.error("Gemini 1ì°¨ ë¶„ì„ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"Gemini 1ì°¨ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                analysis_results.append({
                                    'model': 'Gemini (1ì°¨ ë¶„ì„)',
                                    'error': str(e)
                                })
                        
                        # 4ë‹¨ê³„: ë¬¸ì œ ë°œê²¬ ì‹œ ìë£Œ ì°¸ê³  ë¶„ì„ ìˆ˜í–‰
                        relevant_guidelines = None
                        loaded_stores = []
                        enhanced_analysis = None
                        
                        if has_problems and use_auto_search and first_analysis:
                            # 4ë‹¨ê³„: Gemini File Searchë¥¼ ì‚¬ìš©í•œ ê´€ë ¨ ìë£Œ ê²€ìƒ‰
                            comprehensive_analysis_results = None

                            # ë°œê²¬ëœ ë¬¸ì œì ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                            search_terms = []

                            # ì‚¬ë¬´ ê´€ë ¨ ë¬¸ì œ
                            if any(word in first_analysis for word in ["ì†Œê´€ì‚¬ë¬´", "ì‚¬ë¬´êµ¬ë¶„", "ìœ„ì„ì‚¬ë¬´", "ìì¹˜ì‚¬ë¬´"]):
                                search_terms.extend(["ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ì¡°ë¡€ì œì • ë¶ˆê°€", "ìœ„ì„ì‚¬ë¬´ ì¡°ë¡€ ì œì • í•œê³„"])

                            # ë²•ë ¹ ìœ„ë°˜ ê´€ë ¨ ë¬¸ì œ
                            if any(word in first_analysis for word in ["ë²•ë ¹ ìœ„ë°˜", "ìƒìœ„ë²•ë ¹", "ë²•ë ¹ìš°ìœ„", "ìœ„ë°˜"]):
                                search_terms.extend(["ë²•ë ¹ ìœ„ë°˜ ì¡°ë¡€ ì‚¬ë¡€", "ìƒìœ„ë²•ë ¹ ì¶©ëŒ ì¡°ë¡€"])

                            # ì¡°ë¡€ ì œì • í•œê³„ ê´€ë ¨
                            if any(word in first_analysis for word in ["ì œì • í•œê³„", "ì…ë²•í•œê³„", "ë¶ˆê°€", "ìœ„ë²•"]):
                                search_terms.extend(["ì¡°ë¡€ ì œì • í•œê³„ íŒë¡€", "ìœ„ë²• ì¡°ë¡€ ì œì • ì‚¬ë¡€"])

                            # ê¸°ë³¸ ê²€ìƒ‰ì–´ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ê²€ìƒ‰ì–´ ì‚¬ìš©
                            if not search_terms:
                                search_terms = ["ë²•ë ¹ ìœ„ë°˜ ì¡°ë¡€ íŒë¡€", "ì¡°ë¡€ ì œì • í•œê³„ ì‚¬ë¡€"]

                            # ì—¬ëŸ¬ ê²€ìƒ‰ì–´ ì¤‘ í•˜ë‚˜ ì„ íƒ (ê°€ì¥ êµ¬ì²´ì ì¸ ê²ƒ)
                            search_query = search_terms[0] if search_terms else "ìœ„ë²• ì¡°ë¡€ íŒë¡€"

                            # Gemini File Search ì‚¬ìš©
                            if st.session_state.gemini_store_manager:
                                try:
                                    relevant_guidelines = search_relevant_guidelines_gemini(
                                        query=search_query,
                                        api_key=gemini_api_key,
                                        store_manager=st.session_state.gemini_store_manager,
                                        top_k=8
                                    )
                                    loaded_stores = ["Gemini File Search (í†µí•© ì €ì¥ì†Œ)"]

                                    if relevant_guidelines:
                                        st.success(f"âœ… {len(relevant_guidelines)}ê°œì˜ ê´€ë ¨ ìë£Œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤")

                                except Exception as e:
                                    st.error(f"Gemini ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                                    relevant_guidelines = []
                                    loaded_stores = []
                            else:
                                st.warning("âš ï¸ Gemini File Searchê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                                relevant_guidelines = []
                                loaded_stores = []
                            
                            if relevant_guidelines and loaded_stores:
                                st.success(f"âœ… {len(loaded_stores)}ê°œ ìë£Œì—ì„œ {len(relevant_guidelines)}ê°œ ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤:")
                                for store in loaded_stores:
                                    st.markdown(f"   â€¢ {store}")
                                
                                # ê°€ì´ë“œë¼ì¸ ë¯¸ë¦¬ë³´ê¸° (ì„ íƒì‚¬í•­)
                                with st.expander("ğŸ“– ê²€ìƒ‰ëœ ë¬¸ì œ ê´€ë ¨ ìë£Œ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                                    source_groups = {}
                                    for guideline in relevant_guidelines:
                                        source_store = guideline.get('source_store', 'ì•Œ ìˆ˜ ì—†ëŠ” ìë£Œ')
                                        if source_store not in source_groups:
                                            source_groups[source_store] = []
                                        source_groups[source_store].append(guideline)
                                    
                                    for source_store, guidelines in source_groups.items():
                                        st.markdown(f"**ğŸ“š {source_store}**")
                                        for i, guideline in enumerate(guidelines):
                                            similarity_score = guideline.get('similarity', 1-guideline.get('distance', 0))
                                            st.markdown(f"   [{i+1}] (ìœ ì‚¬ë„: {similarity_score:.3f})")
                                            st.markdown(guideline['text'][:200] + "..." if len(guideline['text']) > 200 else guideline['text'])
                                            st.markdown("---")
                                
                                # 2ì°¨ ë³´ê°• ë¶„ì„ ìˆ˜í–‰ (ì¡°ìš©íˆ) - Ollama Cloud ìš°ì„  ì‚¬ìš©
                                if has_ollama:
                                    try:
                                        # ë³´ê°• ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸
                                        enhanced_prompt = create_analysis_prompt(
                                            pdf_text,
                                            search_results_for_analysis,
                                            superior_laws_content,
                                            relevant_guidelines,
                                            is_first_ordinance,
                                            comprehensive_analysis_results,
                                            theoretical_results
                                        )

                                        with st.spinner("ğŸ¤– AIê°€ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê°• ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                                            enhanced_analysis = call_ollama_cloud_api(enhanced_prompt)

                                        if enhanced_analysis:
                                            analysis_results.append({
                                                'model': f'Ollama Cloud (ìë£Œ ì°¸ê³  ë³´ê°•ë¶„ì„ - {len(loaded_stores)}ê°œ ìë£Œ)',
                                                'content': enhanced_analysis
                                            })
                                    except Exception as e:
                                        st.error(f"Ollama Cloud ë³´ê°• ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                elif gemini_api_key:
                                    try:
                                        # ë³´ê°• ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸
                                        enhanced_prompt = create_analysis_prompt(
                                            pdf_text,
                                            search_results_for_analysis,
                                            superior_laws_content,
                                            relevant_guidelines,
                                            is_first_ordinance,
                                            comprehensive_analysis_results,
                                            theoretical_results
                                        )

                                        enhanced_response = model.generate_content(enhanced_prompt)
                                        if enhanced_response and hasattr(enhanced_response, 'text') and enhanced_response.text:
                                            enhanced_analysis = enhanced_response.text
                                            analysis_results.append({
                                                'model': f'Gemini (ìë£Œ ì°¸ê³  ë³´ê°•ë¶„ì„ - {len(loaded_stores)}ê°œ ìë£Œ)',
                                                'content': enhanced_analysis
                                            })
                                    except Exception as e:
                                        st.error(f"ìë£Œ ì°¸ê³  ë³´ê°• ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                            else:
                                st.info("ë¬¸ì œì ê³¼ ê´€ë ¨ëœ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        elif not has_problems:
                            st.info("âœ… ë¬¸ì œì ì´ ë°œê²¬ë˜ì§€ ì•Šì•„ ìë£Œ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                        elif not use_auto_search:
                            st.info("ğŸ”„ ìë™ ì°¸ê³  ìë£Œ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                        
                        # 5ë‹¨ê³„: OpenAI ì¶”ê°€ ë¶„ì„ (ì„ íƒì‚¬í•­)
                        if openai_api_key:
                            try:
                                openai.api_key = openai_api_key
                                # ê°€ì¥ ì™„ì „í•œ í”„ë¡¬í”„íŠ¸ë¡œ OpenAI ë¶„ì„
                                openai_prompt = create_analysis_prompt(pdf_text, search_results_for_analysis, superior_laws_content, relevant_guidelines, is_first_ordinance, comprehensive_analysis_results, theoretical_results)
                                
                                response = openai.ChatCompletion.create(
                                    model="gpt-4",
                                    messages=[
                                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¡°ë¡€ ë¶„ì„ê³¼ ê²€í† ë¥¼ ë„ì™€ì£¼ì„¸ìš”."},
                                        {"role": "user", "content": openai_prompt}
                                    ],
                                    temperature=0.7,
                                    max_tokens=4000
                                )
                                
                                if response.choices[0].message.content:
                                    analysis_results.append({
                                        'model': 'OpenAI (ì¶”ê°€ ë¶„ì„)',
                                        'content': response.choices[0].message.content
                                    })
                            except Exception as e:
                                st.error(f"OpenAI ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                analysis_results.append({
                                    'model': 'OpenAI (ì¶”ê°€ ë¶„ì„)',
                                    'error': str(e)
                                })
                        
                        if analysis_results:
                            # ğŸ†• ë¶„ì„ ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                            st.session_state.analysis_results = analysis_results
                            st.session_state.analysis_metadata = {
                                'has_problems': has_problems,
                                'relevant_guidelines': relevant_guidelines,
                                'loaded_stores': loaded_stores,
                                'is_first_ordinance': is_first_ordinance,
                                'superior_laws_content': superior_laws_content,
                                'search_results_for_analysis': search_results_for_analysis,
                                'pdf_text': pdf_text,
                                'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            }

                            # ë¶„ì„ ì™„ë£Œ ë©”ì‹œì§€
                            st.markdown("---")
                            if has_problems and relevant_guidelines and loaded_stores:
                                st.success(f"ğŸ¯ **ë³µí•© ìë£Œ ë³´ê°• ë¶„ì„ ì™„ë£Œ**: ë¬¸ì œì  íƒì§€ â†’ {len(loaded_stores)}ê°œ ìë£Œ ì°¸ê³  â†’ ë³´ê°• ë¶„ì„")
                            elif has_problems and relevant_guidelines:
                                st.success("ğŸ¯ **ì§€ëŠ¥í˜• ë¶„ì„ ì™„ë£Œ**: ë¬¸ì œì  íƒì§€ â†’ ìë£Œ ê²€ìƒ‰ â†’ ë³´ê°• ë¶„ì„")
                            elif has_problems:
                                st.info("âš ï¸ **ë¬¸ì œì  íƒì§€ ë¶„ì„ ì™„ë£Œ**: ìë£Œ ê²€ìƒ‰ ì—†ì´ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰")
                            else:
                                st.success("âœ… **ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ**: íŠ¹ë³„í•œ ë¬¸ì œì ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
                            
                            # ë¶„ì„ ê²°ê³¼ ìš”ì•½
                            analysis_count = len([r for r in analysis_results if 'error' not in r])
                            error_count = len([r for r in analysis_results if 'error' in r])
                            
                            if analysis_count > 0:
                                # ğŸ†• ì„ íƒëœ ì¡°ë¡€ ìˆ˜ ì •í™•íˆ ë°˜ì˜
                                if is_first_ordinance:
                                    analysis_type_text = "ìµœì´ˆ ì œì • ì¡°ë¡€"
                                else:
                                    selected_count = len(search_results_for_analysis)
                                    analysis_type_text = f"ì„ íƒëœ {selected_count}ê°œ íƒ€ ì‹œë„ ì¡°ë¡€ ë¹„êµ"
                                st.markdown(f"**ğŸ“‹ ë¶„ì„ ìœ í˜•**: {analysis_type_text}")
                                st.markdown(f"**ğŸ¤– ìˆ˜í–‰ëœ ë¶„ì„**: {analysis_count}ê°œ")
                                if relevant_guidelines:
                                    st.markdown(f"**ğŸ“š ì°¸ê³ ëœ ê°€ì´ë“œë¼ì¸**: {len(relevant_guidelines)}ê°œ")
                            
                            # ìµœì¢… ë³´ê³ ì„œë§Œ í‘œì‹œ (ìë£Œ ì°¸ê³  ë³´ê°• ë¶„ì„ ë˜ëŠ” OpenAI ë¶„ì„)
                            final_report = None

                            # ìš°ì„ ìˆœìœ„: ìë£Œ ì°¸ê³  ë³´ê°•ë¶„ì„ > OpenAI ì¶”ê°€ ë¶„ì„ > 1ì°¨ ë¶„ì„
                            for result in reversed(analysis_results):  # ì—­ìˆœìœ¼ë¡œ ìµœì‹  ê²°ê³¼ ìš°ì„ 
                                if 'error' not in result:
                                    if "ìë£Œ ì°¸ê³  ë³´ê°•ë¶„ì„" in result['model']:
                                        final_report = result
                                        break
                                    elif "ìë£Œ ì°¸ê³ " in result['model'] or "OpenAI" in result['model']:
                                        final_report = result
                                        break

                            # ìë£Œ ì°¸ê³ ë‚˜ OpenAIê°€ ì—†ìœ¼ë©´ 1ì°¨ ë¶„ì„ ì‚¬ìš©
                            if not final_report:
                                for result in analysis_results:
                                    if 'error' not in result and "1ì°¨ ë¶„ì„" in result['model']:
                                        final_report = result
                                        break

                            # ìµœì¢… ë³´ê³ ì„œ í‘œì‹œ
                            if final_report:
                                st.markdown("### ğŸ“‹ ìµœì¢… ë¶„ì„ ë³´ê³ ì„œ")

                                # ë³´ê³ ì„œ íƒ€ì… í‘œì‹œ
                                if "ìë£Œ ì°¸ê³  ë³´ê°•ë¶„ì„" in final_report['model']:
                                    st.success("ğŸ¯ **ìë£Œ ì°¸ê³  ë³´ê°• ë¶„ì„ ê²°ê³¼**")
                                    st.caption(f"ğŸ“š **í™œìš© ëª¨ë¸**: {final_report['model']}")
                                elif "ìë£Œ ì°¸ê³ " in final_report['model']:
                                    st.success("ğŸ¯ **ì°¸ê³  ìë£Œ ê¸°ë°˜ ë³´ê°• ë¶„ì„ ê²°ê³¼**")
                                elif "OpenAI" in final_report['model']:
                                    st.info("ğŸ“Š **OpenAI ì¶”ê°€ ë¶„ì„ ê²°ê³¼**")
                                elif "Ollama Cloud" in final_report['model']:
                                    st.info("ğŸ¤– **Ollama Cloud AI ë¶„ì„ ê²°ê³¼** (ë¬´ë£Œ ì„œë¹„ìŠ¤)")
                                else:
                                    st.info("ğŸ¤– **Gemini ê¸°ë³¸ ë¶„ì„ ê²°ê³¼**")

                                # ë³´ê³ ì„œ ë‚´ìš© (content ë˜ëŠ” analysis í‚¤ ì§€ì›)
                                report_content = final_report.get('content') or final_report.get('analysis', '')
                                st.markdown(report_content)

                            # ì˜¤ë¥˜ ë©”ì‹œì§€ë§Œ ë³„ë„ í‘œì‹œ
                            for result in analysis_results:
                                if 'error' in result:
                                    st.error(f"âŒ {result['model']} ì˜¤ë¥˜: {result['error']}")
                            
                            # Word ë¬¸ì„œ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
                            with st.spinner("ë¶„ì„ ê²°ê³¼ Word ë¬¸ì„œ ìƒì„± ì¤‘..."):
                                doc = create_comparison_document(pdf_text, search_results_for_analysis, analysis_results, superior_laws_content, relevant_guidelines)
                                
                                doc_io = io.BytesIO()
                                doc.save(doc_io)
                                doc_bytes = doc_io.getvalue()
                                
                                # íŒŒì¼ëª…ì— ë¶„ì„ ë°©ì‹ í‘œì‹œ
                                if has_problems and relevant_guidelines and loaded_stores:
                                    stores_count = len(loaded_stores)
                                    filename_prefix = f"ë³µí•©ìë£Œë³´ê°•ë¶„ì„({stores_count}ê°œìë£Œ)" if is_first_ordinance else f"ì¡°ë¡€ë¹„êµ_ë³µí•©ìë£Œë¶„ì„({stores_count}ê°œìë£Œ)"
                                elif has_problems and relevant_guidelines:
                                    filename_prefix = "ìë£Œì°¸ê³ ë³´ê°•ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€ë¹„êµ_ìë£Œë¶„ì„"
                                elif has_problems:
                                    filename_prefix = "ë¬¸ì œì íƒì§€ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€ë¹„êµ_ë¬¸ì œì ë¶„ì„"
                                else:
                                    filename_prefix = "ìµœì´ˆì¡°ë¡€_ê¸°ë³¸ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€_ê¸°ë³¸ë¹„êµë¶„ì„"
                                
                                st.download_button(
                                    label="ğŸ“„ ë¶„ì„ ê²°ê³¼ Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
                                    data=doc_bytes,
                                    file_name=f"{filename_prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx",
                                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                                )
                        else:
                            st.error("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()